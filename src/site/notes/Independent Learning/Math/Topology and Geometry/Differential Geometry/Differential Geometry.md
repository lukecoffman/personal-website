---
{"dg-publish":true,"permalink":"/independent-learning/math/topology-and-geometry/differential-geometry/differential-geometry/","created":"2025-01-15T18:11:38.281-07:00","updated":"2025-04-18T15:17:19.000-06:00"}
---

# Overview
> [!question] What is Differential Geometry?
> When we first learn calculus we are often focused on the $\mathbb{R}^{n}$ while sometimes daring to explore $\mathbb{C}^{n}$. However, there is a rich world of geometry to explore, in fact, we live on a non-Euclidean surface: a sphere! Though no one ever worries about doing calculus on the surface of the earth because everything *locally* looks like $\mathbb{R}^{2}$. This idea motivates **differential geometry**, i.e., how can we do calculus on things that globally look very strange but in a small enough region look flat again. Differential geometry has many applications, most notability in the Einstein's theory of general relativity for describing the curvature of space-time and the expansion of the universe. It also finds applications in quantum mechanics with [[Independent Learning/QIT/Quantum Information Geometry/Introduction to Quantum Information Geometry\|quantum information geometry]]. Here we explore the basics of differential geometry from a mathematical perspective while discussing some applications along the line.

> [!abstract] Summary of Topics Covered
> We begin our discussion with *manifolds* and in particular *smooth manifolds*.

> [!info] Recommended readings and resources
> Canonical text: ["Introduction to Smooth Manifolds" by Lee](https://link.springer.com/book/10.1007/978-1-4419-9982-5l)


> [!warning] Recommended / Assumed Prerequisite Topics
> This note assumes working knowledge of the following topics:
> - Undergraduate multivariate [[Independent Learning/Math/Analysis/Real Analysis\|Real Analysis]]
> - [[Independent Learning/Math/Algebra/Linear Algebra\|Linear Algebra]]
> - [[Independent Learning/Math/Topology and Geometry/Topology\|Topology]]
>
> While the text is written to minimize the required background information, at some points, it will be unavoidable. Hopefully, by following a sufficient number of links one can fill these gaps and return to the topic at a later point.

> [!danger] Disclaimer
> These notes mainly follow Lee's text, but were originally taken during Dr. Jeanne Clelland's graduate differential geometry course at the University of Colorado, Boulder in Spring 2025. There are also some of my personal modifications and deviations present depending on my interests.


# Chapter 1-2: Smooth Manifolds and Maps

## Topological Manifolds

> [!def] Topological Manifold
> Suppose that $M$ is a [[Independent Learning/Math/Topology and Geometry/Topology#^def-topological-space\|topological space]]. $M$ is said to be a **topological $n$-manifold** or a **topological manifold of dimension $n$** if it has the following properties:
> 1. $M$ is [[Independent Learning/Math/Topology and Geometry/Topology#^def-Hausdorff\|Hausdorff]].
> 2. $M$ is [[Independent Learning/Math/Topology and Geometry/Topology#^def-seond-countable\|second-countable]].
> 3. $M$ is **locally Euclidean of dimension $n$**: for all points $p\in M$ there exists an open neighborhood $U$ of $p$, an open set $\hat{U}\subseteq \mathbb{R}^{n}$, and a [[Independent Learning/Math/Topology and Geometry/Topology#^def-homeomorphism\|homeomorphism]] $\varphi:U\to \hat{U}$ between them.
>
{ #def-topological-manifold}


One might wonder if the dimension of a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-topological-manifold\|topological manifold]] is well-defined, and indeed two nonempty topological manifolds of different dimension cannot be homeomorphic to each other. This is called the **topological invariance of dimension**. However, we do not yet have the machinery to tackle this result (and won't for quite some time!). Notice that we said "nonempty", this is because the empty set is a topological manifold for every $n$.

> [!question]- What's the simplest example of a (nonempty) [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-topological-manifold\|topological manifold]]?
> The simplest one that immediately comes to mind is likely $\mathbb{R}$ (or $\mathbb{R}^{n}$). Indeed, the definition of a topological manifold is modeled after $\mathbb{R}^{n}$ which is a [[Independent Learning/Math/Analysis/Real Analysis#^def-metric-space\|metric space]] and hence [[Independent Learning/Math/Topology and Geometry/Topology#^def-Hausdorff\|Hausdorff]] with a [[Independent Learning/Math/Topology and Geometry/Topology#^def-seond-countable\|second-countable]] basis generated by rational centers with rational radii. 

## Coordinate Charts

> [!def] Coordinate Charts (or Charts)
> Let $M$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-topological-manifold\|topological n-manifold]]. A **coordinate chart** (or just a **chart**) on $M$ is a pair $(U,\varphi)$ where $U$ is an open subset of $M$ and $\varphi:U\to \hat{U}$ is a [[Independent Learning/Math/Topology and Geometry/Topology#^def-homeomorphism\|homeomorphism]] from $U$ to $\hat{U}=\varphi(U)$ open in $\mathbb{R}^{n}$. By the definition of a topological manifold, every point $p\in M$ is contained in some chart $(U,\varphi)$. Any chart $(U,\varphi)$ can be **centered at $p$** by translating $\hat{U}$ by $\varphi(p)$ to enforce $\varphi(p)=0$. We call $U$ the **coordinate domain**, or a **coordinate ball** if $\varphi(U)$ is an open ball in $\mathbb{R}^{n}$, or a **coordinate cube** if $\varphi(U)$ is an open cube in $\mathbb{R}^{n}$. Finally, we call $\varphi$ a **(local) coordinate map**, and the component functions $(x^{1},\ldots,x^{n})$ defined by $\varphi(p)=(x^{1}(p),\ldots,x^{n}(p))$ are called **local coordinates** on $U$.
>
{ #def-coordinate-charts}


## Examples of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-topological-manifold\|Topological Manifolds]]


> [!example] Graphs of Continuous Functions
> Let $U\subseteq \mathbb{R}^{n}$ be open and $f:U\to \mathbb{R}^{k}$ be a continuous function. The **graph of $f$** is the subset of $\mathbb{R}^{n}\times \mathbb{R}^{k}$ defined by
> $$
> \Gamma(f)=\{(x,f(x))\in \mathbb{R}^{n}\times \mathbb{R}^{k}\}
> $$
> which is given the [[Independent Learning/Math/Topology and Geometry/Topology#^def-subspace-topology\|subspace topology]]. Let $\pi_{1}:\mathbb{R}^{n}\times \mathbb{R}^{k}\to \mathbb{R}^{k}$ denote the projection onto the first factor, and let $\varphi:\Gamma(f)\to U$ be the restriction of $\pi_{1}$ to $\Gamma(f)$: $\varphi(x,y)=x,(x,f(x))\in\Gamma(f)$. 



# Chapter 3: Tangent Vectors
## Week 5 Notes
### Chapter 3: Tangent Vectors
Derivatives are all about linear approximations, intuitively, the tangent space to a smooth manifold at a point $p\in M$ should be a "linear model" for $M$ near $p$.

Let's start with the case of a manifold in $\mathbb{R}^{n}$. We are accustomed to thinking of points in $\mathbb{R}^{n}$ in two different ways:
1. They are locations via coordinates $(x^{1},\ldots, x^{n})$
2. They are vectors $\vec{v}=v^{i}\vec{e}_{i}$ with magnitude and direction.
These are really two different things! A vector $\vec{v}$ is based at a point $x$, and the same expression for $v^{i}\vec{e}_{i}$ based at 2 different points represents 2 different objections.


> [!def] Geometric Tangent Space
> Given a **point** $\vec{a}\in \mathbb{R}^{n}$, the **geometric tangent space to $\mathbb{R}^{n}$ at $a$** is the set $\mathbb{R}^{n}_{a}=\{(a,\vec{v}):\vec{v}\in \mathbb{R}^{n}\}$ where we denote $(a,\vec{v})$ by $\vec{v}_{a}$. The set $\mathbb{R}^{n}_{a}$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] in the obvious way. For distinct points $a,b\in \mathbb{R}^{n}$, the spaces $\mathbb{R}^{n}_{a}$ and $\mathbb{R}^{n}_{b}$ are disjoint sets.

So if $M\subseteq \mathbb{R}^{n}$ is a smooth submanifold, we could think of the tangent space to $M$ at $a\in M$ as a subspace of $\mathbb{R}^{n}_{a}$. But how could we make sense of this idea if $M$ is just an arbitrary smooth manifold and not a submanifold of some $\mathbb{R}^{n}$? Tools we have so far smooth coordinate chards, functions and maps.


> [!NOTE] Observation
> Geometric tangent vectors act on smooth functions by directional derivative, i.e., any $\vec{v}_{a}\in \mathbb{R}^{n}$ defines a map $D_{\vec{v}}\lvert_{a}:C^{\infty}(\mathbb{R}^{n})\to \mathbb{R}^{n}$ by $D_{\vec{v}}\lvert_{a}(f)=\frac{\partial }{\partial t}\lvert_{{t=0}}f(a+t\vec{v})$. This operator is linear over $\mathbb{R}$ and satisfies the product rule.

If we write $\vec{v}=v^{i}\vec{e}_{i}$ in terms of the standard basis, then the chain rule says: $D_{\vec{v}}\lvert_{a}(f)=v^{i} \frac{\partial f}{\partial x^{i}} (a)$.


> [!def] Derivations
> Given $a\in \mathbb{R}^{n}$, a map $w:C^{\infty}(\mathbb{R}^{n}) \to \mathbb{R}$ is called a **derivation at $a$** if it is linear over $\mathbb{R}$ and satisfies the product rule (or Leibnitz rule).
>
{ #def-derivation}


The set of all derivations at $a\in \mathbb{R}^{n}$ is denoted $T_{a}\mathbb{R}^{n}$ which is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] in the obvious way.


> [!lemma] Properties of Derivations
> Let $a\in \mathbb{R}^{n}$, $w\in T_{a}\mathbb{R}^{n}$, and $f,g\in C^{\infty}(\mathbb{R}^{n})$.
> 1. If $f$ is a constant function, then $w(f)=0$.
> 2. If $f(a)=g(a)=0$, then $w(fg)=0$.

> [!proof]
> 1. First suppose that $f=1$, then $w(f)=w(ff)=2w(f)$ therefore $w(f)=0$. For any other constant, just use that $w$ is a linear operator.
> 2. Product rule.


> [!prop] Derivations correspond to geometric tangent vectors
> Let $a\in \mathbb{R}^{n}$,
> 1. For each $\vec{v}_{a}\in \mathbb{R}^{n}_{a}$, the map $D_{\vec{v}}\lvert_{a}:C^{\infty}(\mathbb{R}^{n})\to \mathbb{R}$ is [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation\|derivation at $a$]].
> 2. The map $\vec{v}_{a}\to D_{\vec{v}}\lvert_{a}$ is an isomorphism from $\mathbb{R}^{n}_{a}$ to $T_{a}\mathbb{R}^{n}$.

This is big! It shows that [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation\|derivations]] at $a$ are in 1-1 correspondence with geometric tangent vectors at $a$, and in fact $\mathbb{R}^{n}_{a}$ and $T_{a}\mathbb{R}^{n}$ are isomorphic as [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector spaces]].


> [!proof]
> 1. Follows from properties of $D_{\vec{v}}\lvert_{a}$.
> 2. The map $\vec{v}_{a}\to D_{\vec{v}}\lvert_{a}$ is clearly linear.
>	Injectivity: Suppose $\vec{v}_{a}$ has the property that $D_{\vec{v}}\lvert_{a}$ is the zero [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation\|derivation]]. Write $\vec{v}=v^{i}\vec{e}_{i}$ in the standard basis, and let $f=x^{j}:\mathbb{R}^{n}\to \mathbb{R}$. Then
> $$
> 0 = D_{\vec{v}}\lvert_{a}(f)=v^{i} \frac{\partial }{\partial x^{i}} (x^{j}) = v^{j}
> $$
>  Therefore, $\vec{v}=0$.
>  Surjectivity: Let $w\in T_{a}\mathbb{R}^{n}$ be arbitrary. For $i=1,\ldots,n$ let $v^{i}=w(x^{i})$, $\vec{v}=v^{i}\vec{e}_{i}$ we'll show that $w=D_{\vec{v}}\lvert_{a}$. Let $f\in C^{\infty}(\mathbb{R}^{n})$, we need to show that $w(f)=D_{\vec{v}}\lvert_{a}(f)$. By Taylor's theorem, we can write
> $$
  f(x)=f(a)+\sum_{i=1}^{n} \frac{\partial f}{\partial x^{i}}(a)(x^{i}-a^{i})+\sum_{i,j=1}^{n}(x^{i}-a^{i})(x^{j}-a^{j})g_{ij}(x)
> $$
>where $g_{ij}$ are smooth functions. By the lemma above, $w\left( \sum_{i,j=1}^{n} (x^{i}-a^{i})(x^{j}-a^{j})g_{ij}(x) \right)=0$ since the grouped terms are $0$ at $x=a$. Therefore,
>$$
w(f)=\underbrace{w(f(a))}_{0}+\sum_{i=1}^{n} w\left(\frac{\partial f}{\partial x^{i}}(a)(x^{i}-a^{i}) \right) = v^{i} \frac{\partial f}{\partial x^{i}}(a)=D_{\vec{v}}\lvert_{a}(f).
>$$

> [!corollary]
> For any $a\in \mathbb{R}^{n}$, the $n$ derivatives $\frac{\partial }{\partial x^{i}}\lvert_{a}$ for $1\le i \le n$ defined by $\frac{\partial }{\partial x^{i}}\lvert_{a}(f) = \frac{\partial f}{\partial x^{i}}(a)$ form a basis for $T_{a}\mathbb{R}^{n}$, which therefore has dimension $n$.
:
#### Tangent Vectors on Manifolds

> [!def] Derivation at a point on a Manifold
> Let $M$ be a smooth manifold and $p\in M$. A [[Independent Learning/Math/Algebra/Linear Algebra#^def-linear-map\|linear map]] $\vec{v}:C^{\infty}(M)\to \mathbb{R}$ is called a **derivation at $p$** if $\vec{v}(fg)=f(p)\vec{v}(g)+g(p)\vec{v}(f)$ for all $f,g\in C^{\infty}(M)$.
>
{ #def-derivation-at-point-manifold}



> [!def] Tangent space at a point
> The set of all [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation-at-point-manifold\|derivations]] at $p\in M$ is called the **tangent space to $M$ at $p$** denoted $T_{p}M$. An element of $T_{p}M$ is called a **tengent vector to $M$ at$p$**.
>
{ #def-tangent-space}



> [!lemma] Properties of Tangent Spaces
> Let $M$ be a smooth manifold with $p\in M$, $\vec{v}\in T_{p}M$ and $f,g\in C^{\infty}(M)$.
> 1. If $f$ is a constant function, then $\vec{v}(f)=0$.
> 2. If $f(p)=g(p)$, then $\vec{v}(fg)=0$.

#### The differential of a smooth map
Recall that for a smooth map $F:\mathbb{R}^{m}\to \mathbb{R}^{n}$ the [[Independent Learning/Math/Analysis/Real Analysis#^def-total-derivative\|total derivative]] (represented by the [[Independent Learning/Math/Analysis/Real Analysis#^prop-total-derivative-matrix-is-Jacobian\|Jacobian Matrix]]) is a linear map that represents the "best linear approximation" to $F$ near the point $a$. What gets brushed aside is the fact that when we write $DF(a):\mathbb{R}^{m}\to \mathbb{R}^{n}$ is **really** a map $DF(a):T_{a}\mathbb{R}^{m}\to T_{F(a)}\mathbb{R}^{n}$. Since we have **canonical** isomorphisms $T_{a}\mathbb{R}^{m}\cong \mathbb{R}^{m}, T_{F(a)}\mathbb{R}^{n}\cong \mathbb{R}^{n}$ (via $(a,\vec{v})\to \vec{v}$) this distinction often gets glossed over. But for a smooth map $F:M\to N$ this distinction becomes important! Therefore, the "best linear approximation" to $F$ near $p\in M$ should be a linear map from $T_{p}M$ to $T_{F(p)}N$. But with no canonical bases for these spaces, we have no canonical identification with $\mathbb{R}^{m}$ or $\mathbb{R}^{n}$. So we need a more intrinsic definition.

> [!def] Differential of a Smooth Map
> Let $M,N$ be smooth manifolds and $F:M \to N$ be a smooth map. For each $p\in M$, define a map $dF_{p}:T_{p}M\to T_{F(p)}M$ as follows: given $\vec{v}\in T_{p}M$, let $dF_{p}(\vec{v})$ be the derivation on $N$ defined as follows, given $f\in C^{\infty}(N)$ we have $dF_{p}(\vec{v})(f):=\vec{v}(f\circ F)$.

Check that $dF_{p}(\vec{v})$ is a derivation on $N$ at $F(p)$.
1. Linearity: Let $f,g\in C^{\infty}(N)$, $a,b\in \mathbb{R}$. Then $dF_{p}(\vec{v})(af+bg)=\vec{v}((af+bg)\circ F)=a\vec{v}(f\circ F)+b\vec{v}(g\circ F)=adF_{p}(\vec{v})(f)+bdF_{p}(\vec{v})(g)$.
2. Product rule: $dF_{p}(\vec{v})(fg)=\vec{v}((fg)\circ F)=\vec{v}((f\circ F)(g\circ F))=(f\circ F)(p)\vec{v}(g\circ F)+(g\circ F)(p)\vec{v}(f\circ F)=f(F(p))dF_{p}(\vec{v})(g)+g(F(p))dF_{p}(\vec{v}(f))$.


> [!prop] Properties of Differentials
> Let $M,N,P$ be smooth manifolds, $F:M\to N$, $G:N\to P$ both smooth and $p\in M$.
> 1. $dF_{p}:T_{p}M\to T_{F(p)}N$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-linear-map\|linear map]].
> 2. $d(G\circ F)_{p}=dG_{F(p)}\circ DF_{p}:T_{p}M \to T_{G(F(p))}P$.
> 3. $d(Id_{M})_{p}=Id_{T_{p}M}$.
> 4. If $F$ is a diffeomorphism, then $d(F^{-1})_{F(p)}=(dF_{p})^{-1}:T_{F(p)}N\to T_{p}M$.

> [!proof]
> Exercise

Local coordinate charts can be used to identify [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry\|tangent spaces]] $T_{p}M$ with Euclidian spaces. But there is a technical issue. [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-tangent-space\|Tangent vectors]] are defined by their actions on functions $f\in C^{\infty}(M)$, but coordinate charts are only defined on open subsets of $M$. Really this isn't a problem because tangent vectors act **locally**.


> [!prop]
> Let $M$ be a smooth manifold, $p\in M$, $\vec{v}\in T_{p}M$. If $f,g\in C^{\infty}(M)$ agree on some open neighborhood of $p$, then $\vec{v}(f)=\vec{v}(g)$.

> [!proof]
> Let $h=f-g$, so $h\in C^{\infty}(M)$ and $h \equiv 0$ on some open neighborhood of $p$. Let $\psi \in C^{\infty}(M)$ be a smooth bump function supported in $M \setminus \{p\}$ and $\equiv 1$ on $\mathrm{supp}~h$. Then, since $\psi=1$ whenever $h\neq0$, we have $\psi h = h$ whenever $h \neq 0$ and since $\psi(p)=h(p)=0$, the lemma implies that $\vec{v}(h)=\vec{v}(\psi h)=0$ therefore $\vec{v}(f)=\vec{g}$.

This means we can identify $T_{p}M$ with the tangent space at $p$ to any open submanifold $U\subseteq M$ with $p\in U$. Specifically,

> [!prop]
> Let $M$ be a smooth manifold, $U\subseteq M$ open, and let $\iota:U\to M$ be the inclusion map. Then for any $p\in M$, $d\iota_{p}:T_{p}U \to T_{p}M$ is an isomorphism.

## Week 6: Notes

### Coordinate Computations
Let $M$ be a smooth manifold and $(U,\varphi)$ a smooth chart. $\varphi$ is a diffeomorphism from $U$ to an open subset $\hat{U}\subseteq \mathbb{R}^{n}$. So for any $p\in U$, the differential $d\varphi_{p}:T_{p}M\to T_{\varphi(p)}\mathbb{R}^{n}$ is an isomorphism. The [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation\|derivations]] $\frac{\partial }{\partial x^{1}} \lvert_{\hat{p}},\ldots, \frac{\partial }{\partial x^{n}}\lvert_{\hat{p}}$ form a (standard) basis for $T_{\hat{p}}\mathbb{R}^{n}$, so their preimages under $d\varphi_{p}$ form a basis for $T_{p}M$. They are denoted by $\frac{\partial }{\partial x^{1}}\lvert_{p},\ldots, \frac{\partial }{\partial x^{n}}\lvert_{p}$. For any $f\in C^{\infty}(M)$, $\frac{\partial }{\partial x^{i}}\lvert_{p}(f)=d\varphi_{p} \left( \frac{\partial }{\partial x^{i}}\lvert_{p} \right)(f\circ \varphi^{-1})= \frac{\partial }{\partial x^{i}}\lvert_{\hat{p}} (f\circ \varphi^{-1})=\frac{\partial \hat{f}}{\partial x^{i}}\lvert_{\hat{p}}$. The vectors $\frac{\partial }{\partial x^{i}}\lvert_{p}\in T_{p}M$ are the *coordinate vectors at $p$ associated to the chart $(U,\varphi)$*. Since they form a basis for $T_{p}M$, any vector $\vec{v}\in T_{p}M$ can be written uniquely as $\vec{v}=v^{i} \frac{\partial }{\partial x^{i}}$ for some $v^{i}\in \mathbb{R}$ called the *components* of $\vec{v}$ with respect to the coordinate basis. If $\vec{v}\in T_{p}M$ is known, we can compute its components from its action on the coordinate functions $x^{i}$:
$$
\vec{v}(x^{i})=v^{j} \frac{\partial }{\partial x^{j}}(x^{i})=v^{j} \delta_{j}^{i}=v^{i}.
$$
### The Differential in Coordinates
Start with the easy case: Let $U\subseteq \mathbb{R}^{n}$ and $V\subseteq \mathbb{R}^{m}$ and $F:U\to V$. For $p\in U$, how do we write the linear map $dF_{p}:T_{p}\mathbb{R}^{n}\to T_{F(p)}\mathbb{R}^{m}$ with respect to the standard coordinate bases? Let $(x^{i})$ be the standard coordinates on $U$ and $(y^{i})$ be the standard coordinates on $V$. Let $p\in U$ and consider the standard basis $\frac{\partial }{\partial x^{i}}\vert_{p}$. To find $dF_{p}\left( \frac{\partial }{\partial x^{i}}\lvert_{p} \right)$ let $f\in C^{\infty}(V)$ then
$$
dF_{p} \left(\frac{\partial }{\partial x^{i}}\lvert_{p} \right)(f)=\frac{\partial }{\partial x^{i}}\lvert_{p} \left( f\circ F \right) = \frac{\partial f}{\partial y^{j}}(F(p)) \frac{\partial F^{j}}{\partial x^{i}}(p) = \frac{\partial F^{j}}{\partial x^{i}}(p) \frac{\partial }{\partial y^{j}}\Bigg\lvert_{F(p)}(f)
$$
So the matrix of $dF_{p}$ with respect to the coordinate bases is the Jacobian matrix. So in this case, $dF_{p}:T_{p}\mathbb{R}^{n}\to T_{F(p)}\mathbb{R}^{m}$ corresponds exactly to the [[Independent Learning/Math/Analysis/Real Analysis#^def-total-derivative\|total derivative]] via the canonical isomorphisms $T_{p}\mathbb{R}^{n}\cong \mathbb{R}^{n}$ and $T_{F(p)}\mathbb{R}^{m}\cong \mathbb{R}^{m}$.

Now let $F:M\to N$ be a smooth map between smooth manifolds. Let $p\in M$ and choose charts $(U,\varphi)$ on $M$ and $(V,\psi)$ on $N$ with $p\in U$, $F(p)\in V$. Let $\hat{p}=\varphi(p)\in \mathbb{R}^{n}$, $\hat{F}:\psi \circ F \circ \varphi^{-1}$. From the previous case, we know that $d \hat{F}_{\hat{p}}:T_{\hat{p}}\mathbb{R}^{n}\to T_{\hat{F}(\hat{p})}\mathbb{R}^{n}$ is represented by the Jacobian matrix of $\hat{F}$ with respect to the standard basis $\frac{\partial }{\partial x^{i}}\lvert_{\hat{p}}$ for $T_{\hat{p}}\mathbb{R}^{m}$ and $\frac{\partial }{\partial y^{i}}\lvert_{\hat{F}(\hat{p})}$ for $T_{\hat{F}(\hat{p})}\mathbb{R}^{m}$. Since $F \circ \varphi^{-1}=\psi^{-1}\circ \hat{F}$ we have 
$$
dF_{p} \left( \frac{\partial }{\partial x^{i}}\Bigg\lvert_{p} \right)=dF_{p} \left( d(\varphi^{-1})_{\hat{p}} \left( \frac{\partial  }{\partial x^{i}} \right)\Bigg\lvert_{\hat{p}} \right) = d(\psi^{-1})_{\hat{F}(\hat{p})}\left( d\hat{F}_{\hat{p}} \left( \frac{\partial }{\partial x^{i}} \Bigg\lvert_{\hat{p}} \right) \right) = d(\psi^{-1})_{\hat{F}(\hat{p})} \left( \frac{\partial \hat{F}^{j}}{\partial x^{i}}(p) \frac{\partial }{\partial y^{j}} \Bigg\lvert_{\hat{F}(\hat{p})} \right) = \frac{\partial \hat{F}^{j}}{\partial x^{i}}(p) d(\psi^{-1})_{\hat{F}(\hat{p})} \left( \frac{\partial }{\partial y^{j}} \Bigg\lvert_{\hat{F}(\hat{p})} \right)= \frac{\partial \hat{F}^{j}}{\partial x^{i}}(p) \frac{\partial }{\partial y^{j}}\Bigg\lvert_{F(p)}
$$
So, in coordinate bases for $T_{p}M$, $T_{F(p)}N$ the map $dF_{p}$ is represented by the Jacobian matrix of the coordinate representation $\hat{F}$ for $F$.


> [!info] Terminology / Notation
> The differential $dF_{p}$ is sometimes called the **tangent map**, the **total derivative**, or simple the **derivative** of $F$ at $p$. It's also called the **pushforward** of $F$, because it "pushes" tangent vectors forward from $M$ to $N$. Notations include: $dF_{p}, F'(p), DF_{p}, F_{\ast}, T_{p}F$.

### Changes of Coordinates
Now, what happens to all of the above when we **change** coordinates? Let $(U,\varphi)$, $(V,\psi)$ be 2 smooth charts on $M$ and $p\in U\cap V$. It's traditional to write the transition function $\psi \circ \varphi^{-1}:\varphi(U\cap V)\to \psi(U\cap V)$ as $\psi \circ \varphi^{-1}(x)=(\tilde{x}^{1}(x),\ldots,\tilde{x}^{n}(x))$. The differential $d(\psi \circ \varphi^{-1})$ acts on the tangent vector $\frac{\partial }{\partial x^{i}}\lvert_{\varphi(p)}$ by $d(\psi \circ \varphi^{-1})_{\varphi(p)} \left( \frac{\partial }{\partial x^{i}} \Big\lvert_{\varphi(p)} \right)= \frac{\partial \tilde{x}^{j}}{\partial x^{i}}(\varphi(p)) \frac{\partial }{\partial \tilde{x}^{j}}\lvert_{\psi(p)}\in T_{\psi(p)}\mathbb{R}^{n}$. From the definition of the coordinate vectors upstairs in $T_{p}M$ we have that
$$
\frac{\partial }{\partial x^{i}}\lvert_{p}=(d\varphi^{-1})_{\varphi(p)} \left( \frac{\partial }{\partial x^{i}}\lvert_{\varphi(p)} \right)=d(\psi^{-1})_{\psi(p)}\circ d(\psi \circ \varphi^{-1})_{\varphi(p)} \left(\frac{\partial }{\partial x^{i}}\lvert_{\varphi(p)} \right)=d(\psi^{-1})_{\psi(p)} \left( \frac{\partial \tilde{x}^{j}}{\partial x^{i}}(\varphi(p)) \frac{\partial }{\partial \tilde{x}^{j}}\lvert_{\psi(p)} \right)= \frac{\partial \tilde{x}^{j}}{\partial x^{i}}(\varphi(p)) \frac{\partial }{\partial \tilde{x}^{j}}\lvert_{p}
$$
which is just the chain rule for partial derivatives. This also lets us see how the components of a tangent vector transform under a change of variables.
$$
\begin{align}
\vec{v}_{p} = v^{i} \frac{\partial }{\partial x^{i}}\Bigg\lvert_{p}=\tilde{v}^{j} \frac{\partial }{\partial \tilde{x}^{j}}\Bigg\lvert_{p} \\
\implies v^{i} \left( \frac{\partial \tilde{x}^{j}}{\partial x^{i}}(\varphi(p)) \right) \frac{\partial }{\partial \tilde{x}^{j}}\Bigg\lvert_{p}=\tilde{v}^{j} \frac{\partial }{\partial \tilde{x}^{j}}\Bigg\lvert_{p} \\
\implies \tilde{v}^{j} = \frac{\partial \tilde{x}^{j}}{\partial x^{i}}(\varphi(p))v^{i}
\end{align}
$$
Thus, to do a change of basis we act the Jacobian from the transition function evaluated at $\varphi(p)$ to our old basis.


> [!example] Change of Basis between cartesian coordinates and polar coordinates
> Let $U\subseteq \mathbb{R}^{2}\setminus \{0\}$ be simply connected. The transition map between cartesian coordinates $(x,y)$ and polar coordinates $(r,\theta)$ is $(x,y)=(r\cos\theta,r\sin \theta)$. Here $x^{1}=r,x^{2}=\theta$ and $\tilde{x}^{1}=x,\tilde{x}^{2}=y$. For $p\in U$ it has polar coordinates $(r,\theta)=(\sqrt{2},\pi / 4)$. Let $\vec{v}\in T_{p}\mathbb{R}^{2}$ have polar coordinate representation $\vec{v}= \frac{\partial }{\partial r}\lvert_{p} + 2 \frac{\partial }{\partial \theta}\lvert_{p}$. Find the Cartesian coordinate representation for $p,\vec{v}$.
>$$
\begin{align}
p: (x,y)=(\sqrt{2} \cos(\pi / 4),\sqrt{2} \sin( \pi / 4))=(1,1). \\
\frac{\partial }{\partial r}\Bigg\lvert_{p} = \cos( \pi / 4) \frac{\partial }{\partial x} + \sin(\pi / 4) \frac{\partial }{\partial y} = \frac{\sqrt{2}}{2} \left( \frac{\partial }{\partial x} + \frac{\partial }{\partial y} \right) \\
\frac{\partial }{\partial \theta}\Bigg\lvert_{p}= -\sqrt{2} \sin( \pi /4) \frac{\partial }{\partial x}\Bigg\lvert_{p} + \sqrt{2} \cos(\pi / 4) \frac{\partial }{\partial y}\Bigg\lvert_{p} = - \frac{\partial }{\partial y}\Bigg\lvert_{p} + \frac{\partial }{\partial y}\Bigg\lvert_{p} \\
\implies \vec{v} = \left( \frac{1-2\sqrt{2}}{\sqrt{2}} \right) \frac{\partial }{\partial x}\Bigg\lvert_{p} + \left( \frac{1+2\sqrt{2}}{\sqrt{2}} \right) \frac{\partial }{\partial y}\Bigg\lvert_{p}
\end{align}
>$$ 
> 

### The Tangent Bundle

> [!def] Tangent Bundle
> Let $M$ be a smooth manifold. The **tangent bundle** $TM$ of $M$ is the **disjoint** union of all the tangent spaces to all points of $M$.
> $$
TM = \bigsqcup_{p\in M} T_{p}M
>$$
>^def-tangent-bundle

Elements may be written as $(p,\vec{v}), \vec{v}_{p}, \vec{v}\in T_{p}M$, etc. The map $\pi: TM \to M$ defined by $\pi(p,\vec{v})=p$ is the **tangent bundle projection map** or **base point map**. For $p\in M$, the set $\pi^{-1}(p)=T_{p}M$ is the **fiber** of $TM$ at $p\in M$. 


> [!prop] $TM$ as a smooth manifold
> The set $TM$ can be given the structure of a smooth manifold of dimension $2n$ (where $\dim M=n$) in a natural way such that $\pi: TM \to M$ becomes a smooth map.

> [!proof]
> Let $(U,\varphi)$ be a chart on $M$. Let $\pi^{-1}(U)\subseteq TM$ be the subset of all tangent vectors to $M$ to all points $p\in U$. Define $\tilde{\varphi}:\pi^{-1}(U)\to \mathbb{R}^{2n}$ as follows: for $p\in U$ and $\vec{v}=v^{i} \frac{\partial }{\partial x^{i}}\lvert_{p}\in T_{p}U$ define
> $$
\tilde{\varphi}(\vec{v}_{p})=(x^{1}(p),\ldots, x^{n}(p),v^{1},\ldots,v^{n}).
>$$
>$\tilde{\varphi}$ is a bijection from $\pi^{-1}(U)$ to $\varphi(U)\times \mathbb{R}^{n}$ with inverse map $\tilde{\varphi}^{-1}(x^{1},\ldots,x^{n},v^{1},\ldots,v^{n})= v^{i} \frac{\partial }{\partial x^{i}}\lvert_{\varphi^{-1}(x)}\in T_{\varphi^{-1}(x)}M$

## Week 7 Notes
FILL IN WEEK 7 Notes

## Week 8 Notes


> [!thm] Constant Rank Level Set Theorem
> Let $\Phi:M^{m}\to N^{n}$ be a smooth map of constant rank $r$. Then every level set of $\Phi$ is a properly embedded, codimension $r$, submanifold of $M$.


> [!cor] Submersion Level Set Theorem
> If $\Phi:M\to N$ is a smooth submersion, then every level set of $\Phi$ is a properly embedded submanifold of $M$ of dimension $\dim M-\dim N$.
>
{ #cor-submersion-level-set-theorem}



So, what about maps of non-constant rank?


> [!def] Regular and Critical Points of a Smooth Map
> Let $\Phi:M\to N$ be a smooth map. A point $p\in M$ is called a **regular point** of $\Phi$ if $d\Phi_{p}:T_{p}M\to T_{\Phi(p)}N$ is surjective, and a **critical point** otherwise. (So every point in $M$ is regular iff $\Phi$ is a submersion).
> 
> From Ch. 4, we know that the subset of $M$ consisting of regular points of $\Phi$ is open in $M$. 
> A point $c\in N$ is a **regular value** of $\Phi$ if every point in $\Phi^{-1}(c)$ is a regular point of $\Phi$. Similarly, $c\in N$ is a **critical value** of $\Phi$ if there exists a critical point of $\Phi$ in $\Phi^{-1}(c)$. A level set $\Phi^{-1}(c)$ is a **regular level set** if $c$ is a regular value.
>
{ #def-regular-and-critical-points-of-a-smooth-map}



> [!thm] Regular Level Set Theorem
> Every [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-regular-and-critical-points-of-a-smooth-map\|regular level set]] of a smooth map between smooth manifolds is a properly embedded submanifold whose codimension is equal to the dimension of the codomain. (a.k.a Implicit Function Theorem for smooth maps).
>
{ #thm-regular-level-set}


> [!proof]
> Let $\Phi:M\to N$ be smooth and $c\in N$ a regular value. The set of regular points $U\subseteq M$ is open in $M$, and $\Phi^{-1}(c)\subseteq U$. Since $\Phi\lvert_{U}:U\to N$ is a smooth submersion, the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^cor-submersion-level-set-theorem\|submersion level set theorem]] implies that $\Phi^{-1}(c)$ is an embedded submanifold of $U$. Since the composition of smooth embeddings $\Phi^{-1}(c)\to U\to M$ is a smooth embedding $\Phi^{-1}(c)$ is an embedded submanifold of $M$, and its a closed submanifold by continuity.


> [!example] Spheres
> Define $f:\mathbb{R}^{n+1}\to \mathbb{R}$ by $f(x^{1},\ldots,x^{n+1})=(x^{1})^{2}+\cdots+(x^{n+1})^{2}$. Critical points:
> $$
> df_{\vec{x}}=[2x^{1},\ldots,2x^{n+1}]
> $$
> Only critical points is $\vec{x}=0$, so the only critical value is $f(0)=0$. So any $a^{2}>0$ is a regular value, so $f^{-1}(a^{2})=\{\text{sphere of radius a}\}$ is a smooth embedded submanifold of $\mathbb{R}^{n+1}$.

Not every smooth embedded submanifold is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-regular-and-critical-points-of-a-smooth-map\|regular level set]] of some smooth map, but *locally* every embedded smooth submanifold has this form.

### Immersed Submanifolds

> [!def] Immersed Submanifolds
> Let $M$ be a smooth manifold. An **immersed submanifold** of $M$ is a subset $S\subset M$ endowed with a topology (not necessarily the subspace topology) with respect to which is a topological manifold, and a smooth structure with respect to which the inclusion map $\iota:S \hookrightarrow M$ is an immersion.
>
{ #def-immersed-submanifolds}


Every embedded submanifold is also an immersed submanifold. By convention, "smooth submanifold" means an immersed submanifold. Some authors defined an immersed submanifold to be the image of *any* immersion, injective or not. Our definition forbids images of non-injective immersions, but it *does* include things like the dense curve of irrational slope on $\mathbb{S}^{1}\times \mathbb{S}^{1}$.

> [!prop] :
> Suppose $M,N$ are smooth manifolds and $F:N\to M$ is an injective smooth immersion. Let $S=F(N)\subset M$. Then $S$ has a unique topology and smooth structure such that it is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-immersed-submanifolds\|smooth submanifold]] of $M$ and such that $F:N\to S$ is a diffeomorphism onto its image.

> [!proof]
> We give $S$ at topology by declaring a subset $U\subseteq S$ to be open iff $F^{-1}(U)$ is open in $N$, and a smooth structure by defining its charts to be $\{(F(U),\varphi\circ F^{-1}):(U,\varphi)\text{ is a chart on N}\}$. This is clearly the unique topology and smooth structure on $S$ that makes $F:N\to S$ a diffeomorphism. Moreover, the inclusion map $\iota:S\hookrightarrow M$ can be written as a composition $S\to N\to M$ via $F\circ F^{-1}$. Since $F^{-1}$ is a diffeo and $F$ is a smooth immersion the composition is also a smooth immersion.


> [!prop] :
> Let $M$ be a smooth manifold, $S\subset M$ an [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-immersed-submanifolds\|immersed submanifold]]. If any of the following holds, then $S$ is embedded:
> 1. $S$ has codimension $0$ in $M$.
> 2. If the inclusion map, $\iota:S\hookrightarrow M$ is proper (a.k.a. the inverse image of compact sets are compact)
> 3. $S$ is compact.

An [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-immersed-submanifolds\|immersed submanifold]] *locally* has the structure of an embedded submanifold.


> [!prop] :
> If $M$ is a smooth manifold, $S\subseteq M$ is an immersed submanifold, then for all $p\in M$ there exists a neighborhood $U$ of $p$ in $S$ that is an embedded submanifold of $M$.

> [!proof]
> We saw in chapter 4 that each $p\in S$ has a neighborhood $U\subseteq S$ such that the inclusion map $\iota:U \hookrightarrow M$ is an embedding.

Immersed submanifolds are often studied using **parameterizations**.


> [!def] Local Parameterization
> Let $S\subset M$ be an immersed $k$-dimensional submanifold. A **local parameterization** of $S$ is a continuous map $X:U\to M$ whose domain is an open subset $U\subseteq \mathbb{R}^{k}$, whose image is an open subset of $S$, and which, when thought of as a map into $S$, is a homeomorphism onto its image (i.e. its needs to be injective). $X$ is a **smooth local parameterization** if it's a diffeomorphism onto $X(U)\subseteq S$
>
{ #def-local-parameterization}



> [!prop] :
> Let $M$ be a smooth manifold, $S\subset M$ an immersed $k$-dimensional submanifold, $\iota:S \hookrightarrow M$ the inclusion map, $U\subseteq \mathbb{R}^{k}$ open. A map $X:U\to M$ is a smooth local parameterization of $S$ if and only if there exists a smooth local coordinate chart $(V,\varphi)$ on $S$ such that $X=\iota\circ \varphi^{-1}$. In particular, every point of $S$ is in the image of some local parameterization.

### Restricting Maps to Submanifolds
Q: Given a smooth map $F:M\to N$ does it remain smooth if the domain or codomain is restricted to a smooth submanifold?


> [!prop] :
> If $F:M\to N$ is a smooth map between smooth manifolds and $S\subseteq M$ is an immersed or embedded submanifold, then $F\lvert_{S}:S\to N$ is smooth.


> [!proof]
> By definition $\iota:S\hookrightarrow M$ is smooth so $F\lvert_{S}=F\circ \iota:S\to N$ is smooth.

Restricting the codomain is slightly more complicated.


> [!prop]
> Let $M,N$ be smooth manifolds, $S\subseteq N$ an immersed submanifold and $F:M\to N$ smooth such that $F(M)\subseteq S$. If $F$ is continuous as a map into $S$, then $F:M\to S$ is smooth.

> [!proof] 
> Book / Exercise


> [!example] How continuity can fail, the "Figure 8 curve"
>  See the book


> [!cor] Embedded Case
> Let $M,N$ be smooth manifolds, $S\subseteq N$ an embedded submanifold. Then every smooth map $F:M\to N$ with $F(M)\subseteq S$ is also a smooth map from $M$ to $S$.

> [!proof]
> Since $S\subseteq M$ has the subspace topology, a continuous map $F:M\to N$ with $F(M)\subseteq S$ is automatically continuous as a map into $S$.

### Extending Functions from Submanifolds
Let $M$ be a smooth manifold, $S\subset M$ a smooth submanifold. There are 2 ways we might define a "smooth function" $f:S\to \mathbb{R}$.
1. A function $f:S\to \mathbb{R}$ that's smooth as a function on the smooth manifold $S$ (i.e. for any chart $(U,\varphi)$ on $S$, $f\circ \varphi^{-1}$ is smooth)
2. As a function $f:S\to \mathbb{R}$ that is smooth on $S$ as a *subset* $S\subseteq M$, i.e. $f$ must admit a smooth extension to a smooth neighborhood in $M$ of each point in $S$.
**Convention:** Notation $f\in C^{\infty}(S)$ means option 1.


> [!lemma] Extension Lemma for Smooth Functions on Submanifolds
> Let $M$ be a smooth manifold, $S\subseteq M$ a smooth submanifold, and $f\in C^{\infty}(S)$.
> 1. If $S$ is embedded, then there exists a neighborhood $U$ of $S$ in $M$ and a smooth function $\tilde{f}\in C^{\infty}(U)$ such that $\tilde{f}\lvert_{S}=f$.
> 2. If $S$ is properly embedded, then the neighborhood $U$ can be taken to be all of $M$.

### Tangent Spaces to Submanifolds
If $S \subseteq M$ is a smooth manifold and $p\in S$, we'd like to think of $T_{p}S$ as a subspace of $T_{p}M$. We can make this precise as follow: Since the inclusion map $\iota:S \hookrightarrow M$ is an immersion, we have that $d \iota_{p}:T_{p}S \to T_{p}M$ is injective. We generally identify $T_{p}S$ with its image $d\iota_{p}(T_{p}S)\subseteq T_{p}M$.


> [!prop]
> Let $M$ be a smooth manifold, $S\subseteq M$ an immersed or embedded submanifold, $p\in S$. A vector $\vec{v}\in T_{p}M$ is in $T_{p}S$ if and only if there exists a smooth curve $\gamma:J\to M$ with $\gamma(J)\subseteq S$, $\gamma$ is smooth as a map into $S$, and $\gamma(0)=p,\gamma'(0)=\vec{v}$.

If $S\subseteq M$ is embedded, we also have the following:

> [!prop] :
> Let $M$ be a smooth manifold, $S\subseteq M$ an embedded submanifold $p\in S$. Then
> $$
> T_{p}S = \{\vec{v}\in T_{p}M: \vec{v}(f)=0\quad \forall f\in C^{\infty}(M), f\lvert_{S}\equiv 0\}.
> $$


> [!def] Defining Map for Embedded Submanifold
> If an embedded submanifold $S\subseteq M$ is a regular level set of a smooth map $\Phi:M\to N$, then $\Phi$ is called the **defining map for $S$**. (If $N=\mathbb{R}^{k}$, then $\Phi$ is called a **defining function** for $S$). If $U\subset M$ is open and $S\cap U$ is a level set of $\Phi:U\to N$, then $\Phi$ is a **local defining map** for $S$.


> [!prop] :
> Let $M$ be a smooth manifold, $S\subseteq M$ an embedded submanifold. If $\Phi:U\to N$ is any local defining map for $S$, then for each point $p\in S\cap U$,
> $$
> T_{p}S = \ker d\Phi_{p}.
> $$


> [!example] :
> Let $M=\mathbb{R}^{2}$ and $S=\{(x,0):x\in \mathbb{R}\}$. Then we have $\Phi(x,y)=y$ where $S=\Phi^{-1}(0)$.


> [!proof]
> Recall that $T_{p}S$ is identified with $d\iota_{p}(T_{p}S)\subseteq T_{p}M$. By hypothesis, $\Phi\circ \iota:S\to N$ is a constant map. So $d\Phi_{p}\circ d\iota_{p}$ is the zero map. But since $S$ is a **regular** level set of $\Phi$, $d\Phi_{p}$ is surjective. So by the rank-nullity theorem, $\dim(\ker d\Phi_{p})=\dim(T_{p}M)-\dim(T_{\Phi(p)}M)=\dim T_{p}S=\dim (Im(d\iota_{p}))$. Since $Im(d\iota_{p})\subseteq \ker d\Phi_{p}$ it follows that $T_{p}S=Im(d\iota_{p})=\ker d\Phi_{p}$.


> [!Cor] :
> If $N=\mathbb{R}^{k}$, $\Phi=(\Phi^{1},\ldots,\Phi^{k}):M\to \mathbb{R}^{k}$ is a smooth submersion, and $S\subseteq M$ is a lvel set of $\Phi$ then a vector $\vec{v}\in T_{p}M$ is in $T_{p}S$ iff $\vec{v}(\Phi^{1})=\cdots=\vec{v}(\Phi^{k})=0$.

## Week 9 of Notes

### Chapter 6: Sard's Theorem (A Glance)


> [!thm] Sard's Theorem
> Let $F:M\to N$ be a smooth map. Then the set of critical values of $F$ has measure zero in $N$.


> [!cor] :
> Let $F:M\to N$ be a smooth map. If $\dim M<\dim N$, then $F(M)$ has measure zero in $N$.


> [!thm] Whitney Embedding Theorem
> Every smooth $n$-dim manifold admits a proper embedding into $\mathbb{R}^{2n+1}$.


> [!thm] Whitney Approximation Theorem for Functions
> Let $M$ be a smooth manifold and $F:M\to \mathbb{R}^{k}$ a continuous function. Given any positive continuous function $\delta:M\to \mathbb{R}$, there exists a smooth function $\tilde{F}:M\to \mathbb{R}^{k}$ with the property that $|F(x)-\tilde{F}(x)|<\delta(x)$ for all $x\in M$. Moreover, if $F$ is smooth on a closed subset $A\subseteq M$, then $\tilde{F}$ can be chosen to agree with $F$ on $A$.

#### Transversality
The intersection of linear subspaces of a vector space is always another linear subspace. But the same is not true for submanifolds in general, unless we add the assumption of **transversality**.


> [!def] Transversality
> Let $M$ be a smooth manifold. Two embedded submanifolds $S,S'\subset M$ **intersect transversely** if $\forall p\in S\cap S'$, the tangent spaces $T_{p}S,T_{p}S'$ together span $T_{P}M$. More generally, if $F:N\to M$ is a smooth map and $S\subset M$ is an embedded submanifold, $F$ is **transverse to $S$** if $\forall x\in F^{-1}(S)$, the spaces $T_{F(x)}S$ and $dF_{x}(T_{x}N)$ together span $T_{F(x)}M$.

Two embedded submanifold intersect transversely if and only if the inclusion map of either one is transverse to the other.


> [!thm] Generalized Regular Level Set Theorem
> Let $M,N$ be smooth manifolds with $S\subset M$ and embedded submanifold.
> 1. If $F:N\to M$ is a smooth map that is transverse to $S$, then $F^{-1}(S)$ is an embedded submanifold of $N$ whose codimension in $N$ is equal to the codimension of $S$ in $M$.
> 2. If $S'\subset M$ is an embedded submanifold of $M$ that intersects $S$ transversely, then $S\cap S'$ is an embedded submanifold of $M$ whose codimension in $M$ is the sum of the codimensions of $S$ and $S'$.

> [!proof]
> Note that 2. follows from 1. by taking $F$ to be the inclusion map of $S'$. Thus to prove 1., let $m=\dim M$, $k=\mathrm{codim}~S$ in $M$. Given $x\in F^{-1}(S)$, we can find a neighborhood $U$ of $F(x)$ in $M$ and a local defining function $\varphi:U\to \mathbb{R}^{k}$ for $S$, i.e., $S\cap U=\varphi^{-1}(0)$. It suffices to show that $0$ is a regular value for $\varphi\circ F$ because $F^{-1}(S)\cap F^{-1}(U)$ is the zero set of $\varphi\circ F$. So, given $\vec{z}\in T_{0}\mathbb{R}^{k}$ and $p\in (\varphi\circ F)^{-1}(0)$ the fact that $0$ is a regular value for $\varphi$ means there exists a vector $\vec{y}\in T_{F(p)}M$ such that $d\varphi_{F(p)}(\vec{y})=\vec{z}$. Since $F$ is transverse to $S$, we can write $\vec{y}=\vec{y}_{0}+dF_{p}(\vec{v})$ for some $\vec{y}_{0}\in T_{F(p)}S$ and $\vec{v}\in T_{p}N$. But $\varphi$ is constant on $S$, so $d\varphi_{F(p)}(\vec{y}_{0})=0$. So $d(\varphi\circ F)_{p}(\vec{v})=d\varphi_{F(p)}(dF_p(\vec{v}))=d\varphi_{F(p)}(\vec{y})=0$. Therefore, $d(\varphi\circ F)_{p}$ is surjective, so $0$ is a regular value of $\varphi\circ F$, as desired.


> [!cor] :
> Let $M,N$ be smooth manifolds, $S\subset M$ an embedded submanifold of codimension $k$, and $F:N\to M$ a submersion. Then $F^{-1}(S)$ is an embedded submanifold of $N$ of codimension $k$.

### Chapter 7: Lie Groups

> [!def] Lie Groups
> A smooth manifold $G$ is called a **Lie group** if it has a group structure such that the multiplication and inverse maps
> $$
> m:G\times G\to G, i:G\to G
> $$
> are both smooth.
>
{ #def-Lie-group}



> [!example] Examples of Lie Groups
> - $\mathbb{R}$ under addition
> - $R^{*}$ under multiplication
> - $S^{1}$ under complex multiplication

The classic example of a Lie group is $GL(n,\mathbb{R})\subseteq M_{n\times n}(\mathbb{R})$ the group of invertible $n\times n$ matrices. *Almost* every finite-dimensional Lie group can be viewed as a subgroup of $GL(n,\mathbb{R})$.


> [!def] Left and Right Translation Maps
> Given a Lie group $G$ and any element $g\in G$, the **left and right translation maps** are:
> $$
> L_{g}:G\to G, R_{g}:G\to G
> $$
> where $L_{g}(h)=gh$ and $R_{g}(h)=hg$.
>
{ #def-left-and-right-translation-maps}


Both $L_{g}$ and $R_{g}$ are smooth as have $\iota_{g}:G\to G\times G$ where $i_{g}(h)=(g,h)$ which we then use multiplication on to get $L_{g}$.


> [!def] Lie Group Homomorphism
> Let $G,H$ be Lie groups, a **Lie group homomorphism** from $G$ to $H$ is a smooth map $f:G\to H$ that is also a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-homomorphism\|group homomorphism]].
>
{ #def-Lie-group-homomorphism}



> [!thm] :
> Every [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group-homomorphism\|Lie group homomorphism]] has constant rank.

> [!proof]
> Let $F:G\to H$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group-homomorphism\|Lie group homomorphism]]. Let $e,\tilde{e}$ denote the identity elements of $G,H$ respectively. Let $g_{0}\in G$ be arbitrary. Since $F$ is a homomorphism for any $g\in G$, we have that $F(L_{g_{0}}(g))=L_{f(g_{0})}(f(g))$ thus $F\circ L_{g_{0}}=L_{F(g_{0})}\circ F$. Taking differentials at $e\in G$,
> $$
> dF_{g_{0}}\circ (dL_{g_{0}})_{e}=d(L_{F(g_{0})})_{\tilde{e}}\circ dF_{e}
> $$
> Since the maps $L_{g_{0}}:G\to G$ and $L_{F(g_{0})}:H\to H$ are diffeomorphisms we have that $(dL_{g_{0}})_{e}$ and $d(L_{F(g_{0})})_{\tilde{e}}$ are isomorphisms. Therefore, $\mathrm{rank}(dF_{g_{0}})=\mathrm{rank}(dF_{e})$.


> [!def] Lie subgroups
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]]. A **Lie subgroup** of $G$ is a subgroup of $G$ of $G$, endowed with a topology and smooth structure making it into a Lie group and an *immersed* submanifold of $G$.
>
{ #def-Lie-subgroup}



> [!prop] :
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]] and $H\subset G$ a subgroup that is also an *embedded* submanifold. Then $H$ is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]].

#### Facts about *open* Lie subgroups
For example, consider $GL^{+}(n,\mathbb{R})\subseteq GL(n,\mathbb{R})$ where $GL^{+}(n,\mathbb{R})$ is the set of invertible matrices with positive determinate. 
- If $H\subseteq G$ is an open subgroup, then $H$ is an embedded [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]]. Moreover, $H$ is actually *closed*, so $H$ is a union of connected components of $G$. Consider $GL^{+}(n,\mathbb{R})$.
- If $W\subset G$ is any neighborhood of $e\in G$ then $W$ generates an open subgroup of $G$. If $W$ is connected, then it generates a connected subgroup of $G$, i.e. it generates the whole connected component containing $e$.
- The connected component of $G$ containing $e$ is called the **identity component** and usually denoted by $G_{0}$ of $G$. $G_{0}$ is a normal subgroup of $G$ and is the only connected open subgroup of $G$. Every connected component of $G$ is diffeomorphic to $G_{0}$.

#### More General Lie Subgroup Facts
Let $F:G\to H$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group-homomorphism\|Lie group homomorphism]]
- $\ker F$ is a properly embedded [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]] of $G$ with codimension equal to the rank of $F$.
- If $F$ is injective, its image has a unique smooth manifold structure such that $F(G)$ is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]] of $H$ and $F:G\to F(G)$ is a Lie group isomorphism.


> [!example]
> - $GL(n,\mathbb{R})^{+}\subset GL(n,\mathbb{R})$ is an open connected subgroup.
> - The **special linear group** $SL(n,\mathbb{R})=\{A\in M_{n\times n}(\mathbb{R}):\det(A)=1\}$. Since $SL(n,\mathbb{R})=\ker(\det:GL(n,\mathbb{R})\to \mathbb{R}^{*})$ it's a properly embedded [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]] of $GL(n,\mathbb{R})$ of codimension $1$
> - The **orthogonal group** $O(n,\mathbb{R})=\{A\in M_{n\times n}(\mathbb{R}):AA^{T}=I\}$. Any matrix $A\in O(n,\mathbb{R})$ must have $\det(A)=\pm 1$ so $O(n,\mathbb{R})\subseteq GL(n,\mathbb{R})$. Further consider the map $\Phi:GL(n,\mathbb{R})\to M_{n\times n}(\mathbb{R})$ by $\Phi(A)=A^{T}A$ has constant rank $\frac{n(n+1)}{2}$. So $\Phi^{-1}(I)$ is a smooth submanifold of $GL(n,\mathbb{R})$ of codimension $\frac{n(n+1)}{2}$. It's also a subgroup, so its a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-subgroup\|Lie subgroup]]. $O(n,\mathbb{R})$ is closed and bounded, so it's compact.
> - **Special orthogonal group**: $SO(n,\mathbb{R})=O(n,\mathbb{R})\cap SL(n,\mathbb{R})$. $SO(n,\mathbb{R})$ is an open subgroup of $O(n,\mathbb{R})$ and its the identity of $O(n,\mathbb{R})$.
>   - As an aside, $O(n,\mathbb{R})$ are rotations and reflections, while $SO(2,\mathbb{R})$ are just rotations.

# Chapter 8: Vector Fields
Vector fields are familiar objects of study in multivariate calculus. Following the theme of generalizing many of the techniques developed in multivariate calculus to smooth manifolds, we can similarly think about vector fields on smooth manifolds by associating to each point $p\in M$ a vector $x_{p}\in T_{p}M$ in some "nice" way. Formally,

> [!def] Vector Field
> Let $M$ be a smooth manifold. A **vector field** on $M$ is a section of $\pi:TM\to M$, i.e. a continuous map $X:M\to TM$, often written $p\mapsto x_{p}$ such that $x_{p}\in T_{p}M$ for all $p\in M$. $X$ is **smooth** if it's smooth as a map from $M$ to $TM$. Any map $X:M\to TM$, continuous or not, with $X(p)\in T_{p}M$, is called a **rough vector field** on $M$.
>
{ #def-vector-field}


Let $(U,(x^{i}))$ be any chart on $M$. Given a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] $X$ on $M$, then there exists functions $x^{1},\ldots,x^{n}:M\to \mathbb{R}$ such that $x_{p}=x^{i}(p) \frac{\partial }{\partial x^{i}}\lvert_{p}$ called the **component functions** of $X$ on the chart.

> [!prop] Smoothness Criterion for Vector Fields
> A [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] $X$ on $M$ is smooth if and only if its component functions with respect to any smooth chart on $M$ are smooth.
>
{ #prop-smoothness-criterion-for-vector-fields}


> [!proof]
> Write $X:M\to TM$ in terms of a chart $(U,\varphi)$ on $M$ and the natural chart corresponding to this $(TU,\tilde{\varphi})$ on $TM$. Then the coordinate representation of $X$ is
> $$
> \hat{X}(x)=(x^{1},\ldots,x^{n},x^{1}(x),\ldots,x^{n}(x))
> $$
> where $x^{i}$ is the $i$-th component function of $X$. Thus, smoothness of $X$ in $U$ is equivalent to smoothness in its component functions. Since smoothness is a local property, we have that $X$ is smooth.

> [!example] Examples of Vector Fields
> 1. **Coordinate vector fields**: If $(U,(x^{i}))$ is a smooth chart on $M$, then the map $p\mapsto \frac{\partial }{\partial x^{i}}\lvert_{p}$ determines a vector field on $U\subset M$ called the **i-th coordinate vector field** on $U$.
> 2. Angle coordinate vector field on $S^{1}$: Let $\theta$ be any angle coordinate chart on an open subset $U\subset S^{1}$, and let $\frac{\partial }{\partial \theta}$ be the coordinate vector field on $U$. Since any other angle coordinate $\tilde{\theta}$ on an open subset $\tilde{U}$ satisfies $\tilde{\theta}=\theta+2n\pi$ for some $n\in \mathbb{Z}$. This implies that $\frac{\partial }{\partial \tilde{\theta}}=\frac{\partial }{\partial \theta}$. So there is a *globally* defined coordinate vector field $\frac{\partial }{\partial \theta}$ on $S^{1}$, even though there's no global angle coordinate $\theta$ on $S^{1}$.


> [!lemma] Extension Lemma:
> Let $M$ be a smooth manifold, $A\subseteq M$ a closed subset, $X:A\to TM$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|smooth vector field]] on $A$. Given any open subset $U$ containing $A$, there exists a smooth global vector field $\tilde{X}$ on $M$ such that $\tilde{X}\lvert_{A}=X$ and $\mathrm{supp}(\tilde{X})\subseteq U$.

> [!proof]
> Exercise

With some of the preliminary results about [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] out of the way its useful to define some notation that we will use in the next section:

> [!info] Notation
> $\mathfrak{X}(M)$ denotes the set of all smooth vector fields on $M$. $\mathfrak{X}(M)$ is a vector space under pointwise addition and scalar multiplication. It is in fact a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-module\|module]] over the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-ring\|ring]] $C^{\infty}(M)$, i.e., if $X\in \mathfrak{X}(M)$ and $f\in C^{\infty}(M)$, then $(fX)_{p}=f(p)X_{p}$.

## Local and Global Frames
Coordinate [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] in a smooth chart are nice: they provide a basis for vector fields on the chart domain. Any $X\in \mathfrak{X}(U)$ has a unique representation $X=X^{i} \frac{\partial }{\partial x^{i}}$ where $X^{i}\in C^{\infty}(M)$. There any many different ways that we understand the components of $X$ at $p$ depending on what basis we assign to $T_{p}M$. 

> [!def] Linearly Independent Vector Fields
> Let $M$ be smooth manifold. $A$ $k$-tuple $(X_{1},\ldots,X_{k})$ of vector fields on a subset $A\subseteq M$ is **linearly independent** if for every point $p\in A$ the vectors $(X_{1}\lvert_{p},\ldots,X_{k}\lvert_{p})$ are linearly independent in $T_{p}M$. Similarly, vector fields $(X_{1},\ldots,X_{k})$ **span $TM$ on $A$** if for all $p\in A$, $((X_{1})\lvert_{p},\ldots,(X_{n})\lvert_{p})$ span $T_{p}M$.
>
{ #def-linearly-independent-vector-fields}


> [!def] Local Frame Field
> A **local frame field** for $M$ is an ordered $n$-tuple of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] $(E_{1},\ldots,E_{n})$ on an open subset $U\subseteq M$ such that for all $p\in U$, $((E_{1})_{p},\ldots,(E_{n})_{p})$ form a **[[Linear Algebra^def-basis\|basis]]** for $T_{p}M$.
>
{ #def-local-frame-field}


> [!def] Global Frame Field
> A [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-local-frame-field\|local frame field]] is a **global frame field** if $U=M$, and a **smooth frame field**, if $E_{1},\ldots,E_{n}$ are smooth vector fields.
>
{ #def-global-frame-field}


While this is a lot of definitions, the idea is as follows: [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] assign to each point on our manifold a tangent vector (i.e. an element of the tangent space), ideally we would like to talk about arbitrary elements in the tangent space in terms of our vector fields, so we use multiple to form a basis for the tangent space at every point (i.e)


> [!prop] Completion of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-local-frame-field\|Local Frames]]
> Let $M$ be a smooth manifold:
> 1. If $(X_{1},\ldots,X_{k})$ $(1\le k\le n)$ is a linearly independent $k$-tuple of smooth vector fields on an open set $U\subseteq M$, then for every point $p\in U$ there exists smooth vector fields $X_{k+1},\ldots,X_{n}$ on a neighborhood $V$ of $p$ such that $(X_{1},\ldots,X_{n})$ is a smooth local frame field on $V\cap U$.
> 2. If $(\vec{v}_{1},\ldots,\vec{v}_{k})$ in $T_{p}M$ are linearly independent then there exists a smooth local frame field $(X_{1},\ldots,X_{n})$ on some neighborhood of $p$ such that $(X_{i})_{p}=\vec{v}_{i}$ for $1\le i \le k$.

For subsets (and submanifolds) $A\subseteq \mathbb{R}^{n}$ it's often useful for geometric problems to work with **orthonormal** frame fields, i.e., frame fields $(E_{1},\ldots,E_{n})$ such that for all $p\in A$ we have that $((E_{1})_{p},\ldots(E_{n})_{p})$ form an orthonormal basis for $T_{p}\mathbb{R}^{n}$.

> [!example]
> On $\mathbb{R}\setminus \{0\}$ define
> $$
 \begin{align}
> E_{1}&= \frac{1}{\sqrt{x^{2}+y^{2}}} \left(x \frac{\partial }{\partial x}+y \frac{\partial }{\partial y} \right), \\
> E_{2}&= \frac{1}{\sqrt{x^{2}+y^{2}}} \left(-y \frac{\partial }{\partial x}+x \frac{\partial }{\partial y} \right)
 \end{align}
> $$


> [!lemma] Gram-Schmidt for Frame Fields
> Suppose $(X_{1},\ldots,X_{n})$ is any smooth local frame field on an open set $U\subseteq \mathbb{R}^{n}$. Then there exists a smooth orthonormal frame field $(E_{1},\ldots,E_{n})$ on $U$ such that for $j=1,\ldots,n$ $\mathrm{span}((E_{1})_{p},\ldots,(E_{n})_{p})=\mathrm{span}((X_{1})_{p},\ldots,(X_{n})_{p})$


> [!def] Parallelizable
> A smooth manifold is **parallelizable** if it admits a smooth [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-global-frame-field\|global frame field]].
>^def-parallelizable

> [!example] Examples of Parallelizable Manifolds
> - $\mathbb{R}^{n}$
> - $\mathbb{S}^{1}$
> - $T^{n}$
> - All Lie groups

## [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|Vector Fields]] as [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation\|derivations]] on $C^{\infty}(M)$
A [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] $X\in \mathfrak{X}(M)$ defines a map by $X:C^{\infty}(M)\to C^{\infty}(M)$ defined by $X(f)(p)=X_{p}(f)$.

> [!prop] :
> Let $M$ be a smooth manifold, $X:M\to TM$ a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|rough vector field]] on $M$. TFAE:
> 1. $X$ is smooth
> 2. $\forall f\in C^{\infty}(M)$, the function $X(f)$ is smooth on $M$.
> 3. $\forall$ open sets $U\subseteq M$, $f\in C^{\infty}(U)$, we have that $X(f)$ is smooth on $U$.

If $X\in \mathfrak{X}(M)$, the map $X:C^{\infty}(M)\to C^{\infty}(M)$ is clearly linear over $\mathbb{R}$, and it satisfies a product rule:
$$
X(fg)=gX(f)+fX(g).
$$
A map $X:C^{\infty}(M)\to C^{\infty}(M)$ with these properties is called a **derivation**.

> [!prop] :
> A map $D:C^{\infty}(M)\to C^{\infty}(M)$ is a derivation if and only if it has the form $D(f)=X(f)$ for some smooth vector field $X\in \mathfrak{X}(M)$.



## [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|Vector Fields]] and Smooth Maps
Let $F:M\to N$ be a smooth map and $X\in \mathfrak{X}(M)$. For each point $p\in M$, the differential of $F$ defines a vector $dF_{p}(X_{p})\in T_{F(p)}N$. But this generally does **not** define a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] on $N$.
- If $F$ is not surjective it does not determine vectors at points of $N\setminus F(M)$.
- If $F$ is not injective it may assign more than one vector to some points of $N$.
*If* there is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] $Y\in \mathfrak{X}(N)$ such that for all $p\in M$, $dF_{p}(X_{p})=Y_{p}$ then we say that $X$ and $Y$ are **$F$-related**.


> [!prop] :
> Let $F:M\to N$ be a smooth map, $X\in \mathfrak{X}(M)$ and $Y\in \mathfrak{X}(M)$. Then $X$ and $Y$ are $F$-related if and only if for any smooth function $f$ defined on an open subset of $N$, we have that $X(f\circ F)=(Y(f))\circ F$.

> [!proof]
> Let $p\in M$, $F(p)\in V \subset N$ open, and $f:V\to \mathbb{R}$ smooth. Evaluate both sides and $p$,
> $$
> X(f\circ F)(p)=X_{p}(f\circ F)=dF_{p}(X_{p})(f)
> $$




> [!prop] :
> If $F:M\to N$ is a diffeomorphism, then $\forall X\in \mathfrak{X}(M)$ there exists a *unique* $Y\in \mathfrak{X}(N)$ that is $F$-related to $X$.

> [!proof]
> Such a $Y$ must satisfy, $\forall p\in M$ $dF_{p}(X_{p})=Y_{F(p)}$. So for any $q\in N$, set $Y_{q}=dF_{F^{-1}(q)}(X_{F^{-1}(q)})$. $Y:N\to TN$ is smooth because it's a composition: $dF\circ \mathfrak{X}\circ F^{-1}$.

When $F$ is a diffeomorphism, $Y$ is called the **pushforward** of $X$, denoted $F_{*}(X)$.

> [!note] Note:
> If $S\subset M$ is smooth submanifold and $X\in \mathfrak{X}(M)$, $X$ may or may not restrict to be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] on $S$, depending on whether $X_{p}\in T_{p}S$ for all $p\in S$.

## Lie Brackets
The Lie bracket is a way of putting a product structure on $\mathfrak{X}(M)$. How to do this is not obvious. Simple compositions don't work because if $X,Y\in \mathfrak{X}(M)$ the operator that maps $f\mapsto Y(X(f))$ doesn't satisfy the product rule and is thus not a derivation. Intuitively, the problem is that $X$ and $Y$ are 1st-order differential operators so $Y\circ X:C^{\infty}(M)\to C^{\infty}(M)$ is a 2nd-order differential operator, so it's not a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]].


> [!def] Lie Bracket
> Given $X,Y\in \mathfrak{X}(M)$, the **Lie bracket** of $X$ and $Y$ is the operator:
> $$
> [X,Y](f)=X(Y(f))-Y(X(f))
> $$
>
{ #def-Lie-bracket}



> [!lemma] Closure of the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-bracket\|Lie bracket]]
> For all $X,Y\in \mathfrak{X}(M)$, $[X,Y]\in \mathfrak{X}(M)$.


> [!proof]
> Need to show that $[X,Y]$ is a derivation on $M$. Linearity is easy, so check the product rule:
> $$
> [X,Y](fg)=X(Y(fg))-Y(X(fg)) = X(f(Y))
> $$





> [!prop] Coordinate Formula for [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-bracket\|Lie bracket]]
> Let $X,Y\in \mathfrak{X}(M)$ and suppose that the coordinate representations for $X,Y$ in some chart $(U,(x^{i}))$ is $X=X^{i}(x) \frac{\partial }{\partial x^{i}}$, $Y=Y^{j}(x) \frac{\partial }{\partial x^{j}}$ then
> $$
> [X,Y] = \left(X^{i} \frac{\partial Y^{j}}{\partial x^{i}}-Y^{i} \frac{\partial X^{j}}{\partial x^{i}} \right) \frac{\partial }{\partial x^{j}}
> $$


> [!proof]
> If $f\in C^{\infty}(M)$, then
> $$
> \begin{align}
> [X,Y](f)&=X(Y(f))-Y(X(f)) = X^{i} \frac{\partial }{\partial x^{i}} \left( Y^{j} \frac{\partial f}{\partial x^{j}} \right)- Y^{j} \frac{\partial }{\partial x^{j}} \left( X^{i} \frac{\partial f}{\partial x^{i}} \right)\\ \\
  &= X^{i} \frac{\partial Y^{j}}{\partial x^{i}} \frac{\partial f}{\partial x^{j}}-Y^{j} \frac{\partial X^{i}}{\partial x^{j}} \frac{\partial f}{\partial x^{i}}
> \end{align}
> $$


> [!corollary] :
> The coordinate vector fields associated to any chart have the property that
> $$
> \left[ \frac{\partial }{\partial x^{i}}, \frac{\partial }{\partial x^{j}} \right]=0.
> $$
> That is, partial derivatives commute.


> [!prop] :
> For all $X,Y,Z \in \mathfrak{X}(M)$, the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-bracket\|Lie bracket]] satisfies
> 1. Bilinearity: for all $a,b\in \mathbb{R}$ , $[aX+bY,Z]=a[X,Z]+b[Y,Z]$
> 2. Antisymmetry: $[X,Y]=-[Y,X]$
> 3. Jacobi Identity:
> $$
> [X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0
> $$
> 4. For all $f,g\in C^{\infty}(M)$ we have that
> $$
> [fX,gY]= fg[X,Y]+fX(g)Y-gY(f)X
> $$


> [!proof] Exercise


> [!prop] :
> Let $F:M\to N$ be smooth and let $X_{1},X_{2}\in \mathfrak{X}(M)$, $Y_{1},Y_{2}\in \mathfrak{X}(N)$ with $X_{i}$ $F$-related to $Y_{i}$ for $i=1,2$. Then $[X_{1},X_{2}]$ if $F$-related to $[Y_{1},Y_{2}]$.


> [!proof] Exercise


> [!corollary] :
> If $F:M\to N$ is a diffeomorphism and $X_{1},X_{2}\in \mathfrak{X}(M)$ then $F_{*}([X_{1},X_{2}])=[F_{*}(X_{1}),F_{*}(X_{2})]$.


> [!corollary] :
> Let $M$ be a smooth manifold, and $S\subset M$ and immersed submanifold. If $Y_{1},Y_{2}\in \mathfrak{X}(M)$ are smooth vector fields on $M$ that are tangent to $S$. Then $[Y_{1},Y_{2}]$ is also tangent to $S$.


> [!proof]
> Let $X_{1},X_{2}$ be vector fields on $S$ that are $\iota$-related to $Y_{1},Y_{2}$. By the proposition above, $[X_{1},X_{2}]$ is $\iota$-related to $[Y_{1},Y_{2}]$, so $[Y_{1},Y_{2}]$ is tangent to $S$.


## The Lie Algebra of a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie Group]]
Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]]. $G$ acts on itself by left translation: for any $g\in G$, $L_{g}:G\to G$ is defined by $L_{g}(h)=gh$.


> [!def] Left-Invariant Vector Field
> A [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector field]] $X$ on $G$ is **left-invariant** if for all $g,h\in G$: $(dL_{g})_{h}(X_{h})=X_{gh}$. Often this is written more concisely as $(L_{g})_{*}(X)=X$ for all $g\in G$.
>
{ #def-left-invariant-vector-field}


Since $(L_{g})_{*}:\mathfrak{X}(G)\to \mathfrak{X}(G)$ is a linear map, the set of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|left-invariant vector fields]] on $G$ is closed under addition and scalar multiplication, so it's a vector space.


> [!prop] The [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|Left-Invariant Vector Fields]] are closed under the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-bracket\|Lie bracket]]
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]], $X,Y\in \mathfrak{X}(G)$ smooth, [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|left-invariant vector fields]] on $G$. Then $[X,Y]$ is also left-invariant.


> [!proof]
> Let $g\in G$. Then $(L_{g})_{*}([X,Y])=[(L_{g})_{*}(X),(L_{g})_{*}(Y)]=[X,Y]$.

This leads to a new object known as a **Lie algebra**

<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/independent-learning/math/algebra/lie-algebras/#def-lie-algebra" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">

<div class="markdown-embed-title">

# Lie Algebra

</div>


> [!def] Lie Algebra
> A **Lie algebra** over a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] space $\mathfrak{g}$ together with a bilinear map called the **Lie bracket** $[\cdot,\cdot]:\mathfrak{g}\times \mathfrak{g} \to \mathfrak{g}$ which satisfies the following properties:
> 1. $[x,x]=0$ for all $x\in \mathfrak{g}$,
> 2. $[x,[y,z]]+[z,[x,y]]+[y,[z,x]]=0$ for all $x,y,z\in \mathfrak{g}$.
>
> Where the last condition is know as the **Jacobi identity**. Note: the Lie bracket is also referred to as the **commutator** of $x$ and $y$.
> 

</div></div>
^def-Lie-algebra



<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/independent-learning/math/algebra/lie-algebras/#def-lie-subalgebras" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



> [!def] Lie Subalgebras
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] and $\mathfrak{h}\subset \mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-subspace\|vector subspace]] of $\mathfrak{g}$. Then $\mathfrak{h}$ is a **Lie subalgebra** of $\mathfrak{g}$ if $(\mathfrak{h},[\cdot,\cdot])$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] (i.e. $\mathfrak{h}$ is closed under the Lie bracket).
>

</div></div>



<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/independent-learning/math/algebra/lie-algebras/#def-lie-algebra-homomorphism" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">




# Overview

> [!question] What are Lie Algebras?
> Lie algebras are one of one of those seemingly nebulous topics that seem all important yet you have no idea what they are until you do.

> [!abstract] Summary of Topics Covered
> Contents

> [!info] Recommended Readings and Resources
> These notes will largely follow the following two texts:
> 1. ["Introduction to Lie Algebras" by Erdmann and Wildon](https://link.springer.com/book/10.1007/1-84628-490-2)
> 2. ["Lie Algebras of Finite and Affine Type" by Carter](https://www.cambridge.org/core/books/lie-algebras-of-finite-and-affine-type/4E6820728C16DC1F812860C974FBB4F6)
>    [Course details](https://euclid.colorado.edu/~thiemn/PastCourses/8174F10/course.html)
> 3. ["Introduction to Lie Algebras and Representation Theory" by Humphreys](https://link.springer.com/book/10.1007/978-1-4612-6398-2)


> [!warning] Recommended / Assumed Prerequisite Topics
> This note assumes working knowledge of the following topics:
> - [[Independent Learning/Math/Algebra/Abstract Algebra\|Abstract Algebra]]
>
> While the text is written to minimize the required background information, at some points, it will be unavoidable. Hopefully, by following a sufficient number of links one can fill these gaps and return to the topic at a later point.

> [!danger] Disclaimer
> These notes largely follow ["Introduction to Lie Algebras" by Erdmann and Wildon](https://link.springer.com/book/10.1007/1-84628-490-2), but are originally taken as apart of a special topics class in abstract algebra taught by Flor (fill this in once course is done).

# Chapter 1: Introduction
## 1.1 Examples and Basics of Lie Algebras
As with many topics in algebra or more generally mathematics there is
1. A fundamental object in question (Lie algebras)
2. Different kinds of "structure preserving maps" (morphisms)
3. Properties that allow one to classify the fundamental objects in question (simple, nilpotent, semi-simple, etc.)
This will effectively be the guiding motivations for these notes. However, there will be an additional emphasis placed on examples in quantum physics which is incredibly rich and will likely shed a new light on topics covered in a typical advanced undergraduate or introductory graduate course on quantum mechanics. For instance, the canonical commutation relation $[x,p]=i\hbar$ is a statement about the Weyl-Heisenberg algebra which underpins the formalism utilized for bosonic, fermionic, or qudit systems in quantum information theory. Similarly, the angular momentum algebra unpins spin $j$ systems and the Clebsch-Gordan coefficients. These are systems that posses a vector space structure and a *bracket* $[\cdot,\cdot]$ which obey certain properties making them a **Lie algebra**:

> [!def] Lie Algebra
> A **Lie algebra** over a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] space $\mathfrak{g}$ together with a bilinear map called the **Lie bracket** $[\cdot,\cdot]:\mathfrak{g}\times \mathfrak{g} \to \mathfrak{g}$ which satisfies the following properties:
> 1. $[x,x]=0$ for all $x\in \mathfrak{g}$,
> 2. $[x,[y,z]]+[z,[x,y]]+[y,[z,x]]=0$ for all $x,y,z\in \mathfrak{g}$.
>
> Where the last condition is know as the **Jacobi identity**. Note: the Lie bracket is also referred to as the **commutator** of $x$ and $y$.
> ^def-lie-algebra

> [!prop] Antisymmetry of Lie Bracket
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]], then $[x,y]=-[y,x]$ for all $x,y\in \mathfrak{g}$.
> ^prop-antisymmetry-of-Lie-bracket

> [!proof]-
> Let $x,y\in \mathfrak{g}$, then from the properties of the [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] we have that
> $
[x+y,x+y]=0=[x,x]+[y,y]+[x,y]+[y,x] \implies [x,y]=-[y,x].
>$

One of the simplest examples of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] which we all have encountered is the **general linear group** $\mathfrak{gl}_{n}(\mathbb{F})$:

> [!def] General Linear Group $\mathfrak{gl}_{n}(\mathbb{F})$ and General Linear Algebra $\mathfrak{gl}(V)$
> **The general linear group** $\mathfrak{gl}_{n}(\mathbb{F})$ as a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] formed from the [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] of $n\times n$ matrices over the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ with the [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] defined as $[A,B]=AB-BA$ for $A,B\in \mathfrak{gl}_{n}(\mathbb{F})$. **The general linear algebra** $\mathfrak{gl}(V)$ is the set of all [[Independent Learning/Math/Algebra/Linear Algebra#^def-linear-map\|linear maps]] from the finite-dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] $V$ over $\mathbb{F}$ to itself. $\mathfrak{gl}(V)$ is again a vector space over $\mathbb{F}$ and further a Lie algebra when equipped with the Lie bracket $[x,y]=x\circ y-y\circ x$ for $x,y\in \mathfrak{gl}(V)$ where $\circ$ denotes the composition of maps. Note that $\mathfrak{gl}_{n}(\mathbb{F})=\mathfrak{gl}(\mathbb{F}^{n})$.
> ^def-general-linear-group-and-general-linear-algebra

> [!exercise] Prove that $\mathfrak{gl}_{n}(\mathbb{F})$ and $\mathfrak{gl}(V)$ are [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]].

As a vector space, $\mathfrak{gl}_{n}(\mathbb{F})$ has a basis consisting of the *elementary matrices* $e_{ij}$ which have a $1$ in the $(i,j)$-th entry and zero else where for $1\le i,j \le n$. A useful property to note is that $[e_{ij},e_{kl}]=\delta_{jk}e_{il}-\delta_{il}e_{kj}$ where $\delta$ is the Kronecker delta defined by $\delta_{ij}=1$ if $i=j$ and zero otherwise.

In general, if $\mathcal{A}$ is an associative algebra then $(\mathcal{A},[\cdot,\cdot])$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] where $[a,b]=ab-ba$ is the **commutator**. The intuition for the name is due to $a$ and $b$ commuting when $[a,b]=0=ab-ba \implies ab=ba$. So in some sense the commutator gives the left over-piece for two elements to commute. Note that in general, the **[[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] is not the commutator**, however most cases it will be. Any [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] $V$ can be made a Lie algebra by defining $[x,y]=0$ for all $x,y\in V$ which is an *abelian* Lie algebra structure, i.e. the Lie bracket vanishes for all elements. Some other examples of Lie algebras are given below:

> [!def] Special Linear Group $\mathfrak{sl}_{n}(\mathbb{F})$
> **The special linear group** $\mathfrak{sl}_{n}(\mathbb{F})=\{A\in M_{n\times n}(\mathbb{F}):\mathrm{Tr}\left[ A \right]=0\}$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] formed from the [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] of $n\times n$ matrices over the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ with the [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] defined as $[A,B]=AB-BA$ for $A,B\in \mathfrak{sl}_{n}(\mathbb{F})$.
> ^def-special-linear-group

> [!exercise]- Prove that $\mathfrak{sl}_{n}(\mathbb{F})$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]].
> First, $\mathfrak{sl}_{n}(\mathbb{F})$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] as for $a,b\in \mathbb{F}$ and $x,y\in \mathfrak{sl}_{n}(\mathbb{F})$ we have that $\mathrm{Tr}\left[ ax+by \right]=a\mathrm{Tr}\left[ x \right]+b\mathrm{Tr}\left[ y \right]=0$. To see that it is a Lie algebra, observe that we have closure under the Lie bracket as $\mathrm{Tr}\left[ xy-yx \right]=\mathrm{Tr}\left[ xy \right]-\mathrm{Tr}\left[ yx \right]=\mathrm{Tr}\left[ xy \right]-\mathrm{Tr}\left[ xy \right]=0$ for all $x,y\in \mathfrak{sl}_{n}(\mathbb{F})$ by the cyclicity of the trace. Further, the fact that $\mathfrak{gl}_{n}(\mathbb{F})$ is a Lie algebra demonstrates that $\mathfrak{sl}_{n}(\mathbb{F})$ is a Lie algebra since the Jacobi identity didn't rely on the trace being non-zero.

As a vector space, $\mathfrak{sl}_{n}(\mathbb{F})$ has a basis given by $e_{ij}$ for $i\neq j$ together with $e_{ii}-e_{i+1,i+1}$ for $1\le i <n$.

> [!def] Upper Triangular Group $\mathfrak{t}_{n}(\mathbb{F})$
> **The upper triangular group $\mathfrak{t}_{n}(\mathbb{F})=\{A\in M_{n\times n}(\mathbb{F}):A_{ij}=0 \text{ if } i>j\}$** is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] formed from the [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] of upper triangular matrices over the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ with the [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] defined as $[A,B]=AB-BA$ for $A,B\in \mathfrak{t}_{n}(\mathbb{F})$.
> ^def-upper-triangular-group

> [!exercise] Prove that $\mathfrak{t}_{n}(\mathbb{F})$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]].

> [!def] Strictly Upper Triangular Group $\mathfrak{u}_{n}(\mathbb{F})$
> **The strictly upper triangular group $\mathfrak{u}_{n}(\mathbb{F})=\{A\in M_{n\times n}(\mathbb{F}):A_{ij}=0 \text{ if } i\ge j\}$** is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] formed from the [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] of strictly upper triangular matrices over the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ with the [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie bracket]] defined as $[A,B]=AB-BA$ for $A,B\in \mathfrak{t}_{n}(\mathbb{F})$.
> ^def-strictly-upper-triangular-group

> [!exercise] Prove that $\mathfrak{u}_{n}(\mathbb{F})$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]]

## 1.2 Subalgebras, Ideals, and Homomorphisms
In the previous section we were introduced to examples of typical matrix [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]]: $\mathfrak{gl}_{n}(\mathbb{F}),\mathfrak{sl}_{n}(\mathbb{F}),\mathfrak{t}_{n}(\mathbb{F}),\mathfrak{u}_{n}(\mathbb{F})$. However, you might have noticed that proving $\mathfrak{sl}_{n}(\mathbb{F}),\mathfrak{t}_{n}(\mathbb{F}),$ or $\mathfrak{u}_{n}(\mathbb{F})$ are Lie algebras follows from the fact that $\mathfrak{gl}_{n}(\mathbb{F})$ is a Lie algebra once we have show that each are closed under the Lie bracket. This is because each is a **Lie subalgebra** of $\mathfrak{gl}_{n}(\mathbb{F})$:

> [!def] Lie Subalgebras
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] and $\mathfrak{h}\subset \mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-subspace\|vector subspace]] of $\mathfrak{g}$. Then $\mathfrak{h}$ is a **Lie subalgebra** of $\mathfrak{g}$ if $(\mathfrak{h},[\cdot,\cdot])$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] (i.e. $\mathfrak{h}$ is closed under the Lie bracket).
>^def-lie-subalgebras

An important example of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|Lie subalgebra]] is an **ideal:**

> [!def] Ideal of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]]
> An **ideal** $\mathcal{I}\subset\mathfrak{g}$ of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] $\mathfrak{g}$, is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-subspace\|vector space]] such that $[\mathcal{I},\mathfrak{g}]\subset \mathcal{I}$ or $\{[y,x]:y\in \mathcal{I},x\in \mathfrak{g}\}\subset \mathcal{I}$.
> ^def-ideal-of-a-lie-algebra

Its worth noting that all ideals of a Lie algebra are two sided, i.e. $[\mathcal{I},\mathfrak{g}]=[\mathfrak{g},\mathcal{I}]$ due to the [[Independent Learning/Math/Algebra/Lie Algebras#^prop-antisymmetry-of-Lie-bracket\|antisymmetry of the Lie bracket]].

> [!example] [[Independent Learning/Math/Algebra/Lie Algebras#^def-special-linear-group\|$\mathfrak{sl}_n(\mathbb{F})$]] is an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] of [[Independent Learning/Math/Algebra/Lie Algebras#^def-general-linear-group-and-general-linear-algebra\|$\mathfrak{gl}_n(\mathbb{F})$]]
> To see that $\mathfrak{sl}_{n}(\mathbb{F})$ is an ideal of $\mathfrak{gl}_{n}(\mathbb{F})$, observe that the commutator of two matrices always has trace zero due to the cyclicity of the trace. Therefore, $[\mathfrak{sl}_{n}(\mathbb{F}),\mathfrak{gl}_{n}(\mathbb{F})]\subset [\mathfrak{gl}_{n}(\mathbb{F}),\mathfrak{gl}_{n}(\mathbb{F})]\subset \mathfrak{sl}_{n}(\mathbb{F})$.

An important example of an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] is the **center of a Lie algebra:**

> [!def] Center of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie Algebra]]
> The **center** of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] $\mathfrak{g}$ is $\mathcal{Z}(\mathfrak{g})=\{x\in\mathfrak{g}: [x,y]=0, \forall y\in \mathfrak{g}\}$.
> ^def-center-of-a-lie-algebra

Notably, a Lie algebra $\mathfrak{g}$ is abelian if and only if $\mathfrak{g}=\mathcal{Z}(\mathfrak{g})$. Just as [[Independent Learning/Math/Algebra/Abstract Algebra#^def-homomorphism\|group homomorphisms]] preserve the [[Independent Learning/Math/Algebra/Abstract Algebra#^def-group\|group structure]], here **Lie algebra homomorphisms** preserve the bracket structure:

> [!def] Lie Algebra Homomorphism
> A [[Independent Learning/Math/Algebra/Linear Algebra#^def-linear-map\|linear map]] $\varphi:\mathfrak{g}\to \mathfrak{h}$ between [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]] $\mathfrak{g}$ and $\mathfrak{h}$ is a **Lie algebra homomorphism** if $\varphi([X,Y])=[\varphi(X),\varphi(Y)]$.
> ^def-Lie-algebra-homomorphism

Just as with standard algebraic objects, a [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie algebra homomorphism]] $\varphi$ is called an **isomorphism** if $\varphi$ is also bijective. Importantly we have the following properties of [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie algebra homomorphisms]]:

> [!prop] Kernel and Image of [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie Algebra Homomorphisms]]
> Let $\varphi:\mathfrak{g} \to \mathfrak{h}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie algebra homomorphism]]. Then $\ker \varphi=\{\varphi(g)=0:g\in \mathfrak{g}\}$ is an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] of $\mathfrak{g}$ and $\mathrm{Im}~\varphi=\varphi(\mathfrak{g})$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|Lie subalgebra]] of $\mathfrak{h}$.
> ^prop-kernel-and-image-of-Lie-algebra-hom

> [!proof]
> Exercise

In the next section we'll prove the analogous isomorphism theorems for Lie algebras that holds for many of our favorite algebraic objects. One of the most important homomorphisms which we will constantly study is the **adjoint homomorphism:**

> [!def] Adjoint Homomorphism
> The **adjoint homomorphism** is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie algebra homomorphism]] $ad:\mathfrak{g}\to \mathfrak{gl}(\mathfrak{g})$ from a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] $\mathfrak{g}$ to its [[Independent Learning/Math/Algebra/Lie Algebras#^def-general-linear-group-and-general-linear-algebra\|general linear algebra]] $\mathfrak{gl}(\mathfrak{g})$ defined by $(ad~x)(y):=[x,y]$ for all $x,y\in \mathfrak{g}$.
> ^def-adjoint-homomorphism

> [!exercise] Prove that the [[Independent Learning/Math/Algebra/Lie Algebras#^def-adjoint-homomorphism\|adjoint homomorphism]] is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-Lie-algebra-homomorphism\|Lie algebra homomorphism]] and that $\ker ad=\mathcal{Z}(\mathfrak{g})$.

## 1.3 Derivations and Structure Constants
While not of central importance for our discussion of the theory of Lie algebras, **derivations** can be seen as one of the primary reasons for studying Lie algebras as they appear in [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry\|Differential Geometry]]. First we need to define what an **algebra** is

> [!def] Algebras
> An **algebra** over a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|field]] $\mathbb{F}$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] $A$ together with a bilinear map $\cdot:A\times A\to A$, $x\cdot y=xy$, called the **product** of $x,y\in A$. $A$ is said to be associative if $(xy)z=x(yz)$ for all $x,y,z\in A$ and **unital** if there is an element $1\in A$ such that $1x=x=x1$ for all $x\neq 0$.
> ^def-algebra

[[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]] are examples of [[Independent Learning/Math/Algebra/Lie Algebras#^def-algebra\|algebras]] and algebras are one of the central objects of interest in algebra (go figure). An interesting example of an algebra (and the motivation for derivations) is the vector space of all infinitely differentiable functions from $\mathbb{R}\to \mathbb{R}$ denoted $C^{\infty}(\mathbb{R})$ with the product of $f,g\in C^{\infty}(\mathbb{R})$ defined as the pointwise multiplication $fg\in C^{\infty}(\mathbb{R})$. In $C^{\infty}(\mathbb{R})$ we also can take **derivatives** $\frac{d}{dx}:C^{\infty}(\mathbb{R})\to C^{\infty}(\mathbb{R})$ which are linear maps satisfying the **Leibniz rule** where for $f,g\in C^{\infty}(\mathbb{R})$ we have that $\frac{d}{dx}(fg)=f'g+fg'$. From this, one can derive many of the algebraic properties of derivatives such as $\frac{d}{dx}c=0$ for $c\in \mathbb{R}$, power rule, chain rule, etc. The properties of derivates can be abstracted to algebras via **derivations:**

> [!def] Derivations
> Let $A$ be an [[Independent Learning/Math/Algebra/Lie Algebras#^def-algebra\|algebra]] over a [[Independent Learning/Math/Algebra/Abstract Algebra#^def-field\|Field]] $\mathbb{F}$. A **derivation** of $A$ is an $\mathbb{F}$-linear map $D:A\to A$ satisfying the Leibniz rule: $D(ab)=D(a)b+aD(b)$ for all $a,b\in A$. The set of all derivations on $A$ is denoted $Der(A)\subset \mathfrak{gl}(A)$ which is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|Lie subalgebra]] of $\mathfrak{gl}(A)$.
> ^def-derivations




Namely, the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-tangent-space\|tangent space]] $T_{p}M$ at a point $p$ in a [[Differential Geometry^def-smooth-manifold\|smooth manifold]] $M$ is the [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] of all [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-derivation-at-point-manifold\|derivations at $p$]] on $C^{\infty}(M)$. Further, [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] are derivations on 

## 1.R Recap
## 1.E Exercises

# Chapter 2: Ideals and Homomorphisms


> [!def] Derived Algebra
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]]. The **derived algebra** of $\mathfrak{g}$ is $D(\mathfrak{g})=[\mathfrak{g},\mathfrak{g}]=\{[x,y]:x,y\in \mathfrak{g}\}$.
> ^def-derived-algebra


> [!prop] Quotient Lie Algebras
> Let $\mathcal{I}$ be an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] of a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] $\mathfrak{g}$. Then $(\mathfrak{g} / \mathcal{I}, [\cdot,\cdot])$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] where the bracket is defined as $[x+\mathcal{I},y+\mathcal{I}]=[x,z]+\mathcal{I}$.
> ^prop-quotient-Lie-algebra


> [!proof]-
> Suppose the above assumptions, then since $\mathcal{I}$ is a subspace of $\mathfrak{g}$ we have that $\mathfrak{g} / \mathcal{I}$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-quotient-space\|quotient space]], thus we just need to check that the Lie bracket is well-defined (i.e. the definition is independent of our choice of representatives). Suppose $x+\mathcal{I}=x'+\mathcal{I}$ and $y+\mathcal{I}=y'+\mathcal{I}$, then $x-x',y-y'\in \mathcal{I}$. Therefore,
> $
[x',y']= [x'+(x-x'),y'+(y-')]=[x,y]+[x-x',y']+[x',y-y']+[x-x',y-y'],
>$
>where the final three terms are all in $\mathcal{I}$.


## Week 4 of Class

> [!prop] Solvability Properties
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie Algebra]] over $\mathbb{F}$.
> 1. If $\mathfrak{g}$ is solvable then so are all of its subalgebras and homomorphic images
> 2. If $\mathcal{I}$ is a solvable [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] of $\mathfrak{g}$ such that $\mathfrak{g} / \mathcal{I}$ is solvable then so is $\mathfrak{g}$.
> 3. If $\mathcal{I},\mathcal{J}$  are solvable ideals of $\mathfrak{g}$ then $\mathcal{I}+\mathcal{J}$ is also solvable.


> [!proof] 
> 1. $\mathfrak{h}\subset \mathfrak{g}$ then $\mathfrak{h}^{(i)}\subset \mathfrak{g}^{(i)}$ so result follows. For $\varphi$ homomorphism then $\varphi(\mathfrak{g}^{(i)})=\varphi(\mathfrak{g})^{(i)}$ and the result follows.
> 2. Since any ideal is the kernel of a homomorphism, we use that $(\mathfrak{g} / \mathcal{I})^{(n)}=0$ and take the homomorphism $\varphi:\mathfrak{g} \to \mathfrak{g} / \mathcal{I}$. Then at some point $\mathfrak{g}^{n}\subseteq \mathcal{I}$ result follows.
> 3. Third isomorphism theorem: $(\mathcal{I}+\mathcal{J}) / \mathcal{J} \cong \mathcal{I} / (\mathcal{I} \cap \mathcal{J})$. 

Assume that $\dim \mathfrak{g} < \infty$. By 3. of the above proposition, there exists a unique maximal solvable ideal of $\mathfrak{g}$ called the **radical of $\mathfrak{g}$** denoted $\mathrm{rad}~\mathfrak{g}$.


> [!def] Semisimple Lie algebras
> Let $\mathfrak{g}$ be a finite dimensional [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] such that its $\mathrm{rad}~ \mathfrak{g}=0$. Then $\mathfrak{g}$ is called **semisimple**.
> ^def-semisimple-lie-algebra

> [!example] Semisimple Lie algebras
> - $\mathfrak{sl}_{2}(\mathbb{C})$ is simple and thus $\mathrm{ad}(\mathfrak{sl}_{2(\mathbb{C})})=0$.
> - 0 is semisimple
> - $\mathfrak{g}$ simples $\implies$ $\mathfrak{g}$ semisimple. Note: $\mathfrak{sl}_{2}(\mathbb{C})\oplus \mathfrak{sl}_{2}(\mathbb{C})$ is semisimple but not simple.
> - $\mathfrak{g}$ finite-dimensional $\implies$ $\mathfrak{g} / \mathrm{rad(\mathfrak{g})}$ is SS.

> [!example] Non-semisimple Lie algebras
> - The Heisenberg algebra: $[\mathcal{H},\mathcal{H}]=\mathbb{C}z$ thus $\mathrm{rad}(\mathcal{H})=\mathcal{H}$.


> [!def] Descending Central Series
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]]. **The descending central series** is defined by:
> $\mathfrak{g}^{0}=\mathfrak{g},\mathfrak{g}^{1}=[\mathfrak{g},\mathfrak{g}], \mathfrak{g}^{i}=[\mathfrak{g},\mathfrak{g}^{i-1}]$.
> ^def-descending-central-series


> [!def] Nilpotent Lie Algebras
> A [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] is called **nilpotent** if $\mathfrak{g}^{n}=0$ for some $n \ge 0$.
> ^def-nilpotent-lie-algebras

> [!NOTE] Abelian [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]] are [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]].

> [!NOTE] [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|Nilpotent]] [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebras]] are solvable.


> [!prop] [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|Nilpotent Lie Algebras]] Properties
> 1. If $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]], so are all [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|subalgebras]] and homomorphic images.
> 2. $\mathfrak{g} / \mathcal{Z}(\mathfrak{g})$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]] then so is $\mathfrak{g}$.
> 3. If $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]] and $\mathfrak{g}\neq0$ then $\mathcal{Z}(\mathfrak{g})\neq0$.
> ^prop-properties-of-nilpotent-Lie-algebras

> [!proof] 
> Exercise


> [!NOTE] 
> If $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]] then $\exists n\in \mathbb{N}_{0}$ such that
> $
ad(x_{1})\circ ad(x_{2})\circ \cdots \circ ad(x_{n})(y)=0
>$
>for all $x_{1},x_{2},\ldots,x_{n},y\in \mathfrak{g}$. 

In particular, $[ad(x)]^{n}=0$ as an endomorphism in $\mathfrak{gl}(\mathfrak{g})=\mathrm{End}(\mathfrak{g})$.



> [!def] Nilpotent Endomorphism
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] and $x\in \mathfrak{g}$, we say that $x$ is ad-nilpotent if $ad(x)$ is a **nilpotent endomorphism**.
> ^def-nilpotent-endomorphism

Remark: If $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]] then every element of $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|ad-nilpotent]].

Preview:
> [!thm] Engel's Theorem
> Let $\mathfrak{g}$ be a **finite dimensional** [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie Algebra]] such that all elements are [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|ad nilpotent]]. Then $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-lie-algebras\|nilpotent]].


> [!lemma] Lemma: 
> Let  $x\in \mathfrak{gl}(V)$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|nilpotent endomorphism]] then $ad(x)$ is nilpotent.

> [!proof]
> Let $\lambda_{x}: \mathfrak{gl}(V)\to \mathfrak{gl}(V)$ where $\lambda_{x}(y)=x.y$ and $\rho_{x}:\mathfrak{gl}(V)\to \mathfrak{gl}(V)$ where $\rho_{x}(y)=y.x$. Then $\lambda_{x},\rho_{x}$ are nilpotent in $\mathfrak{gl}(\mathfrak{gl}(V))$. Note that $\lambda_{x}\circ \rho_{x}=\rho_{x}\circ \lambda_{x}$ thus $\lambda_{x}-\rho_{x}$ is nilpotent, therefore $ad(x)=\lambda_{x}-\rho_{x}$ is nilpotent. 

> [!NOTE] Remark
(Note: $ad(x)$ nilpotent and $x\in \mathfrak{gl}(V)$ does NOT imply that $x$ is nilpotent. Take $x=\mathrm{Id}$.)


> [!thm] Title
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|subalgebra]] of $\mathfrak{gl}(V)$ where $V$ is finite dimensional. If $\mathfrak{g}$ consists of nilpotent endomorphism and $V\neq 0$, then there exists a non-zero $v\in V$ such that $\mathfrak{g}.v=0$. That is to say, there exists a simultaneous eigenvector with eigenvalue $0$.

> [!proof]
> We proceed by induction on $\dim \mathfrak{g} = n$. If $\dim \mathfrak{g}=0$ or $\dim \mathfrak{g}=1$ the theorem holds. Suppose that $K$ is a maximal proper [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|subalgebra]] of $\mathfrak{g}$. Then $K \overset{ad}{\to}\mathfrak{gl}(V)$ and $K\overset{\overline{ad}}{\to}\mathfrak{gl}(\mathfrak{g} / K)$ over the vector space $V=\mathfrak{g} / K$. Because $\dim K<\dim \mathfrak{g}$ by inductive hypothesis there exists $x+K\neq K$ such that $ad(K).(x+K)=0$ in $\mathfrak{g} / K$. ($\mathfrak{g}$ consists of nilpotent endomorphisms $K\subseteq \mathfrak{g}$, thus K consists of nilpotent endomorphism, such that $\overline{ad}(K)\subseteq \mathfrak{gl}(\mathfrak{g} / K)$ consists of nilpotent endomorphisms). Since there exists $x+K\in \mathfrak{g} / K$ such that $ad(K)(x+K)=0$ where $x\not\in K$ we have that $[y,x]\in K$ for all $y\in K$. Therefore $K\subset N_{\mathfrak{g}}(K)=\{z\in \mathfrak{g}:[z,K]\subseteq K\}$ (strict). By maximalty of $K$ we obtain $N_{\mathfrak{g}}=\mathfrak{g}$ therefore $K$ is an ideal. Claim: $\dim \mathfrak{g} / K = 1$ as $\pi:\mathfrak{g} \to \mathfrak{g} / K$ take one dim proper subalg of $\mathfrak{g} / K$, pull back that element, don't get all of $\mathfrak{g}$ which contradicts maximality of $K$ or something. Therefore $\mathfrak{g} \cong K \oplus \mathbb{F}z$ for some non-zero $z\in \mathfrak{g}\setminus K$. By induction, $W=\{v\in V: K.v=0\}$ has a non-zero $v$ ad $\dim K < \dim \mathfrak{g}$. Claim: $W$ is a $\mathfrak{g}$-module $\mathfrak{g}.W\subseteq W$. To see this, take $g\in \mathfrak{g}, w\in W$ must show that $g.w\in W$, then let $k\in K$ therefore $k.g.w=gkw-[\mathfrak{g},k]w$ where $[g,k]\in K$ since $K$ is an ideal, further $gkw=0$ so $gw\in W$. Therefore $(K+\mathbb{F}z)W\subseteq W$. Now that $z$ has an eigenvector with eigenvalue $0$ in $W$ by inductive hypothesis. Call it $0\neq v_{0}\in W$ and $zv_{0}=0$. Therefore, $0\neq v_{0}\in V$ and $\mathfrak{g} v_{0}=0$.


> [!thm] Engel's Theorem
> Let $\mathfrak{g}$ be a finite-dimensional [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] such that all elements are [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|ad-nilpotent]]. Then, $\mathfrak{g}$ is nilpotent.

> [!proof]
> If $\mathfrak{g}=0$ then done, otherwise consider $ad(\mathfrak{g})\subseteq \mathfrak{gl}(\mathfrak{g})$. By assumption, $ad(\mathfrak{g})$ consists of [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|nilpotent endomoprhisms]] and $\mathfrak{g} \neq 0$ with $\dim \mathfrak{g} < \infty$. Thus, there exists $x\neq 0$ with $x\in \mathfrak{g}$ such that $ad(\mathfrak{g})x=0$ therefore $x\in \mathcal{Z}(\mathfrak{g})\neq0$. Now consider $\mathfrak{g} / \mathcal{Z}(\mathfrak{g})$ which consists of [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|ad-nilpotent]] elements and $\dim \mathfrak{g} / \mathcal{Z}(\mathfrak{g})< \dim \mathfrak{g}$ such that by the inductive hypothesis $\mathfrak{g} / \mathcal{Z}(\mathfrak{g}) \cong ad(\mathfrak{g})$ is nilpotent. Therefore, $\mathfrak{g}$ is nilpotent.


> [!def] Flag
> A **flag** for a finite-dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] $V$ is a chain $0=V_{0} \subset V_{1}\subset V_{1} \subset \cdots \subset V_{n}=V$ of subspaces with $\dim V_{i}=i$. We say that $x\in \mathrm{End}(V)$ leaves the flag invariant if $xV_{i}\subset V_{i}$ for all $i$.
> ^def-flag


> [!corollary] title
> If $\mathfrak{g}$ is a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|subalgebra]] of $\mathfrak{gl}(\mathfrak{g})$ for $V\neq 0$ and $\dim V< \infty$. Consisting of [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|nilpotent endomorphisms]]. Then, there exist a [[Independent Learning/Math/Algebra/Lie Algebras#^def-flag\|flag]] for $V$ stable under $\mathfrak{g}$ with $xV_{i}\subset V_{i}$ for all $i$ and $x\in \mathfrak{g}$. (**Something about upper triangular matrices**)


> [!proof]
> Grab $0\neq v\in V$ such that $gv=0$. Define $V_{1}=\mathbb{F}v$ and $W=V / V_{1}$. Then $\mathfrak{g}$ acts on $V / V_{1}$ as [[Independent Learning/Math/Algebra/Lie Algebras#^def-nilpotent-endomorphism\|nilpotent endomorphisms]] therefore by inductive hypothesis there exists [[Independent Learning/Math/Algebra/Lie Algebras#^def-flag\|flag]] for $W$ stable under $\mathfrak{g}$ such that $\mathfrak{g} W_{i}\subseteq W_{i-1}$. Then $0=W_{0}\subset W_{1}\subset \cdots \subset W_{m}=W$ and consider $\pi:V \to W = V / V_{1}$ such that $0=V_{0}\subseteq V_{1}\subseteq \pi^{-1}(W_{1})\subseteq \pi^{-1}(W_{2})\subseteq \cdots \subseteq \pi^{-1}(W_{m})=V$.

## Week 5 of Class
For today, $\mathbb{F}$ will be algebraically closed and $char \mathbb{F}=0$.

> [!thm] Finite-dimensional Solvable means common eigenvector
> Let $\mathbb{F}$ be algebraically closed field with $char \mathbb{F}=0$. Let $\mathfrak{g}\subset \mathfrak{gl}(V)$ be a solvable subalgebra with $\dim V <\infty$. If $V\neq 0$, then $V$ contains a common eigenvector for all endomorphisms in $\mathfrak{g}$. (i.e. there exists $0\neq v_{0}\in V$ such that $x.v_{0}=\lambda(x)v_{0}$ for $\lambda:\mathfrak{g} \to \mathbb{F}$ linear).

> [!proof] 
> Induction on $\dim \mathfrak{g} = n$. If $\dim g = 0$ then every non-zero vector is an eigenvector, if $\dim \mathfrak{g} = 1$ then $\mathfrak{g} = \mathbb{F} x$ which admits an eigenvector in $V$ since $\mathbb{F}$ is algebraically closed.
> 
> The idea for the proof is as follows:
> 1. Find an ideal $K$ of codimension $1$ in $\mathfrak{g}$.
> 2. Use induction to find $v\in V$ that is a common eigenvector for $K$.
> 3. $\mathfrak{g}. \{\text{simultaneous eigenvectors for K}\}\subseteq \{\text{simultaneous eigenvectors for K}\}$.
> 4. Find an eigenvector for $z$ such that $\mathfrak{g} = K\oplus \mathbb{F}z$ in $\{\text{simultaneous eigenvectors for K}\}$.
> 
> Assume $\dim \mathfrak{g} \ge 2$ (since we've covered the lower dim cases already). Now, since $\mathfrak{g}$ is solvable implies that $[\mathfrak{g},\mathfrak{g}]$  is strictly contained in $\mathfrak{g}$. Also $\mathfrak{g} / [\mathfrak{g},\mathfrak{g}]$ is abelian. Any subspace of $\mathfrak{g} / [\mathfrak{g},\mathfrak{g}]$ is an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]]. Take a subspace $N$ of $\mathfrak{g} / [\mathfrak{g},\mathfrak{g}]$ of codimension $1$. Then for $\pi:\mathfrak{g} \to [\mathfrak{g},\mathfrak{g}]$ we have that $\pi^{-1}(N)=K$ is an ideal of codimension $1$. $[\mathfrak{g},\mathfrak{g}]\subseteq K$ where $\mathfrak{g} / K \cong \mathfrak{g} / [\mathfrak{g},\mathfrak{g}] / (K / [\mathfrak{g},\mathfrak{g}])\cong \mathfrak{g} / [\mathfrak{g},\mathfrak{g}] / N$.
> 
> For 2., $K$ is solvable $0\neq \dim K < \dim \mathfrak{g}$ such that by inductive hypothesis there exists a non-zero element element $0\neq v\in V$ such that for $k\in K$ we have that $kv=\lambda(k)v$ for $\lambda:K\to \mathbb{F}$ linear. Fix this $\lambda:K \to \mathbb{F}$.
> 
> For 3., define $W_{\lambda} = \{w\in V: kw=\lambda(k)w, \forall k\in K\}$ (which we know is non-zero by part 2). Now we must show that $\mathfrak{g}. W \subseteq W$ **this will be done later**.
> 
> For 4. If we prove 3., then $\mathfrak{g} = K \oplus \mathbb{F}z$ (by 1.) for some $0\neq z\in \mathfrak{g}$ then $z\in \mathfrak{gl}(W)$ (by 3.) and there exists $0\neq v_{0}'\in W$ such that $zv_{0}'=\Lambda v_{0}'$ for some $\Lambda \in \mathbb{F}$. Then define $\overline{\lambda}:\mathfrak{g} \to \mathbb{F}$ by $\overline{\lambda}(x)=\lambda(x)$ if $x\in K$ and $\Lambda$ if $x=z$ which we extend linearly. Then $v_{0}'$ is a common eigenvector for $\mathfrak{g}$ and we obtain the desired result.
> 
> Now back to 3. : We want to show that $\mathfrak{g} . W \subseteq W$. Let $w\in W$ and $x\in \mathfrak{g}$ further let $k\in K$ then $k.x.w=x.(k.w)-[x,k].w=x\lambda(k)w-\lambda([x,k])w$. So we really need to show that $\lambda([x,k])=0$ for all $k\in K$ and $x\in \mathfrak{g}$. Fix $0\neq x\in \mathfrak{g}$ and $w\in W$ then $w,x.w, x^{2}.w, x^{3}.w,\ldots, x^{n}.w$ where $n>0$ is the smallest value such that this is **linearly dependent**. Let $W_{i}=\mathrm{span}\{w,x.w,\ldots,x^{i-1}.w\}$ where $W_{0}=0$, $\dim W_{n}=n$, with $W_{n+i}=W_{n}$. Then $x:W_{n}\to W_{n}$. Let $k\in K$ then $k.W_{i}\subseteq W_{i}$ as $k.w=\lambda(k)w \in W_{i}$, then $kxw=\lambda(k)xw-\lambda([x,k])w$,..., $kx^{i}w=\lambda(k)x^{i}w+w'$ with $w'\in W_{i}$ by induction. Thus, $W_{i}$ is $K$-invariant and $\mathrm{Tr}_{W_{n}}\left[k  \right]=n\lambda(k)$ for all $k\in K$ where we are viewing $k$ as an endomorphism acting on the basis in $W_{n}$. In particular, if $k_{1}\in K$ and $x\in \mathfrak{g}$ then $0=\mathrm{Tr}_{W_{n}}\left[ [x,k_{1}]  \right]=n\lambda([x,k_{1}])$ and since $char(\mathbb{F})=0$, $n\neq 0$ we must have $\lambda([x,k_{1}])=0$. Thus, 3. holds for all $x\in \mathfrak{g}$, $k_{1}\in K$ and the Thm is proved (thank god).



> [!corollary] Lie's Theorem
> Let $\mathfrak{g}$ be a solvable $\mathbb{F}$-subalgebra of $\mathfrak{gl}(V)$ with $\overline{\mathbb{F}}=\mathbb{F}$, $char(\mathbb{F})=0$, and $\dim V=n<\infty$. Then $\mathfrak{g}$ stabilizes some [[Independent Learning/Math/Algebra/Lie Algebras#^def-flag\|flag]] in $V$. (In other words, the matrices of elements of $\mathfrak{g}$ relative to a suitable basis of $V$ are all upper triangular).

> [!proof]
> EXERCISE (QUIZ)


> [!NOTE] Fill in notes from Friday 2/14

## Week 6 of Class
Look into Jordan Canonical Form (see Dummit and Foote)

### Review of Jordan Canonical Form
If $x\in \mathrm{End}(V)$ where $\dim V<\infty$ and $\mathbb{F}=\overline{\mathbb{F}}$ (algebraically closed).
$
[x] = \begin{bmatrix}
J_{a_{1}}  \\
& J_{a_{2}} \\
&& J_{a_{3}}  \\
&&& J_{a_{k}}
\end{bmatrix}
$
consists of Jordan blocks of the form
$
[J_{a}] = \begin{bmatrix}
a& 1 & 0 &\cdots &0\\
& a& 1& \cdots \\
&&a & 1& \cdots
\end{bmatrix}
$
the diagonal part of this matrix is the simisimple part of $J_{a}$ while the other is the nilpotent part of $J_{a}$.


> [!def] Semisimple Endomorphisms
> Let $x\in \mathrm{End}(V)$ we say that $x$ is **semisimple** if the roots of its minimal polynomial over $\mathbb{F}$ are all distinct.

> [!NOTE] Because $\mathbb{F} = \overline{\mathbb{F}}$, $x$ being semisimple iff $x$ diagonalizable.


> [!Note] Facts
> - Two semisimple endomorphisms that commute can be simultaneously diagonalized
> - The sum of two semisimple endomorphisms that commute is again semisimple


> [!prop] (Fact)
> Let $V$ be a finite-dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] over $\mathbb{F} = \overline{\mathbb{F}}$ and let $x\in \mathrm{End}(V)$. Then,
> 1. There exists *unique* $x_{s},x_{n}\in \mathrm{End}(V)$ satisfying $x=x_{s}+x_{n}$ where $x_{s}$ is semisimple and $x_{n}$ is nilpotent and $x_{s}x_{n}=x_{n}x_{s}$.
> 2. There exists polynomials $p(z),q(z)$ in one indeterminant without constant term such that $x_{s}=p(x)$ and $x_{n}=q(x)$. In particular, $x_{s}$ and $x_{n}$ commute with any endomorphism that commutes with $x$.
> 3. If $A\subset B\subset V$ are [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-subspace\|subspaces]] and $x(B)\subset A$ then $x_{s}(B)\subset A$ and $x_{n}(B)\subset A$. The decomposition $x=x_{s}+x_{n}$ is called the **Jordan-Chevalley cdeomposition** of $x$.

Let's now use this in Lie theory. Let $\mathfrak{g}\subset \mathfrak{gl}(V)$ be a subalgebra with $\mathbb{F} = \overline{\mathbb{F}}$ and $\dim \mathbb{F} <\infty$. If $x\in \mathfrak{g}$ is nilpotent then $ad(x)$ is nilpotent. If $x\in \mathfrak{g}$ is semisimple then $ad(x)$ is semisimple.


> [!lemma] Lemma
> Let $x\in \mathrm{End}(V)$ ($\dim V<\infty, \mathbb{F} = \overline{\mathbb{F}}$) and let $x=x_{s}+x_{n}$ be its Jordan-Chevalley decomposition. Then $ad(x)\in \mathrm{End}(\mathrm{End}(V))$ has decomposition $ad(x)=ad(x_{s})+ad(x_{n})$ is the J-C decomposition of $ad(x)$.

> [!proof]
> By the previous proposition and that the adjoint map is linear we know that it holds that $ad(x)=ad(x_{s})+ad(x_{n})$. $ad(x_{s})$ is semisimple and $ad(x_{n})$ is nilpotent, further using that $ad$ is a Lie algebra homomorphism we have that $[ad(x_{n}),ad(x_{s})]=ad([x_{n},x_{s}])=0$. Finally, uniqueness from the proposition means this is the Jordan-Chevalley decomposition.


> [!lemma] Lemma
> Let $U$ be a finite-dimensional $\mathbb{F}$-algebra (witih $\mathbb{F} = \overline{\mathbb{F}}$). Then, $\mathrm{Der}~U = \{\delta:U\to U:\delta(uv)=u\delta(v)+\delta(u)v\}$ (set of derivations on $U$) contains the semisimple and nilpotent parts of all its elements.

> [!proof]
> If $\delta \in \mathrm{Der}~U$ let $\sigma, \nu$ be its semisimple and nilpotent parts in $\mathrm{End}(U)$. We want to show that $\sigma,\nu \in \mathrm{Der}~U$. Its enough to show that $\sigma \in \mathrm{Der}~U$ (since linear combinations of derivations are derivations). For $\lambda \in \mathbb{F}$ let $U_{\lambda}=\{x\in U : (\delta-\lambda \mathrm{id})^{n}x=0\text{ for some n=n(x)}\}$ (set of generalized eigenvectors) then $U = \bigoplus_{\substack{\lambda \in \mathbb{F} \\ \lambda \text{ eigenvalue for } \delta}} U_{\lambda}$ and $\sigma \lvert_{U_{\lambda}}=\lambda \mathrm{Id}\lvert_{U_{\lambda}}$. Claim: $U_{\lambda}.U_{\mu}\subseteq U_{\lambda + \mu}$. This follows from $(\delta-(\lambda+\mu)\mathrm{Id})^{n}(xy)=\sum_{i=0}^{n} \binom{n}{i} ((\delta-\lambda \mathrm{id})^{n-i}x)((\delta-\mu \mathrm{Id})^{i}y)$. Therefore, if $x\in U_{\lambda},y\in U_{\mu}$ then $xy\in U_{\lambda+\mu}$. On the other hand $(\sigma x)y+x(\sigma y)=(\lambda +\mu)(xy)$. Thus, $\sigma(xy)=(\sigma x)y+x\sigma(y)$ on $U_{\lambda+\mu}$ for all $\lambda+\mu \in \mathbb{F}$. Therefore, $\sigma$ is a derivation on $U_{r}$ for each $r\in \mathbb{F}$ and hence a derivation on $U=\bigoplus_{r\in \mathbb{F}}U_{r}$.

### Cartan's Criteria for Solvability:
Assume that $\mathfrak{g}$ is finite-dimensional over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. 
1. If $[\mathfrak{g},\mathfrak{g}]$ is nilpotent then $\mathfrak{g}$ is solvable.
		$K=[\mathfrak{g},\mathfrak{g}]$ with $\mathfrak{g}^{(i)}\subseteq K^{i-1}$
2. 


> [!lemma] Lemma
> If $[\mathfrak{g},\mathfrak{g}]$ is nilpotent then $\mathfrak{g}$ is solvable.

> [!proof]
> Hint: $K=[\mathfrak{g},\mathfrak{g}]$ with $\mathfrak{g}^{(i)}\subseteq K^{i-1}$.


> [!lemma] Lemma
> Let $\mathbb{F}=\overline{\mathbb{F}}$, $char(\mathbb{F})=0$ and let $V$ be a finite-dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] with $\dim V<\infty$. Let $A\subset B \subset \mathfrak{gl}(V)$ (subspaces). Let $M = \{x\in \mathfrak{gl}(V):[x,B]\subset A\}$. Suppose that for $x\in M$, then $\mathrm{Tr}\left[ xy \right]=0$ for all $y\in M$ then $x$ is nilpotent.

The proof of this lemma will come later.

> [!thm] Cartan's Criteria for Solvability
> Let $\mathfrak{g}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-subalgebras\|subalgebra]] of $\mathfrak{gl}(V)$ ($V$ as in previous Lemma). If $\mathrm{Tr}\left[ xy \right]=0$ for all $x\in [\mathfrak{g},\mathfrak{g}]$, $y\in \mathfrak{g}$ then $\mathfrak{g}$ is solvable.

> [!proof]
> By a previous lemma it is enough to show that $[\mathfrak{g},\mathfrak{g}]$ is nilpotent. We will show that all $x\in [\mathfrak{g},\mathfrak{g}]$ is a nilpotent endomorphism (using the other lemma) and then note this implies that $\forall x\in [\mathfrak{g},\mathfrak{g}]$ is ad-nilpotent which implies by Engel's theorem that $[\mathfrak{g},\mathfrak{g}]$ is nilpotent.
> 
> Take $A=[\mathfrak{g},\mathfrak{g}]$ and $B=\mathfrak{g}$. Then $M=\{x\in \mathfrak{gl}(V):[x,\mathfrak{g}]\subset [\mathfrak{g},\mathfrak{g}]\}$ where we have that $\mathfrak{g}\subset M$. This is still not enough to use Lemma 2. We would like to have that $\mathrm{Tr}\left[ \tilde{x}.z \right]=0$ for all $\tilde{x}\in[\mathfrak{g},\mathfrak{g}]$ and $z\in M$. Since $\tilde{x}\in [\mathfrak{g},\mathfrak{g}]$ there exists $x,y\in \mathfrak{g}$ such that $\mathrm{Tr}\left[ \tilde{x}.z \right]=\mathrm{Tr}\left[ [x,y].z \right]=\mathrm{Tr}\left[ x.[y,z] \right]$ where $[y,z]\in [\mathfrak{g},\mathfrak{g}]$. Therefore we have that $\mathrm{Tr}\left[ x.[y,z] \right]=0$ which by the Lemma gives that $\tilde{x}$ is a nilpotent endomorphism which gives the result.

## Week 7 of Class
FILL IN THESE NOTES



## Week 8 of Class
Recall:
$\mathfrak{g}$ finite dimensional [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] is [[Independent Learning/Math/Algebra/Lie Algebras#^def-semisimple-lie-algebra\|semisimple]] if $rad(\mathfrak{g})=0$.

Claim: $\mathfrak{g}$ finite dimensional is [[Independent Learning/Math/Algebra/Lie Algebras#^def-semisimple-lie-algebra\|ss]] iff $\mathfrak{g}$ has no nonzero Abelian Ideals.
> [!proof]
> If $rad(\mathfrak{g})\neq 0$ then $\mathfrak{g}$ contains an ideal $J$ that is solvable and can be used to produce a nonzero Abelian ideal of $\mathfrak{g}$.
> 
> Exercise: Complete the proof of this claim

> [!thm]
> Let $\mathfrak{g}$ be a finite dimensional [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] over $\mathbb{F}$ with $\mathbb{F} = \overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Then, $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-semisimple-lie-algebra\|semisimple]] iff its Killing form is non-degenerate.

> [!proof]
> For the forward direction, assume that $rad(\mathfrak{g})=0$. Let $S_{\kappa}$ be the radical of the killing form of $\mathfrak{g}$. Then $S_{\kappa}=\{x\in \mathfrak{g}:\kappa(x,y)=0\quad \forall y\in \mathfrak{g}\}$ where $\kappa(x,y)=\mathrm{Tr}\left[ ad(x).ad(y) \right]$. Claim: $\mathrm{Tr}\left[ ad(x).ad(y) \right]=0$ for all $x\in S_{\kappa}$ and $y\in \mathfrak{g}$. In particular, $\mathrm{Tr}\left[ ad(x).ad(y) \right]=0$ for all $x\in S_{\kappa}$ and $y\in [S_{\kappa},S_{\kappa}]$. By Cartan's criterion for solvability $ad_{\mathfrak{g}}(S_{\kappa})$ is solvable implies that $S_{\kappa}$ is solvable which implies that $S_{\kappa}\subseteq rad(\mathfrak{g})=0$. Therefore, $\kappa$ is nondegenerate.
> 
> For the reverse direction, assume that $S_{\kappa}=0$. To show that $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-semisimple-lie-algebra\|semisimple]] it's enough to show that $\mathfrak{g}$ has no nonzero Abelian ideals. Suppose there is an Abelian ideal $I\subseteq \mathfrak{g}$. Let's show that $I\subseteq S_{\kappa}=0$. Assume that $x\in I$ and $y\in \mathfrak{g}$. Then $ad(x).ad(y):\mathfrak{g}\to \mathfrak{g}\to I$ and $(ad(x).ad(y))^{2}:\mathfrak{g}\to [I,I]=0$. Thus, $ad(x).ad(y)$ is nilpotent (as an endomorphism) and $0=\mathrm{Tr}\left[ ad(x).ad(y) \right]=\kappa(x,y)$ so $I\subseteq S_{\kappa}=0$.


> [!note] Remarks
> 1. $(\Leftarrow)$ still true if $char(\mathbb{F})=p>0$.
> 2. $S_{\kappa}\subset rad(\mathfrak{g})$ but its not necessarily true that $rad(\mathfrak{g})\subset S_{\kappa}$


> [!def] Direct Sums of Ideals
> For a finite dimensional Lie algebra $\mathfrak{g}$ and ideals $I_{1},I_{2},\ldots, I_{t}$ of $\mathfrak{g}$ we say that $\mathfrak{g}$ is the direct sum of $I_{1},\ldots,I_{t}$ if
> $
\mathfrak{g}=I_{1}\oplus I_{2}\oplus\cdots\oplus I_{t}
>$
>as vector spaces. This forces $[I_{i},I_{j}]\subseteq I_{i}\cap I_{j}=0$ for all $i\neq j$.


> [!thm]
> Let $\mathfrak{g}$ be a finite dimensional [[Independent Learning/Math/Algebra/Lie Algebras#^def-lie-algebra\|Lie algebra]] over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Then, if $\mathfrak{g}$ is [[Independent Learning/Math/Algebra/Lie Algebras#^def-semisimple-lie-algebra\|semisimple]] there exists [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideals]] $I_{1},\ldots,I_{l}$ of $\mathfrak{g}$ that are simple (as Lie algebras themselves) such that
> $
\mathfrak{g}=I_{1}\oplus\cdots\oplus I_{l}
>$
>and every ideal of $\mathfrak{g}$ is one of the $I_{j}


> [!example] Examples of Lie algebras
> 1. The space $\mathfrak{X}(M)$ of all smooth [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] on a smooth manifold $M$, under Lie bracket.
> 2. If $G$ is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]], the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|left-invariant]] vector fields on $G$ form a subalgebra of all smooth vector fields on $G$, call the **Lie algebra** of $G$ denote $\mathrm{Lie}(G)$.
> 3. $\mathbb{R}^{3}$ under cross product.
> 4. The vector space $M_{n\times n}(\mathbb{R})$ under the **commutator bracket** $[A,B]=AB-BA$.


> [!thm] The Lie Algebra and its Lie group have the same dimension
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]]. The evaluation map $\epsilon:\mathrm{Lie}(G)\to T_{e}(G)$ given by $\epsilon(X)=X_{e}$ is a vector space isomorphism. In particular, $\dim(\mathrm{Lie}(G))=\dim G$.


> [!proof]
> First, $\epsilon$ is clearly a linear map. To show its injective, suppose that $\epsilon(X)=X_{e}=0$. Then, for any $g\in G$, $0=(dL_{g})_{e}(X_{e})=X_{g}=0$ so $X=0$. To show its surjective, let $v\in T_{e}G$ and define a (rough) vector field $v^{L}$ on $G$ by $v^{L}_{g}=(dL_{g})_{e}(v)\in T_{g}G$. If there is a left-invariant vector field on $G$ whose value at $e$ is $v$, this must be it. Check: $v^{L}$ is smooth and that $v^{L}$ is left-invariant.

**Fill in all of the missing lectures (pain)**



# Chapter 11: Covector Fields

## Restricting Covector Fields to Submanifolds
Nicer than restricting vector fields! If $S\subseteq M$ is an immersed submanifold, let $\iota:S\to M$ denote the inclusion map. If $\omega\in \mathfrak{X}^{*}(M)$, then $\iota^{*}\omega$ is a covector field on $S$. To see how it's related to $\omega$, let $p\in S$ and $\vec{v}\in T_{p}S$, then $(\iota^{*}\omega)_{p}(\vec{v})=\omega(d\iota_{p}(\vec{v}))=\omega(\vec{v})$. So $\iota^{*}\omega$ is defined as the pull back, but its really just the restriction of $\omega$ to $T_{p}S$ often called the **restriction** of $\omega$ to $S$.


> [!example]
> Let $M=\mathbb{R}^{2}$, $\omega=dy$, $S=\{x-\mathrm{axis}\}$. Let $p\in S$ and $v\in T_{p}S$, then $\vec{v}=a \frac{\partial }{\partial x}$ for some $a\in \mathbb{R}$. Then $\omega(\vec{v})=dy\left( a \frac{\partial }{\partial x} \right)=0$. So even though $\omega$ is a nonzero covector field on $M$ its restriction to $S$ is the zero covector field on $S$.

## Line Integrals
Simplest case: Let $[a,b]$ be an interval in $\mathbb{R}$, let $\omega$ be smooth covector field on $[a,b]$. If $t$ is the standard coordinate on the interval $[a,b]$ then $\omega$ has the form $w=f(t)dt$ for some smooth function $f(t)$. We define $\int_{[a,b]}\omega:=\int_{a}^{b} f(t)dt$ where the right hand side is Riemann integration.


> [!prop] Title
> If $\omega$ is a smooth covector field on $[a,b]$ and $\varphi:[c,d]\to [a,b]$ is an increasing diffeomorphism then
> $$
> \int_{[a,b]} \varphi^{*}\omega = \int_{[a,b]} \omega
> $$


> [!proof] 
> Standard Calc 1 change of variables for integrals. $t=\varphi(u)$ and $\varphi^{*}(dt)=\varphi'(u)du$.

Now let $M$ be a smooth manifold. A **smooth curve segment** in $M$ is a smooth map $\gamma:[a,b]\to M$.

> [!def] Line Integral of a Smooth Covector Field
> If $\gamma:[a,b]\to M$ is a smooth curve segment and $\omega\in \mathfrak{X}^{*}(M)$, then the **line integral** of $\omega$ over the curve $\gamma$ is
> $$
> \int_{\gamma} \omega = \int_{[a,b]} \gamma^{*}\omega.
> $$

Reoccurring example: Let $M=\mathbb{R}^{2}\setminus\{0\}$ and $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$. Let $\gamma:[0,2\pi]\to M$ be the unit circle $\gamma(t)=(\cos(t),\sin(t))$. Then
$$
\int_{\gamma} \omega = \int_{[0,2\pi]} \gamma^{*}\omega = \int_{0}^{2\pi} \frac{(\cos(t)\cos(t)-\sin(t)(-\sin(t)))dt}{\cdot} = \int_{0}^{2\pi} dt=2\pi
$$
Because of the proposition above, line integrals are *invariant* under *forward* reparameterizations of $\gamma$.
Special case: When $\omega$ is the differential of a function

> [!thm] Fundamental Thm of Line Integrals
> Let $M$ be a smooth manifold, $f\in C^{\infty}(M)$, $\gamma:[a,b]\to M$ a smooth curve segment. Then
> $$
> \int_{\gamma}df = f(\gamma(b))-f(\gamma(a)).
> $$
> In this case, the covector field $df$ is called **exact** or an **exact differential** and $f$ is its **potential function**. This property also implies that $\int_{\gamma}\omega=0$ for any **closed** curve $\gamma$. A covector field with this property is called **conservative**.



> [!thm] Conservative is equivalent to exact
> A smooth covector field on $M$ is **conservative** if and only if it is exact

> [!proof]
> We already know that exact implies conservative. For the conserve, if $\omega$ is conservative, then path independence of line integrals of $\omega$ implies that we can define a potential function as follows: choose $p_{0}\in M$ and for any other $p\in M$, define
> $$
> f(p)=\int_{\gamma}\omega
> $$
> where $\gamma:[a,b]\to M$ is any smooth curve segment with $\gamma(a)=p_{0}$ and $\gamma(b)=p$. Then $\omega=df$.

> [!note] $\omega$ in the reoccurring example is **not** exact, since its integral around the unit circle is $2\pi \neq 0$.

There's a clear necessary condition for exactness. Write $\omega=a_{i}(x)dx^{i}$. It $\omega=df$ then $a_{i}(x)=\frac{\partial f}{\partial x^{i}}$. So we would have $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial^{2} f }{\partial x^{i} \partial x^{j}}= \frac{\partial^{2} f }{\partial x^{j} \partial x^{i}}=\frac{\partial a_{j}}{\partial x^{i}}$.


> [!def] Closed Covector Fields
> A covector field $\omega=a_{i}(x)dx^{i}$ is called **closed** if $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial a_{j}}{\partial x^{i}}$ for all $1\le i,j\le n$ and all coordinate expressions for $\omega$.

> [!note] Therefore, exact implies closed. However, the conserve is only *almost* true.

> [!lemma] Poincare Lemma for Covector Fields
> If $M$ is simply connected, then every closed covector field on $M$ is exact.

Even if $M$ is not simply connected, we have:

> [!prop] 
> Let $\omega$ be a closed covector field on a manifold $M$. Then every point $p\in M$ has a neighborhood $U$ such that $\omega\lvert_{U}$ is exact.


> [!prop]
> 1. Closedness of covector fields is independent of choice of local coordinates. In fact, a covector field $\omega$ is closed if and only if for all $X,Y \in \mathfrak{X}(M)$ $X(\omega(Y))-Y(\omega(X))=\omega([X,Y])$.
> 2. Pullbacks preserve closedness and exactness: If $F:M\to N$ is smooth then for $f\in C^{\infty}(N)$ and $\omega=df$ then $F^{*}\omega=F^{*}(df)=d(f\circ F)$. If $\omega \in \mathfrak{X}^{*}(M)$ is closed then $F^{*}\omega \in \mathfrak{X}^{*}(M)$ is also closed.

> [!example]
> $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$ on $\mathbb{R}\setminus\{0\}$ is closed. $\omega=d\left( \tan^{-1}\left( \frac{y}{x} \right) \right)=d\theta$ for any polar angle $\theta$. However, this function cannot be defined continuously on $\mathbb{R}^{2}\setminus \{0\}$. 

Next chapter is 14

# Chapter 14: Differential Forms

## Prologue: Tensor Products

> [!def] Tensor Product
> Let $V,W$ be finite dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector spaces]] and $\alpha \in V^{*}, \beta \in W^{*}$. The **tensor product** $\alpha \otimes \beta$ is the linear function $\alpha \otimes \beta:V\times W \to \mathbb{R}$ defined by $\alpha \otimes \beta (\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})$. The tensor product space is $V^{*}\otimes W^{*}=\mathrm{span}_{\mathbb{R}}\{\alpha \otimes \beta: \alpha \in V^{*}, \beta\in W^{*}\}$. If $\dim V=m$ and $\dim W=n$ then $\dim(V^{*}\otimes W^{*})=mn$ with basis given by the tensor product of basis elements from $V^{*}$ and $W^{*}$ $\{\epsilon^{i}\otimes \varphi^{j}\}_{1\le i \le m, 1 \le j \le n}$.
>
{ #def-tensor-products}



> [!note]
> We won't need this, but [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-tensor-products\|tensor products]] $\vec{v}\otimes \vec{w}$ with $\vec{v}\in V$ and $\vec{w}\in W$ are defined via the canonical identifications $V \cong V^{**}$ and $W\cong W^{**}$. Tensor products are associative, so $\alpha \otimes \beta \otimes \gamma$ makes sense unambiguously, but its not commutative.

Now restrict to the case $V=W$.


> [!def] Wedge Product and Alternating $k$-tensors
> Let $\alpha,\beta \in V^{*}$. The **wedge product** of $\alpha$ and $\beta$ is $\alpha \wedge \beta = \alpha \otimes \beta - \beta \otimes \alpha$. This means that for $\vec{v},\vec{w}\in V$ we have $\alpha \wedge \beta(\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})-\beta(\vec{v})\alpha(\vec{w})$. More generally, if $\alpha^{1},\ldots, \alpha^{k}\in V^{*}$ then $\alpha^{1}\wedge \cdots \wedge \alpha^{k}=\sum_{_{\sigma \in S^{k}}}^{} (\mathrm{sgn} ~\sigma) \alpha^{\sigma(1)}\otimes \cdots \otimes \alpha^{\sigma(k)}$. The space of **alternating $k$-tensors** or (**$k$-covectors**)  $\bigwedge$ of $V^{*}$ is $\bigwedge^{k}V^{*}=\mathrm{span}\{\alpha^{1}\wedge \cdots \wedge \alpha^{k}\}$.
>
{ #def-wedge-product-and-alternating-k-tensors}



> [!prop] Properties of the Wedge Product
> Let $\omega, \omega'\in \bigwedge^{k} V^{*}$ and $\eta,\eta' \in \bigwedge^{l}V^{*}$ and $\varphi\in \bigwedge^{m}V^{*}$
> 1. The wedge is bilinear
> 2. The wedge is associative
> 3. The wedge is antisymmetric
> 4. For any $\omega^{1},\ldots, \omega^{k}\in V^{*}$ we have that for $\vec{v}_{1},\ldots,\vec{v}_{k}\in V$ that $\omega^{1}\wedge \cdots \wedge \omega^{k}(\vec{v}_{1},\ldots,\vec{v}_{k})=\det([\omega^{i}(\vec{v}_{j})])$


> [!example]
> For $0\le k \le n$ we have that $\dim(\bigwedge^{k} V^{*})=\binom{n}{k}$, and a basis for $\bigwedge^{k}V^{*}$ is given by the wedge of the basis for $V^{*}$ up to $k$.


> [!def] Decomposable $k$-covectors
> A $k$-covector $\eta$ is called **decomposable** if it can be expressed as $\eta=\omega^{1}\wedge \cdots\wedge \omega^{k}$ for some covectors $\omega^{1},\ldots, \omega^{k}$. 
>
{ #def-decomposable-k-covectors}



> [!example]
> The $2$-covector in $(\mathbb{R}^{4})^{*}$ $dx^{1}\wedge dx^{2}+dx^{3}\wedge dx^{4}$ is not decomposable.


> [!def] Exterior Algebra
> For any $n$-dimensional vector space $V$, the **exterior algebra** of $V$ is the vector space $\bigwedge(V^{*})=\bigoplus_{k=0}^{n} \bigwedge^{k}(V^{*})$ equipped with the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-wedge-product-and-alternating-k-tensors\|wedge product]].
>
{ #def-exterior-algebra}


## Interior Multiplication
Let $V$ be a finite-dimensional vector space. Each $\vec{v}\in V$ defines a map $i_{\vec{v}}:\bigwedge^{k}(V^{*})\to \bigwedge^{k-1}(V^{*})$ as follows: Given $\omega \in \bigwedge^{k}(V^{*})$ and $\vec{v}_{1},\ldots, \vec{v}_{k-1}\in V$ then $i_{\vec{v}}(\omega)(\vec{v}_{1},\ldots,\vec{k-1})=\omega(\vec{v},\vec{v}_{1},\ldots,\vec{v}_{k-1})$. So $i_{\vec{v}}\omega$ is like a "partial evaluation" of $\omega$ obtained by putting $\vec{v}$ in the 1st slot of the input vectors for $\omega$. Also called "left hook" of $\vec{v}$ with $\omega$ denoted (weird symbol). Straightforward to show:
1. $i_{\vec{v}}(i_{\vec{v}}(\omega))=0$
2. For $\omega \in \bigwedge^{k}V^{*}, \eta\in \bigwedge^{l}V^{*}$, we have that $i_{\vec{v}}(\omega \wedge \eta)=(i_{\vec{v}}\omega)\wedge \eta+(-1)^{k}\omega \wedge(i_{\vec{v}}\eta)$

## Differential Forms on Manifolds

> [!def] Bundle of Alternating $k$-covectors
> The **bundle of alternating $k$-covectors** on a smooth manifold $M$ is $\bigwedge^{k}(T^{*}M)= \coprod_{p\in M} \bigwedge^{k}(T_{p}^{*}M)$ .
>
{ #def-bundle-of-alternating-k-covectors}



> [!def] Differential $k$-forms
> A **differential $k$-forms** on $M$ is a smooth section of $\bigwedge^{k}(T^{*}M)$ where $k$ is called the **degree** of the form.
>
{ #def-differential-k-forms}



> [!def] Title
> The vector space of smooth $k$-forms on $M$ is denoted $\Omega^{k}(M)=\Gamma(\bigwedge^{k}(T^{*}M))$ which are sections of a bundle.

The wedge product is defined pointwise $(\omega \wedge \eta)_{p}=\omega_{p}\wedge \eta_{p}$. So the wedge product of a $k$-form and $l$-form is a $(k+l)$-form. If $k=0$, $\omega$ is just a function $f:M\to \mathbb{R}$ and we define $f \wedge \eta=f\eta$.

With this product the vector space $\Omega^{*}(M)=\bigoplus_{k=0}^{n} \Omega^{k}(M)$ becomes a graded algebra.

In any local coordinate chart $(x^{i})$ any [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|k-form]] can be written as
$$
\omega = \sum_{(i_{1},\dots,i_{k})}^{} f_{i_{1},\ldots,i_{k}}(x) dx^{i_{1}} \wedge \cdots \wedge dx^{i_{k}}
$$
where the sum is over all *multi-indices* of degree $k$ $I=(i_{1},\ldots,i_{k})$. $\omega$ is smooth if and only if all the coefficient functions $f_{I}(x)$ are smooth. And since
$$
dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}\left(  \frac{\partial }{\partial x^{j_{1}}}, \ldots, \frac{\partial  }{\partial x^{j_{k}}} \right)=\pm\delta_{J}^{I}
$$
we can recover the function $f_{I}(x)$ by $f_{I}(x)=\omega \left( \frac{\partial }{\partial x^{i_{1}}} ,\ldots \frac{\partial }{\partial x^{i_{k}}}\right)$.

## Pullbacks
Let $F:M\to N$ be a smooth map and $\omega$ a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $N$. Then $F^{*}\omega$ is the $k$-form on $M$ defined by: for $\vec{v}_{1},\ldots,\vec{v}_{k}\in T_{p}M$ we have that $(F^{*}\omega)_{p}(\vec{v}_{1},\ldots,\vec{v}_{k})=\omega_{F(p)}(dF_{p}(\vec{v}_{1}),\ldots,dF_{p}(\vec{v}_{k}))$.


> [!lemma] Lemma
> Suppose $F:M\to N$ smooth
> 1. $F^{*}:\Omega^{k}(N)\to \Omega^{k}(M)$ is linear over $\mathbb{R}$.
> 2. $F^{*}(\omega \wedge \eta)=(F^{*}\omega)\wedge(F^{*}(\eta))$
> 3. In any smooth chart $(y^{i})$ on $N$,
> $$
> F^{*}\left( \sum_{|I|=k}^{} f_{I}(y)dy^{i_{1}}\wedge \cdots\wedge dy^{i_{k}} \right)=\sum_{|I|=k}^{} f_{I}(y \circ F) d(y^{i_{1}}\circ F)\wedge\cdots\wedge d(y^{i_{k}}\circ F)
> $$


> [!example] Computing the Pull Back
> Let $\omega=dx \wedge dy$ on $\mathbb{R}^{2}$. Consider the coordinate transformation $F:\mathbb{R}^{2}\to \mathbb{R}^{2}$ by sending $(r,\theta)\mapsto(x,y)$ for $F(r,\theta)=(r\cos \theta,r\sin\theta)$. Compute $F^{*}\omega$:
> $$
> \begin{align}
> F^{*}\omega&=d(r\cos \theta)\wedge d(r\sin \theta) \\
  &= (-(r\sin \theta )d\theta+(\cos\theta)dr)\wedge ((r\cos \theta)d\theta+(\sin \theta)dr) \\
  &= -(r\sin^{2}\theta)d\theta \wedge dr+(r\cos^{2}\theta)dr\wedge d\theta \\
  &= r(\cos^{2}\theta+\sin^{2}\theta)dr\wedge d\theta \\
  &= r dr\wedge d\theta
  \end{align}
> $$

In general, for a change of coordinates
$$
\tilde{x}^{i}=\tilde{x}^{i}(x^{1},\ldots,x^{n})\quad i=1,\ldots,n
$$
we have that
$$
d\tilde{x}^{1}\wedge \cdots \wedge d\tilde{x}^{n}= \det\left( \frac{\partial \tilde{x}^{j}}{\partial x^{j}} \right) dx^{1}\wedge \cdots \wedge dx^{n}
$$
## Exterior Derivative
> [!def] Exterior Derivative
> Let $\omega=\sum_{|I|=k}^{}f_{I}(x)dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $\mathbb{R}^{n}$. Then the **exterior derivative** of $\omega$ is the $(k+1)$-form $d\omega=\sum_{|I|=k}^{}df_{I} \wedge dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ where $df_{I}$ is the differential of the function $f_{I}$.
>
{ #def-exterior-derivative}



> [!example] Computing the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivative]]
> If $\omega$ is a $1$-form on $\mathbb{R}^{n}$ $\omega=\sum_{i=1}^{n}f_{i}(x)dx^{i}$ then
> $$
> \begin{align}
> d\omega &= \sum_{i=1}^{n}df_{i}\wedge dx^{i}= \sum_{i=1}^{n} \left( \frac{\partial f_{i}}{\partial x^{j}} dx^{j} \right) \wedge dx^{i} \\
  &= \sum_{i<j}^{} \left(  \frac{\partial f_{j}}{\partial x^{i}}- \frac{\partial f_{i}}{\partial x^{j}} \right) dx^{i}\wedge dx^{j}
  \end{align}
> $$
> Note that our previous definition for $\omega$ to be *closed* is equivalent to $d\omega=0$.


> [!prop] Properties of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivatives]] $d$ on $\mathbb{R}^{n}$
> 1. $d$ is linear over $\mathbb{R}$
> 2. If $\omega\in \Omega^{k}(\mathbb{R}^{n})$ and $\eta\in \Omega^{l}(\mathbb{R}^{n})$ then $d(\omega \wedge \eta)=(d\omega)\wedge \eta+(-1)^{k}\omega \wedge (d \eta)$
> 3. $d\circ d=0$
> 4. $d$ commutes with pullback, i.e., if $F:M\to N$ is smooth and $\omega \in \Omega^{*}(N)$ then $F^{*}(d\omega)=d(F^{*}\omega)$.
>
{ #prop-properties-of-exterior-derivatives-on-Rn}


> [!proof]
> We'll give a proof of 2. for the case when $k=l=1$. By linearity, it suffices to consider $\omega=fdx^{i}$ and $\eta=gdx^{j}$ (no sums!). So $\omega \wedge \eta=fg dx^{i}\wedge dx^{j}$
> $$
> \begin{align}
  d\omega &= \left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}} dx^{k}\right)\wedge dx^{i} \\
  d\eta &= \left( \sum_{k}^{} \frac{\partial g}{\partial x^{k}} dx^{k} \right) \wedge dx^{j} \\
  \implies d(\omega \wedge \eta) &= \left( \sum_{k}^{} \left( g \frac{\partial f}{\partial x^{k}}+f \frac{\partial g}{\partial x^{k}} \right) dx^{k} \right)\wedge dx^{i}\wedge dx^{j} \\
  &= \underbrace{\left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}}dx^{k} \wedge dx^{i} \right)}_{d\omega} \wedge (g dx^{i}) - (fdx^{i}) \wedge d\eta \\
  &= d\omega \wedge \eta - \omega \wedge d\eta
  \end{align}
> $$
> For part 3. consider a $0$-form $f$ then
> $$
> \begin{align}
> d(df)&=d\left( \frac{\partial f}{\partial x^{i}} dx^{i}\right)=d\left( \frac{\partial f}{\partial x^{i}} \right)\wedge dx^{i}=\cdots
> \end{align}
> $$
> where we conclude the result by using the fact that mixed partial derivatives commute.


> [!def] Closed and Exact Differential Forms
> [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] $\omega$ on $U\subseteq \mathbb{R}^{n}$ is **closed** if $d\omega=0$ and **exact** if there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-closed-and-exact-differential-forms}


We have already shown that exact $\Rightarrow$ closed, while the converse is *almost* true.


> [!lemma] Poincare Lemma
> If $\omega$ is a closed [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on an open set $U \subseteq \mathbb{R}^{n}$ that's homeomoprhic to an open ball in $\mathbb{R}^{n}$ then there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-poincare-lemma}


s. Moreover, the Killing form satisfies $\kappa\lvert_{I_{i}}=\kappa\lvert_{I_{i}\times I_{i}}$.

> [!proof]
> Let $I$ be any [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] in $\mathfrak{g}$. Then, $I^{\perp}=\{x\in \mathfrak{g}:\kappa(x,y)=0\quad \forall y\in I\}$. Claim: $I^{\perp}$ is also an [[Independent Learning/Math/Algebra/Lie Algebras#^def-ideal-of-a-lie-algebra\|ideal]] of $\mathfrak{g}$. Observe that $\kappa([x,g],y)=\kappa(x,[g,y])=0$. Using Cartan's criteria on $I\cap I^{\perp}$ shows that $I\cap I^{\perp}$ is solvable (Exercise). Then $I\cap I^{\perp}\subset rad(\mathfrak{g})=0$. Therefore, since $\dim I +\dim I^{\perp}=\dim \mathfrak{g}$ we have that $\mathfrak{g}=I\oplus I^{\perp}$. (To see that $\dim I +\dim I^{\perp}=\mathfrak{g}$, since $\kappa:\mathfrak{g}\times\mathfrak{g}\to \mathbb{F}$ is non-degenerate we have that the map $\Phi_{\kappa}:\mathfrak{g}\to \mathfrak{g}^{*}$ where $\Phi_{\kappa}(x)$ sends $y\mapsto\kappa(x,y)$. Then since $\kappa$ is non-degenerate we have that $\ker\Phi_{\kappa}=0$ that is it is injective. Now grab a basis for $\mathfrak{g}$ and find a dual basis to it. Then take the preimage of this basis then it is dual w.r.t. to the Killing form. This then forms a basis for $I^{\perp}$). If $\mathfrak{g}$ is simple we are done. If not, let $I_{1}$ be a minimal non-zero ideal in $\mathfrak{g}$. Then $\mathfrak{g}=I_{1}\oplus I_{1}^{\perp}$where any idea of $I_{1}$ is also an ideal of $\mathfrak{g}$. Thus, $I_{1}$ has no non-zero Abelian ideal and $I_{1}$ is semisimple and $\dim I_{1}<\dim \mathfrak{g}$. By inductive hypothesis $I_{1}$ is the sum of its simple ideals which together with minimality implies $I_{1}$ is simple. Now $I_{1}^{\perp}$ is semisimple and by inductive hypothesis is the direct sum of its simple ideals. Therefore $\mathfrak{g}$ is the direct sum of $I_{1}$ and the simple ideals of $I_{1}^{\perp}$.
> 
> Uniqueness follows by contradiction. Suppose $J$ is a simple ideal of $\mathfrak{g}$ then $J\cap I_{i}\neq 0$ for some $i$ then $J\cap I_{i}=I_{i}=J$. To see this intersection argument, observe that $[J,\mathfrak{g}]\subseteq J$ is an ideal of $J$ but by simplicity of $J$ we must have equality $J=[J,\mathfrak{g}]$. Now decompose $\mathfrak{g}=I_{1}\oplus\cdots\oplus I_{l}$ such that $J=[J,\mathfrak{g}]=[J,I_{1}]\oplus\cdots \oplus[J,I_{l}]$. Again by simplicity only one of these can be non-zero which gives the result.


> [!cor] :
> If $\mathfrak{g}$ is semisimple, finite-dimensional over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$, then $\mathfrak{g}=[\mathfrak{g},\mathfrak{g}]$ and all of its ideals and homomorphic images are semisimple. Morever, ever ideal of $\mathfrak{g}$ is the sum of certain simple ideals of $\mathfrak{g}$.


In general, $ad(\mathfrak{g})$ is an ideal in $Der(\mathfrak{g})$. It turns out that the non-degeneracy of the Killing form implies that $\mathfrak{g}$ is semisimple and that $ad(\mathfrak{g})\cong Der(\mathfrak{g})$. "$\mathfrak{g}$ semisimple implies that all derivations are inner". Exercise.

### Complete Reducibility of Representations

> [!info] Idea
> $\mathfrak{g}$ is semisimple finite-dimensional over $\mathbb{F}=\overline{\mathbb{F}}$ with $char(\mathbb{F})=0$. Study representation theory of $\mathfrak{sl}(2,\mathbb{F})$ and use it to study any semisimple Lie algebra $\mathfrak{g}$.

Recall:

> [!def] $\mathfrak{g}$-Module $V$
> A $\mathfrak{g}$-module $V$ is a [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector space]] together with a map $\mathfrak{g} \times V \to V$ that is bilinear and such that $([x,y]).v=x.(y.v)-y.(x.v)$.


> [!def] A representation of $\mathfrak{g}$
> A representation $(V,\rho)$ of $\mathfrak{g}$ is a homomorphism $\rho:\mathfrak{g} \to \mathfrak{gl}(V)$.


> [!def] Irreducible $\mathfrak{g}$-modules
> A $\mathfrak{g}$-module $V$ is irreducible if it admits precisely 2 $\mathfrak{g}$-modules: $V$ and $0$.


> [!def] Completely Reducible
> A $\mathfrak{g}$-module $V$ is said to be **completely reducible** if it is a direct sum of irreducible $\mathfrak{g}$-modules.


> [!def] $\mathfrak{g}$-Module Homomorphism
> A linear map $\varphi:V\to W$ between $\mathfrak{g}$-modules is a $\mathfrak{g}$-module homomorphism $\varphi(x.v)=x.\varphi(v)$. (Same Lie algebra)


> [!lemma] Schur's Lemma
> Let $\mathfrak{g}$ be a finite dimensional Lie algebra over $\mathbb{F}=\overline{\mathbb{F}}$ and let $\rho:\mathfrak{g} \to \mathfrak{gl}(V)$ be an irreducible finite dimensional representation. Then, the only endomorphisms in $\mathfrak{gl}(V)$ commuting with all $\rho(x)$ are proportional to the identity.

> [!proof]
> Let $T\in \mathfrak{gl}(V)$ such that $T\rho(x)=\rho(x)T$ for all $x\in \mathfrak{g}$ (this exists since identity map would work). Then $T(\rho(x).v)=\rho(x).(T(v))$. Let $w\neq 0$ be an eigenvector for $T$ with eigenvalue $\mu\in \mathbb{F}$ (which exists because the field is closed). Now consider $E_{\mu}=\{v\in V: Tv=\mu v\}$. Then $T\rho(x)w=\mu \rho(x)T$. Then $E_{\mu}$ is a subrepresentation, but $V$ is irreducible so we have that $E_{\mu}=V$. Thus, $T=\mu \mathrm{Id}_{V}$.

## Week 9 of Class

### New $\mathfrak{g}$-modules from old ones
$V$ $\mathfrak{g}$-module finite-dimensional, then $V^{*}=\mathrm{Hom}_{\mathbb{F}}(V,\mathbb{F})$ is a $\mathfrak{g}$-module. For $x\in \mathfrak{g}$ , $\alpha\in V^{*}$ we define $(x.\alpha)(v)=-\alpha(xv)$. 
> [!exercise] Exercise: Show the following
> $[x,y].\alpha=x.(y.\alpha)-y.(x.\alpha)$.

(Secretly, $U(\mathfrak{g})$ is a Hopf Algebra, if you know, you known, but I don't so)
For $V,W$ $\mathfrak{g}$-modules then $V\otimes W$ is a $\mathfrak{g}$-module with action $x(V\otimes W)=(xV)\otimes W+V\otimes(xW)$. Now for $V^{*}\otimes V\to \mathrm{End}(V)$ where $(\alpha,v)\mapsto \alpha(\cdot) v$ is an isomorphism. Then $\mathrm{End}(V)$ is a $\mathfrak{g}$-module.


> [!exercise] Exercise
> Let $T\in \mathrm{End}(V)$ and $x\in \mathfrak{g}$. Show that $(x.T)(w)=x.(Tw)-T(xw)$.

### The Casimir Element
We used Cartan's criterion to show that if $\mathfrak{g}$ is semisimple finite-dimensional over $\mathbb{F}=\overline{\mathbb{F}}$ with $char(\mathbb{F})=0$ implies that $\kappa$ is nondegenerate $\kappa(x,y)=\mathrm{Tr}\left[ ad(x)\circ ad(y) \right]$. We replace $ad$ with any faithful (1-1) representation of $\mathfrak{g}$, $\phi:\mathfrak{g} \to \mathfrak{gl}(V)$ with $V$ finite-dimensional. Then $\beta_{\phi}(x,y)=\mathrm{Tr}\left[ \phi(x)\phi(y) \right]$ is associate (invariant) and $rad~\beta_{\phi}\subseteq \mathfrak{g}$ is a ideal with $\phi(rad~\beta_{\phi})\cong rad \beta_{\phi}$ is solvable.
> [!exercise] Exercise: Prove that the above assertion is true. 

Remark: Let $\{x_{1},x_{2},\ldots,x_{n}\}$ and $\{y_{1},y_{2},\ldots,y_{n}\}$ be bases for $\mathfrak{g}$ such that $\beta(x_{i},y_{j})=\delta_{ij}$ for $\beta$ nondegenerate symmetric, associative, bilinear. For $x\in \mathfrak{g}$ define $a_{ij}\in \mathbb{F}$ by $[x,x_{i}]=\sum_{j}^{}a_{ij}x_{j}$ and $b_{ij}$ by $[x,y_{i}]=\sum_{j}^{}b_{ij}y_{j}$. Then,
$
a_{ik}=\sum_{j=1}^{n}a_{ij}\beta(x_{j},y_{k})=\cdots=-b_{ki}.
$

If $\phi:\mathfrak{g}\to\mathfrak{gl}(V)$ is any finite-dimensional representation of $\mathfrak{g}$ then
$
C_{\phi}(\beta_{\phi})=C_{\phi}=\sum_{i=1}^{n}\phi(x_{i})\phi(y_{i})
$
for $\{x_{i}\}$ and $\{y_{i}\}$ dual bases for $\mathfrak{g}$ through $\beta_{\phi}$.

> [!exercise] Exercise:
> 1. $[x,y].z+y.[x,z]=[x,y.z]\in \mathfrak{gl}(V)$
> 2. Since $a_{ik}=-b_{ki}$, one can show that for $x\in\mathfrak{g}$ that $[\phi(x),C_{\phi}]=0$. Conclude that $C_{\phi}$ is a Endomorphism that commutes with $\phi(x)$ for all $x\in \mathfrak{g}$.

$C_{\phi}$ is called the **Casimir Element** of $\phi$.

Summary:
If $\mathfrak{g}$ is semisimple with $char(\mathbb{F})=0$ and $\mathbb{F}=\overline{\mathbb{F}}$, and $\phi:\mathfrak{g}\to \mathfrak{gl}(V)$ faithful representation, $\beta_{\phi}(x,y)=\mathrm{Tr}\left[ \phi(x) \phi(y) \right]$. The Casimir element for $\mathfrak{g}.\phi$ is $C_{\phi}=\sum_{i=1}^{n}\phi(x_{i})\phi(y_{i})$ for $\{x_{i}\}$ and $\{y_{i}\}$ dual bases for $\mathfrak{g}$ through $\beta_{\phi}$ $\mathrm{Tr}\left[ C_{\phi} \right]=\dim \mathfrak{g}$. $\phi$ is irreducible (Schur's Lemma) then $C_{\phi}=\lambda \mathrm{Id}_{V}$ where $\lambda=\dim \mathfrak{g} / \dim V$.

Let $\mathfrak{g}=\mathfrak{sl}(2,\mathbb{F})$ and $V=\mathbb{F}^{2}$ over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Consider $\phi:\mathfrak{sl}(2,\mathbb{F})\to \mathfrak{gl}(\mathbb{F}^{2})$ via $\phi=\mathrm{id}$. Then $\beta_{\phi}(x,y)=\mathrm{Tr}\left[ xy \right]$ we can find the dual basis to $\{e,h,f\}$ as $\{f,h / 2, e\}$. Such that $C_{\phi}=ef+\frac{h^{2}}{2}+fe= \frac{3}{2} I$ where $\frac{3}{2} =\frac{\dim\mathfrak{g}}{\dim \mathbb{F}^{2}}$.
> [!lemma] :
> Let $\phi:\mathfrak{g}\to \mathfrak{gl}(V)$ be a finite-dimensional representation of a finite-dimensional semisimple Lie algebra $\mathfrak{g}$ over $\mathbb{F}=\overline{\mathbb{F}}$, $char(\mathbb{F})=0$. Then $\phi(g)\subseteq \mathfrak{sl}(V)$. In particular, $\mathfrak{g}$ acts trivially on any dimensional representation.

> [!proof]
> Because $\mathfrak{g}$ is semisimple, we have that $\mathfrak{g}=[\mathfrak{g},\mathfrak{g}]$. Therefore, $\phi(\mathfrak{g})=\phi([\mathfrak{g},\mathfrak{g}])=[\phi(\mathfrak{g}),\phi(\mathfrak{g})]\subset [\mathfrak{gl}(V),\mathfrak{gl}(V)]\subset \mathfrak{sl}(V)$.


> [!thm] Weyl's Theorem
> Let $\mathfrak{g}\neq 0$ be a finite-dimensional semisimple Lie algebra over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Let $\phi:\mathfrak{g}\to \mathfrak{gl}(V)$ be a representation of $\mathfrak{g}$ that is finite-dimensional. Then, $\phi$ is completely reducible.


> [!exercise]
> Show that $(\phi,V)$ is completely reducible if and only if for any $\mathfrak{g}$-submodule $W\subset V$ there exists a $\mathfrak{g}$-submodule $X$ such that $V=W\oplus X$.


> [!proof]
> We will use the fact that $(\phi,V)$ is completely reducible if and only if for any $\mathfrak{g}$-submodule $W\subset V$ there exists a $\mathfrak{g}$-submodule $X$ such that $V=W\oplus X$.
> 1. Assume first that $V$ has a $\mathfrak{g}$-submodule $W$ of codimension $1$. Then $\mathfrak{g}. V / W = 0$ and $V / W \cong \mathbb{F}$ as a $\mathfrak{g}$-module. Therefore we have a short and exact sequence $0\to W\to V\to \mathbb{F} \to 0$. We want to show that the exact sequence splits, i.e., we want to show that there exists a $\mathfrak{g}$-submodule $Y$ of $V$ isomorphic to $\mathbb{F}$ such that $V=W\oplus Y$.
> 	1.1 Claim: By induction on $\dim W$ we can reduce to the case in which $W$ is irreducible. The idea is that for $W'\subset W$ which is a proper $\mathfrak{g}$-submodule of $W$, we have the short exact sequence $0\to W / W' \to V / W' \to \mathbb{F} \to 0$. By inductive hypothesis there exists $\tilde{W} / W' \subset V /W'$ such that $\tilde{W} / W' \cong \mathbb{F}$ and $V / W'= W /W' \oplus \tilde{W} / W'$. We can build a new short exact sequence with $W'$ and $\tilde{W}$: $0\to W'\to \tilde{W} \to \mathbb{F} \to 0$. Then by inductive hypothesis this short exact sequence splits. Therefore, there exists $X\cong F$ such that $\tilde{W}=W'\oplus X$. Recall $V / W' = W / W'  \oplus \tilde{W} / W'$ which implies $W\cap X=0$ such that $V=W\oplus X$.
> Note that we may assume $\phi:\mathfrak{g}\to\mathfrak{gl}(V)$ is faithful. Let $C_{\phi}$ be the Casimir element for $\{x_{i}\}_{i=1}^{n}$ and $\{y_{i}\}_{i=1}^{n}$ (bases of $\mathfrak{g}$): $C_{\phi}=\sum_{i=1}^{n}\phi(x_{i})\phi(y_{i})$. Since $C_{\phi}$ commutes with $\phi(g)$, $C_{\phi}$ is a $\mathfrak{g}$-module homomorphism
> $
> C_{\phi}:V\to V \quad C_{\phi}(\phi(x)v)=\phi(x)C_{\phi}(v).
> $
> Also, note that $C_{\phi}(W)\subset W$ as $C_{\phi}$ is a $\mathfrak{g}$-module homomorphism and $W$ is a $\mathfrak{g}$-module. Further, $\ker C_{\phi}\subset V$ is a $\mathfrak{g}$-submodule. Therefore, $\mathfrak{g}$ acts trivially on $V / W \cong \mathbb{F}$ as $\mathfrak{g}$-modules and $\phi(\mathfrak{g}).V\subset W$. Thus, $C_{\phi}$ has trace 0 on $V / W$ and $C_{\phi}=\lambda \mathrm{Id}_{W}$ on $W$ by Schur's lemma for $\lambda \in \mathbb{F}$. But $\lambda \neq 0$ as $\mathrm{Tr}\left[ C_{\phi} \right]=\dim \mathfrak{g}$ ($\lambda = \dim \mathfrak{g} / \dim W$). Therefore, $\ker C_{\phi}$ is a 1-dimensional $\mathfrak{g}$-submodule. Thus, $V = W\oplus\ker C_{\phi}$. This finishes the proof in the case that $V$ has a codimension $1$-submodule.
> 2.  Now for the general case. Let $W$ be a proper non-zero submodule of $V$. We have the following short exact sequence
> $
> 0 \to W \to V \to V / W \to 0
> $
> Let $\mathrm{Hom}_{\mathbb{F}}(V,W)=\{f:V\to W, \mathbb{F}\mathrm{-linear}\}$. Then
> $
> \Lambda_{W} = \{f\in \mathrm{Hom}(V,W):f\lvert_{W}=\lambda \mathrm{id}_{W}\}.
> $
> Claim: $\mathfrak{g}.\Lambda_{W}\subseteq \Lambda_{W}$ (i.e. $\Lambda_{W}$ is a $\mathfrak{g}$-submodule of $\mathrm{Hom}_{\mathbb{F}}(V,W)$). To see this claim, we have defined the action for $x\in \mathfrak{g}$, $(x.f)\lvert_{W}=x.(f(w))-f(x.w)=x.(\lambda w)-\lambda(x.w)=0$. Thus, $xf\lvert_{W}=0 \mathrm{Id}_{W}$. Thus $x.f\in \Lambda_{W}$. Now define $\chi_{W}=\{f\in \Lambda_{W}:f\lvert_{W}=0\mathrm{Id}_{W}\}$. Then $\chi_{W}\subset \Lambda_{W}\subset \mathrm{Hom}_{\mathbb{F}}(V,W)$. Now $\Lambda_{W} / \chi_{W} \cong \mathbb{F}$ as both vector spaces *and* as $\mathfrak{g}$-modules (since $\mathfrak{g}.\Lambda_{W}\subset \chi_{W}$). This yields a short and exact sequence of $\mathfrak{g}$-modules
> $
> 0\to \chi_{W}\to \Lambda_{W}\to \mathbb{F}\to 0
> $
> which falls under case 1. proven earlier. Therefore, $\Lambda_{W}$ has a $n$-dimensional $\mathfrak{g}$-module $Y_{W}\subset \Lambda_{W}$ such that $\Lambda_{W}=\chi_{W}\oplus Y_{W}$. Let $f_{y}:V\to W$ be such that $\langle f_{y}\rangle = Y_{W}$. Claim: We can assume $f_{y}\lvert_{W}=1 \mathrm{Id}_{W}$. We showed in 1. that $\mathfrak{g}.Y_{W}=0$. Thus, $0=(x.f_{y})(v)=x.(f_{y}(v))-f_{y}(x.v)$ for all $x\in \mathfrak{g},v\in V$. Therefore, $f_{y}\in \mathrm{Hom}_{\mathfrak{g}}(V,W)$. Further, $\ker f_{y}$ is a $\mathfrak{g}$-submodule of $V$. With $f_{y}(v)\subset W$ and $f_{y}\lvert_{W}=\mathrm{Id}_{W}$ it follows that $V=W\oplus \ker f_{y}$ as desired and concluding the proof.

## Week 10
### Representations of $\mathfrak{sl}(2,\mathbb{F})$ for $char(\mathbb{F})=0$, $\mathbb{F}=\overline{\mathbb{F}}$

Let $V$ be *finite-dimensional* $\mathfrak{sl}(2,\mathbb{F})$ module. With representation $\phi:\mathrm{sl}(2,\mathbb{F})\to\mathfrak{gl}(V)$
$
e = \begin{bmatrix}
0 & 1 \\ 0 & 0
\end{bmatrix}, \quad h = \begin{bmatrix}
1 & 0 \\ 0 & -1
\end{bmatrix}, \quad f = \begin{bmatrix}
0 & 0 \\ 1 & 0
\end{bmatrix}
$
where $[h,e]=2e, [h,f]=-2f,[e,f]=h$. $h$ semisimple implies that $\phi(h)$ is semisimple in $\mathrm{End}(V)$ by preservation of Jordan-Chevalley decomposition. This implies that $h$ *acts* diagonally on $V$:
$
V = \bigoplus_{\lambda\in \mathrm{spec}(h)}V_{\lambda}
$$V_{\lambda}=\{v\in V:h.v=\lambda v\}$ for $\lambda\in \mathbb{F}$. (Note: $\mathrm{spec}(h)$ denotes the spectrum of $h$).


> [!def] Weights
> $\lambda\in \mathbb{F}$ is a called a **weight of $h$ in $V$** if $V_{\lambda}\neq0$ for $h\in \mathfrak{g}$ and $V$ a $\mathfrak{g}$-module. If $\lambda$ is a weight, we say that $V_{\lambda}=\{v\in V:h.v=\lambda v\}$ is a weight space of $V$.
> ^def-weights


> [!lemma] :
> If $v\in V_{\lambda}$ then $e.v\in V_{\lambda}$ and $f.v\in V_{\lambda}\in V_{\lambda-2}$.

> [!proof]
> We want to show that $h.(e.v)=(\lambda+2)e.v$. This is seen through
> $
> h.e.v = e.h.v + [h,e].v = e\lambda v + 2e v = (\lambda+2)e.v
> $

The lemma reimples that $\phi(e),\phi(f)$ are nilpotent endomorphisms in $\mathfrak{gl}(V)$. Since $\dim_{\mathbb{F}}V<\infty$:
$
V = \bigoplus_{\lambda\in \mathbb{F}}V_{\lambda}
$
there must exists $\mu\in \mathbb{F}$ such that $V_{\mu}\neq 0, V_{\mu+2}=0$.


> [!def] Maximal Weight Vectors
> Any non-zero vector $v_{\mu}\in V_{\mu}$ such that $ev_{\mu}=0$ is called a **maximal weight vector of weight $\mu$**.
> ^def-maximal-weight-vector

### Classification of Irreducible Finite Dimensional $\mathfrak{sl}(2,\mathbb{F})$-Modules

Let $V$ be a finite dimensional irreducible $\mathfrak{sl}(2,\mathbb{F})$-module ($\mathbb{F}=\overline{\mathbb{F}},char(\mathbb{F})=0$). Let $v_{0}$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-maximal-weight-vector\|maximal vector]] where $v_{0}\in V_{\lambda}$ and $\lambda\in\mathbb{F}$. Let $v_{-1}=0$ and $v_{i}=\frac{f^{i}}{i!}v_{0}$ for $i\ge 0$.


> [!lemma] :
> 1. $hv_{i}=(\lambda-2i)v_{i}$
> 2. $fv_{i}=(i+1)v_{i+1}$
> 3. $ev_{i}=(\lambda-i+1)v_{i-1}$


> [!proof]
> This is mainly left as an exercise, but the idea is to commute the operator through and apply it onto $v_{0}$. The trickiest is 3.
> 3. $ev_{1}=efv_{0}=fev_{0}+[e,f]v_{0}=0+hv_{0}=\lambda v_{0}$
> Then $ev_{i}=e \frac{f}{i} v_{i-1}=\frac{fe}{i}v_{i-1}+\left[ e, \frac{f}{i} \right]v_{i-1}=\frac{f}{i}(\lambda-i+1+2)v_{i-2}+\frac{h}{i}v_{i-1}=\cdots=(\lambda-i+1)v_{i-1}$.

By part 1. of the above Lemma, all nonzero $v_{i}


> [!example] Examples of Lie algebras
> 1. The space $\mathfrak{X}(M)$ of all smooth [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] on a smooth manifold $M$, under Lie bracket.
> 2. If $G$ is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]], the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|left-invariant]] vector fields on $G$ form a subalgebra of all smooth vector fields on $G$, call the **Lie algebra** of $G$ denote $\mathrm{Lie}(G)$.
> 3. $\mathbb{R}^{3}$ under cross product.
> 4. The vector space $M_{n\times n}(\mathbb{R})$ under the **commutator bracket** $[A,B]=AB-BA$.


> [!thm] The Lie Algebra and its Lie group have the same dimension
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]]. The evaluation map $\epsilon:\mathrm{Lie}(G)\to T_{e}(G)$ given by $\epsilon(X)=X_{e}$ is a vector space isomorphism. In particular, $\dim(\mathrm{Lie}(G))=\dim G$.


> [!proof]
> First, $\epsilon$ is clearly a linear map. To show its injective, suppose that $\epsilon(X)=X_{e}=0$. Then, for any $g\in G$, $0=(dL_{g})_{e}(X_{e})=X_{g}=0$ so $X=0$. To show its surjective, let $v\in T_{e}G$ and define a (rough) vector field $v^{L}$ on $G$ by $v^{L}_{g}=(dL_{g})_{e}(v)\in T_{g}G$. If there is a left-invariant vector field on $G$ whose value at $e$ is $v$, this must be it. Check: $v^{L}$ is smooth and that $v^{L}$ is left-invariant.

**Fill in all of the missing lectures (pain)**



# Chapter 11: Covector Fields

## Restricting Covector Fields to Submanifolds
Nicer than restricting vector fields! If $S\subseteq M$ is an immersed submanifold, let $\iota:S\to M$ denote the inclusion map. If $\omega\in \mathfrak{X}^{*}(M)$, then $\iota^{*}\omega$ is a covector field on $S$. To see how it's related to $\omega$, let $p\in S$ and $\vec{v}\in T_{p}S$, then $(\iota^{*}\omega)_{p}(\vec{v})=\omega(d\iota_{p}(\vec{v}))=\omega(\vec{v})$. So $\iota^{*}\omega$ is defined as the pull back, but its really just the restriction of $\omega$ to $T_{p}S$ often called the **restriction** of $\omega$ to $S$.


> [!example]
> Let $M=\mathbb{R}^{2}$, $\omega=dy$, $S=\{x-\mathrm{axis}\}$. Let $p\in S$ and $v\in T_{p}S$, then $\vec{v}=a \frac{\partial }{\partial x}$ for some $a\in \mathbb{R}$. Then $\omega(\vec{v})=dy\left( a \frac{\partial }{\partial x} \right)=0$. So even though $\omega$ is a nonzero covector field on $M$ its restriction to $S$ is the zero covector field on $S$.

## Line Integrals
Simplest case: Let $[a,b]$ be an interval in $\mathbb{R}$, let $\omega$ be smooth covector field on $[a,b]$. If $t$ is the standard coordinate on the interval $[a,b]$ then $\omega$ has the form $w=f(t)dt$ for some smooth function $f(t)$. We define $\int_{[a,b]}\omega:=\int_{a}^{b} f(t)dt$ where the right hand side is Riemann integration.


> [!prop] Title
> If $\omega$ is a smooth covector field on $[a,b]$ and $\varphi:[c,d]\to [a,b]$ is an increasing diffeomorphism then
> $$
> \int_{[a,b]} \varphi^{*}\omega = \int_{[a,b]} \omega
> $$


> [!proof] 
> Standard Calc 1 change of variables for integrals. $t=\varphi(u)$ and $\varphi^{*}(dt)=\varphi'(u)du$.

Now let $M$ be a smooth manifold. A **smooth curve segment** in $M$ is a smooth map $\gamma:[a,b]\to M$.

> [!def] Line Integral of a Smooth Covector Field
> If $\gamma:[a,b]\to M$ is a smooth curve segment and $\omega\in \mathfrak{X}^{*}(M)$, then the **line integral** of $\omega$ over the curve $\gamma$ is
> $$
> \int_{\gamma} \omega = \int_{[a,b]} \gamma^{*}\omega.
> $$

Reoccurring example: Let $M=\mathbb{R}^{2}\setminus\{0\}$ and $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$. Let $\gamma:[0,2\pi]\to M$ be the unit circle $\gamma(t)=(\cos(t),\sin(t))$. Then
$$
\int_{\gamma} \omega = \int_{[0,2\pi]} \gamma^{*}\omega = \int_{0}^{2\pi} \frac{(\cos(t)\cos(t)-\sin(t)(-\sin(t)))dt}{\cdot} = \int_{0}^{2\pi} dt=2\pi
$$
Because of the proposition above, line integrals are *invariant* under *forward* reparameterizations of $\gamma$.
Special case: When $\omega$ is the differential of a function

> [!thm] Fundamental Thm of Line Integrals
> Let $M$ be a smooth manifold, $f\in C^{\infty}(M)$, $\gamma:[a,b]\to M$ a smooth curve segment. Then
> $$
> \int_{\gamma}df = f(\gamma(b))-f(\gamma(a)).
> $$
> In this case, the covector field $df$ is called **exact** or an **exact differential** and $f$ is its **potential function**. This property also implies that $\int_{\gamma}\omega=0$ for any **closed** curve $\gamma$. A covector field with this property is called **conservative**.



> [!thm] Conservative is equivalent to exact
> A smooth covector field on $M$ is **conservative** if and only if it is exact

> [!proof]
> We already know that exact implies conservative. For the conserve, if $\omega$ is conservative, then path independence of line integrals of $\omega$ implies that we can define a potential function as follows: choose $p_{0}\in M$ and for any other $p\in M$, define
> $$
> f(p)=\int_{\gamma}\omega
> $$
> where $\gamma:[a,b]\to M$ is any smooth curve segment with $\gamma(a)=p_{0}$ and $\gamma(b)=p$. Then $\omega=df$.

> [!note] $\omega$ in the reoccurring example is **not** exact, since its integral around the unit circle is $2\pi \neq 0$.

There's a clear necessary condition for exactness. Write $\omega=a_{i}(x)dx^{i}$. It $\omega=df$ then $a_{i}(x)=\frac{\partial f}{\partial x^{i}}$. So we would have $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial^{2} f }{\partial x^{i} \partial x^{j}}= \frac{\partial^{2} f }{\partial x^{j} \partial x^{i}}=\frac{\partial a_{j}}{\partial x^{i}}$.


> [!def] Closed Covector Fields
> A covector field $\omega=a_{i}(x)dx^{i}$ is called **closed** if $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial a_{j}}{\partial x^{i}}$ for all $1\le i,j\le n$ and all coordinate expressions for $\omega$.

> [!note] Therefore, exact implies closed. However, the conserve is only *almost* true.

> [!lemma] Poincare Lemma for Covector Fields
> If $M$ is simply connected, then every closed covector field on $M$ is exact.

Even if $M$ is not simply connected, we have:

> [!prop] 
> Let $\omega$ be a closed covector field on a manifold $M$. Then every point $p\in M$ has a neighborhood $U$ such that $\omega\lvert_{U}$ is exact.


> [!prop]
> 1. Closedness of covector fields is independent of choice of local coordinates. In fact, a covector field $\omega$ is closed if and only if for all $X,Y \in \mathfrak{X}(M)$ $X(\omega(Y))-Y(\omega(X))=\omega([X,Y])$.
> 2. Pullbacks preserve closedness and exactness: If $F:M\to N$ is smooth then for $f\in C^{\infty}(N)$ and $\omega=df$ then $F^{*}\omega=F^{*}(df)=d(f\circ F)$. If $\omega \in \mathfrak{X}^{*}(M)$ is closed then $F^{*}\omega \in \mathfrak{X}^{*}(M)$ is also closed.

> [!example]
> $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$ on $\mathbb{R}\setminus\{0\}$ is closed. $\omega=d\left( \tan^{-1}\left( \frac{y}{x} \right) \right)=d\theta$ for any polar angle $\theta$. However, this function cannot be defined continuously on $\mathbb{R}^{2}\setminus \{0\}$. 

Next chapter is 14

# Chapter 14: Differential Forms

## Prologue: Tensor Products

> [!def] Tensor Product
> Let $V,W$ be finite dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector spaces]] and $\alpha \in V^{*}, \beta \in W^{*}$. The **tensor product** $\alpha \otimes \beta$ is the linear function $\alpha \otimes \beta:V\times W \to \mathbb{R}$ defined by $\alpha \otimes \beta (\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})$. The tensor product space is $V^{*}\otimes W^{*}=\mathrm{span}_{\mathbb{R}}\{\alpha \otimes \beta: \alpha \in V^{*}, \beta\in W^{*}\}$. If $\dim V=m$ and $\dim W=n$ then $\dim(V^{*}\otimes W^{*})=mn$ with basis given by the tensor product of basis elements from $V^{*}$ and $W^{*}$ $\{\epsilon^{i}\otimes \varphi^{j}\}_{1\le i \le m, 1 \le j \le n}$.
>
{ #def-tensor-products}



> [!note]
> We won't need this, but [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-tensor-products\|tensor products]] $\vec{v}\otimes \vec{w}$ with $\vec{v}\in V$ and $\vec{w}\in W$ are defined via the canonical identifications $V \cong V^{**}$ and $W\cong W^{**}$. Tensor products are associative, so $\alpha \otimes \beta \otimes \gamma$ makes sense unambiguously, but its not commutative.

Now restrict to the case $V=W$.


> [!def] Wedge Product and Alternating $k$-tensors
> Let $\alpha,\beta \in V^{*}$. The **wedge product** of $\alpha$ and $\beta$ is $\alpha \wedge \beta = \alpha \otimes \beta - \beta \otimes \alpha$. This means that for $\vec{v},\vec{w}\in V$ we have $\alpha \wedge \beta(\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})-\beta(\vec{v})\alpha(\vec{w})$. More generally, if $\alpha^{1},\ldots, \alpha^{k}\in V^{*}$ then $\alpha^{1}\wedge \cdots \wedge \alpha^{k}=\sum_{_{\sigma \in S^{k}}}^{} (\mathrm{sgn} ~\sigma) \alpha^{\sigma(1)}\otimes \cdots \otimes \alpha^{\sigma(k)}$. The space of **alternating $k$-tensors** or (**$k$-covectors**)  $\bigwedge$ of $V^{*}$ is $\bigwedge^{k}V^{*}=\mathrm{span}\{\alpha^{1}\wedge \cdots \wedge \alpha^{k}\}$.
>
{ #def-wedge-product-and-alternating-k-tensors}



> [!prop] Properties of the Wedge Product
> Let $\omega, \omega'\in \bigwedge^{k} V^{*}$ and $\eta,\eta' \in \bigwedge^{l}V^{*}$ and $\varphi\in \bigwedge^{m}V^{*}$
> 1. The wedge is bilinear
> 2. The wedge is associative
> 3. The wedge is antisymmetric
> 4. For any $\omega^{1},\ldots, \omega^{k}\in V^{*}$ we have that for $\vec{v}_{1},\ldots,\vec{v}_{k}\in V$ that $\omega^{1}\wedge \cdots \wedge \omega^{k}(\vec{v}_{1},\ldots,\vec{v}_{k})=\det([\omega^{i}(\vec{v}_{j})])$


> [!example]
> For $0\le k \le n$ we have that $\dim(\bigwedge^{k} V^{*})=\binom{n}{k}$, and a basis for $\bigwedge^{k}V^{*}$ is given by the wedge of the basis for $V^{*}$ up to $k$.


> [!def] Decomposable $k$-covectors
> A $k$-covector $\eta$ is called **decomposable** if it can be expressed as $\eta=\omega^{1}\wedge \cdots\wedge \omega^{k}$ for some covectors $\omega^{1},\ldots, \omega^{k}$. 
>
{ #def-decomposable-k-covectors}



> [!example]
> The $2$-covector in $(\mathbb{R}^{4})^{*}$ $dx^{1}\wedge dx^{2}+dx^{3}\wedge dx^{4}$ is not decomposable.


> [!def] Exterior Algebra
> For any $n$-dimensional vector space $V$, the **exterior algebra** of $V$ is the vector space $\bigwedge(V^{*})=\bigoplus_{k=0}^{n} \bigwedge^{k}(V^{*})$ equipped with the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-wedge-product-and-alternating-k-tensors\|wedge product]].
>
{ #def-exterior-algebra}


## Interior Multiplication
Let $V$ be a finite-dimensional vector space. Each $\vec{v}\in V$ defines a map $i_{\vec{v}}:\bigwedge^{k}(V^{*})\to \bigwedge^{k-1}(V^{*})$ as follows: Given $\omega \in \bigwedge^{k}(V^{*})$ and $\vec{v}_{1},\ldots, \vec{v}_{k-1}\in V$ then $i_{\vec{v}}(\omega)(\vec{v}_{1},\ldots,\vec{k-1})=\omega(\vec{v},\vec{v}_{1},\ldots,\vec{v}_{k-1})$. So $i_{\vec{v}}\omega$ is like a "partial evaluation" of $\omega$ obtained by putting $\vec{v}$ in the 1st slot of the input vectors for $\omega$. Also called "left hook" of $\vec{v}$ with $\omega$ denoted (weird symbol). Straightforward to show:
1. $i_{\vec{v}}(i_{\vec{v}}(\omega))=0$
2. For $\omega \in \bigwedge^{k}V^{*}, \eta\in \bigwedge^{l}V^{*}$, we have that $i_{\vec{v}}(\omega \wedge \eta)=(i_{\vec{v}}\omega)\wedge \eta+(-1)^{k}\omega \wedge(i_{\vec{v}}\eta)$

## Differential Forms on Manifolds

> [!def] Bundle of Alternating $k$-covectors
> The **bundle of alternating $k$-covectors** on a smooth manifold $M$ is $\bigwedge^{k}(T^{*}M)= \coprod_{p\in M} \bigwedge^{k}(T_{p}^{*}M)$ .
>
{ #def-bundle-of-alternating-k-covectors}



> [!def] Differential $k$-forms
> A **differential $k$-forms** on $M$ is a smooth section of $\bigwedge^{k}(T^{*}M)$ where $k$ is called the **degree** of the form.
>
{ #def-differential-k-forms}



> [!def] Title
> The vector space of smooth $k$-forms on $M$ is denoted $\Omega^{k}(M)=\Gamma(\bigwedge^{k}(T^{*}M))$ which are sections of a bundle.

The wedge product is defined pointwise $(\omega \wedge \eta)_{p}=\omega_{p}\wedge \eta_{p}$. So the wedge product of a $k$-form and $l$-form is a $(k+l)$-form. If $k=0$, $\omega$ is just a function $f:M\to \mathbb{R}$ and we define $f \wedge \eta=f\eta$.

With this product the vector space $\Omega^{*}(M)=\bigoplus_{k=0}^{n} \Omega^{k}(M)$ becomes a graded algebra.

In any local coordinate chart $(x^{i})$ any [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|k-form]] can be written as
$$
\omega = \sum_{(i_{1},\dots,i_{k})}^{} f_{i_{1},\ldots,i_{k}}(x) dx^{i_{1}} \wedge \cdots \wedge dx^{i_{k}}
$$
where the sum is over all *multi-indices* of degree $k$ $I=(i_{1},\ldots,i_{k})$. $\omega$ is smooth if and only if all the coefficient functions $f_{I}(x)$ are smooth. And since
$$
dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}\left(  \frac{\partial }{\partial x^{j_{1}}}, \ldots, \frac{\partial  }{\partial x^{j_{k}}} \right)=\pm\delta_{J}^{I}
$$
we can recover the function $f_{I}(x)$ by $f_{I}(x)=\omega \left( \frac{\partial }{\partial x^{i_{1}}} ,\ldots \frac{\partial }{\partial x^{i_{k}}}\right)$.

## Pullbacks
Let $F:M\to N$ be a smooth map and $\omega$ a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $N$. Then $F^{*}\omega$ is the $k$-form on $M$ defined by: for $\vec{v}_{1},\ldots,\vec{v}_{k}\in T_{p}M$ we have that $(F^{*}\omega)_{p}(\vec{v}_{1},\ldots,\vec{v}_{k})=\omega_{F(p)}(dF_{p}(\vec{v}_{1}),\ldots,dF_{p}(\vec{v}_{k}))$.


> [!lemma] Lemma
> Suppose $F:M\to N$ smooth
> 1. $F^{*}:\Omega^{k}(N)\to \Omega^{k}(M)$ is linear over $\mathbb{R}$.
> 2. $F^{*}(\omega \wedge \eta)=(F^{*}\omega)\wedge(F^{*}(\eta))$
> 3. In any smooth chart $(y^{i})$ on $N$,
> $$
> F^{*}\left( \sum_{|I|=k}^{} f_{I}(y)dy^{i_{1}}\wedge \cdots\wedge dy^{i_{k}} \right)=\sum_{|I|=k}^{} f_{I}(y \circ F) d(y^{i_{1}}\circ F)\wedge\cdots\wedge d(y^{i_{k}}\circ F)
> $$


> [!example] Computing the Pull Back
> Let $\omega=dx \wedge dy$ on $\mathbb{R}^{2}$. Consider the coordinate transformation $F:\mathbb{R}^{2}\to \mathbb{R}^{2}$ by sending $(r,\theta)\mapsto(x,y)$ for $F(r,\theta)=(r\cos \theta,r\sin\theta)$. Compute $F^{*}\omega$:
> $$
> \begin{align}
> F^{*}\omega&=d(r\cos \theta)\wedge d(r\sin \theta) \\
  &= (-(r\sin \theta )d\theta+(\cos\theta)dr)\wedge ((r\cos \theta)d\theta+(\sin \theta)dr) \\
  &= -(r\sin^{2}\theta)d\theta \wedge dr+(r\cos^{2}\theta)dr\wedge d\theta \\
  &= r(\cos^{2}\theta+\sin^{2}\theta)dr\wedge d\theta \\
  &= r dr\wedge d\theta
  \end{align}
> $$

In general, for a change of coordinates
$$
\tilde{x}^{i}=\tilde{x}^{i}(x^{1},\ldots,x^{n})\quad i=1,\ldots,n
$$
we have that
$$
d\tilde{x}^{1}\wedge \cdots \wedge d\tilde{x}^{n}= \det\left( \frac{\partial \tilde{x}^{j}}{\partial x^{j}} \right) dx^{1}\wedge \cdots \wedge dx^{n}
$$
## Exterior Derivative
> [!def] Exterior Derivative
> Let $\omega=\sum_{|I|=k}^{}f_{I}(x)dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $\mathbb{R}^{n}$. Then the **exterior derivative** of $\omega$ is the $(k+1)$-form $d\omega=\sum_{|I|=k}^{}df_{I} \wedge dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ where $df_{I}$ is the differential of the function $f_{I}$.
>
{ #def-exterior-derivative}



> [!example] Computing the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivative]]
> If $\omega$ is a $1$-form on $\mathbb{R}^{n}$ $\omega=\sum_{i=1}^{n}f_{i}(x)dx^{i}$ then
> $$
> \begin{align}
> d\omega &= \sum_{i=1}^{n}df_{i}\wedge dx^{i}= \sum_{i=1}^{n} \left( \frac{\partial f_{i}}{\partial x^{j}} dx^{j} \right) \wedge dx^{i} \\
  &= \sum_{i<j}^{} \left(  \frac{\partial f_{j}}{\partial x^{i}}- \frac{\partial f_{i}}{\partial x^{j}} \right) dx^{i}\wedge dx^{j}
  \end{align}
> $$
> Note that our previous definition for $\omega$ to be *closed* is equivalent to $d\omega=0$.


> [!prop] Properties of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivatives]] $d$ on $\mathbb{R}^{n}$
> 1. $d$ is linear over $\mathbb{R}$
> 2. If $\omega\in \Omega^{k}(\mathbb{R}^{n})$ and $\eta\in \Omega^{l}(\mathbb{R}^{n})$ then $d(\omega \wedge \eta)=(d\omega)\wedge \eta+(-1)^{k}\omega \wedge (d \eta)$
> 3. $d\circ d=0$
> 4. $d$ commutes with pullback, i.e., if $F:M\to N$ is smooth and $\omega \in \Omega^{*}(N)$ then $F^{*}(d\omega)=d(F^{*}\omega)$.
>
{ #prop-properties-of-exterior-derivatives-on-Rn}


> [!proof]
> We'll give a proof of 2. for the case when $k=l=1$. By linearity, it suffices to consider $\omega=fdx^{i}$ and $\eta=gdx^{j}$ (no sums!). So $\omega \wedge \eta=fg dx^{i}\wedge dx^{j}$
> $$
> \begin{align}
  d\omega &= \left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}} dx^{k}\right)\wedge dx^{i} \\
  d\eta &= \left( \sum_{k}^{} \frac{\partial g}{\partial x^{k}} dx^{k} \right) \wedge dx^{j} \\
  \implies d(\omega \wedge \eta) &= \left( \sum_{k}^{} \left( g \frac{\partial f}{\partial x^{k}}+f \frac{\partial g}{\partial x^{k}} \right) dx^{k} \right)\wedge dx^{i}\wedge dx^{j} \\
  &= \underbrace{\left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}}dx^{k} \wedge dx^{i} \right)}_{d\omega} \wedge (g dx^{i}) - (fdx^{i}) \wedge d\eta \\
  &= d\omega \wedge \eta - \omega \wedge d\eta
  \end{align}
> $$
> For part 3. consider a $0$-form $f$ then
> $$
> \begin{align}
> d(df)&=d\left( \frac{\partial f}{\partial x^{i}} dx^{i}\right)=d\left( \frac{\partial f}{\partial x^{i}} \right)\wedge dx^{i}=\cdots
> \end{align}
> $$
> where we conclude the result by using the fact that mixed partial derivatives commute.


> [!def] Closed and Exact Differential Forms
> [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] $\omega$ on $U\subseteq \mathbb{R}^{n}$ is **closed** if $d\omega=0$ and **exact** if there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-closed-and-exact-differential-forms}


We have already shown that exact $\Rightarrow$ closed, while the converse is *almost* true.


> [!lemma] Poincare Lemma
> If $\omega$ is a closed [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on an open set $U \subseteq \mathbb{R}^{n}$ that's homeomoprhic to an open ball in $\mathbb{R}^{n}$ then there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-poincare-lemma}


s are Linearly independent. Since $\dim V<\infty$ there exists $m\in \mathbb{Z}_{\ge 0}$ such that $v_{m}\neq0$ and $v_{m+1}=0$, $v_{m+i}=0$  for all $i\ge 1$. Thus, $V=\mathrm{span}\{v_{0},v_{1},\ldots,v_{m}\}$ is an $m+1$ dimensional $\mathfrak{sl}(2,\mathbb{F})$-module for $V\neq 0$.

Evaluate part 3. of Lemma for $i=m+1$. $o=ev_{m+1}=(\lambda-m)v_{m}$ therefore $\lambda=m$. Thus, the weight of the [[Independent Learning/Math/Algebra/Lie Algebras#^def-maximal-weight-vector\|maximal vector]] is a nonnegative integer (*one less than the $\dim V$*) We call $m$ the highest weight of $V$. In addition, each weight $\mu$ occurs with multiplicity $1$. $\lambda=\dim \mu-1$, and $v_{0}$ is unique up to rescaling.


> [!thm] Irreducible Finite-Dimensional Representations of $\mathfrak{sl}(2,\mathbb{F})$.
> Let $V$ be an irreducible $\mathfrak{sl}(2,\mathbb{F})$-module where $\dim_{\mathbb{F}}V<\infty$, $\mathbb{F}=\overline{\mathbb{F}}$, $char(\mathbb{F})=0$. Then:
> 1. Relative to $h$, $V=\bigoplus_{\mu \in I_{V}}V_{\mu}$ where $I_{V}=\{m,m-2,m-4,\ldots,-m\}$. Where $\dim V = m+1$, and for $\dim V_{\mu}=1$ for all $\mu\in I_{V}$.
> 2. $V$ has (up to scaling) a unique [[Independent Learning/Math/Algebra/Lie Algebras#^def-maximal-weight-vector\|maximal vector]] whose weight (known as the highest weight of $V$) is $m$.
> 3. The action of $\mathfrak{sl}(2,\mathbb{F})$ on $V$ is given explicitly by the prescribed $e,f,h$ formulas on $v_{i}$. In particular, there exists exactly $1$ irreducible $\mathfrak{sl}(2,\mathbb{F})$-module of dimension $m+1$ for each $m\in \mathbb{Z}_{\ge 0}$ $V(m)$.

There exists a *unique* irreducible $\mathfrak{sl}(2,\mathbb{F})$-module (up to isomorphism) of dimension $m+1$ for each $m\in \mathbb{Z}_{\ge 0}$.
$
\begin{align}
m&=0 \quad V=\mathbb{F} \quad \text{trivial representation}\\ \\
m&=1 \quad V=\mathbb{F}^{2} \quad \mathfrak{sl}(2,\mathbb{C})\hookrightarrow \mathfrak{gl}(\mathbb{F}^{2}) \quad \text{standard representation} \\
m&=2 \quad V=\mathbb{F}^{3} \\
&\vdots
\end{align}
$
For $\lambda\in \mathbb{F}$: $M(\lambda)=\mathrm{span}_{\mathbb{F}}\left\{ v_{0},fv_{0}, \frac{f^{2}v_{0}}{2!},\cdots\right\}$
$
\begin{align}
h. \frac{f^{i}v_{0}}{i!} &= (\lambda-2i) \frac{f^{i}v_{0}}{i!} \\
hv_{i} &= (\lambda-2i)v_{i}
\end{align}
$
1. $h.v_{i}=(\lambda-2i)v_{i}$
2. $fv_{i}=(i+1)v_{i+1}$
3. $ev_{i}=(\lambda-i+1)v_{i-1}$ for $i\ge 0$

Note: $M(\lambda)$ is irreducible if $\lambda\not\in \mathbb{Z}_{\ge 0}$.

If $\lambda\in \mathbb{Z}_{\ge 0}$, for instance $\lambda=N$. Then consider $e. \frac{f^{N+1}v_{0}}{(N+1)!}=0$. This means we cannot go back down from this element.
$
0\to M(-N-2)\to M(N)\to V(N)\to 0
$
Then $M(N) / M(-N-2) \cong V(N)$.


> [!corollary] To Weyl's Theorem
> Let $V$ be an finite-dimensional $\mathfrak{sl}(2,\mathbb{F})$-module for $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Then, the $h$ eigenvalues of $V$ are all integers and each occurs along with its negative (an equal number of times). Moreover, any decomposition of $V$ into a direct sum of irreps, the number of summands is $\dim V_{0}+\dim V_{1}$.


> [!proof]
> $V=0$ trivially holds. Otherwise, by Weyl's theorem $V=\bigoplus_{j\in I} V(j)$ where $I$ is some index set.

## Week 12
*Fill in the past two weeks [pain]*

$\mathfrak{g}$ finite-dimensional over $\mathbb{F}=\overline{\mathbb{F}}$ with $char(\mathbb{F})=0$. $\mathfrak{g}$ is semisimple. Fix $H\subset \mathfrak{g}$ maximal toral subalgebra, $\mathfrak{g}=H\oplus \bigsqcup_{\alpha \in \Phi}\mathfrak{g}_{\alpha}$. Let $M_{\alpha}=\mathrm{span}\{H,\mathfrak{g}_{c\alpha}, c\in \mathbb{F}^{*}\}\subseteq \mathfrak{g}$. ($\mathfrak{sl}(2,\mathbb{F})\cong S_{\alpha}=\mathbb{F} x_{\alpha}\oplus \mathbb{F}y_{\alpha}\oplus\mathbb{F}h_{\alpha}$). $M_{\alpha}$ is an $S_{\alpha}$-submodule of $\mathfrak{g}$. Its $h_{\alpha}$-weights are: 0 (on $H$), the $h_{\alpha}$ weights are $2c$ for $c\in \mathbb{F}^{*}$ such that $\mathfrak{g}_{c\alpha}\neq 0$ implies $2c\in \mathbb{Z}$ which implies $c\in \tfrac{1}{2}\mathbb{Z}$. $S_{\alpha}$ acts trivially on $\ker \alpha \subset H$ where $\dim (H / \ker \alpha)=1$. $H=\ker \alpha \oplus \mathbb{F} h_{\alpha}$. $S_{\alpha}$ acts irreducibly on $S_{\alpha}\subseteq M_{\alpha}$ and (since $\dim S_{\alpha}=3$) its $h_{\alpha}$ weights are $-2,0,2$. $\ker \alpha$ and $S_{\alpha}$ exhaust all occurrences of weight $0$ for $S_{\alpha}$ in $M_{\alpha}$. Thus, $-2,0,2$ are the only even weights therefore $2\alpha$ is NOT A ROOT. TWICE A NON-ZERO ROOT IS NEVER A ROOT. Thus $\frac{1}{2}\alpha$ cannot be a root implying that $M_{\alpha}=H+S_{\alpha}$. In particular, $\dim \mathfrak{g}_{\alpha}=1$ (and $S_{\alpha}$ is generated as a subalgebra by $\mathfrak{g}_{\alpha}$ and $\mathfrak{g}_{-\alpha}$.) THE ONLY MULTIPLES OF $\alpha$ WHICH ARE ROOTS ARE $\alpha$ AND $-\alpha$.

$S_{\alpha}\to \mathfrak{g}_{\beta}$? $\alpha\neq\beta \in \Phi$. Define $K:=\sum_{i\in \mathbb{Z}}^{} \mathfrak{g}_{\beta+i\alpha}$ each $\mathfrak{g}_{\gamma}$ is 1-dimensional and $\beta \neq -\alpha,\alpha$ AND $\beta \neq c\alpha$ for an $c\in \mathbb{F}$. $\beta+i\alpha \neq 0$. $K$ is an $S_{\alpha}$ submodule of $\mathfrak{g}$ with weight spaces of $\dim 1$ and weights given by $[h_{\alpha},x_{\beta+i\alpha}]=(\beta(h_{\alpha})+2i)x_{\beta+i\alpha}$ so $\beta(h_{\alpha})+2i\in \mathbb{Z}$. Note that $K$ is irreducible and has some highest weight $\beta(h_{\alpha})+2q$ and some lowest weight $\beta(h_{\alpha})-2r$.
$
\{\beta-r\alpha,\beta-(r+1)\alpha\,\ldots,\beta+2\alpha,\ldots,\beta+(q-1)\alpha,\beta+q\alpha\}
$
are all roots of $\mathfrak{g}$. Thus, the roots $\beta+i\alpha$ form a string of roots called the $\alpha$-string through $\beta$.

(Recall $[h_{\alpha},g_{c\alpha}]=\left[ \frac{2t\alpha}{\kappa(t_{\alpha},t_{\alpha})}, \mathfrak{g}_{c\alpha} \right]\subset 2c \mathfrak{g}_{c\alpha}$) 


> [!prop] Prop
> 1. $\alpha \in \Phi$ then $\dim \mathfrak{g}_{\alpha}=1$. In part $S_{\alpha}=\mathfrak{g}_{\alpha}+\mathfrak{g}_{-\alpha}+H_{\alpha}$ where $H_{\alpha}=[g_{\alpha},g_{-\alpha}]$ and for $x_{\alpha}\neq 0$ there exists a unique $y_{\alpha}\in \mathfrak{g}_{-\alpha}$ such that $[x_{\alpha},y_{\alpha}]=h\alpha$.
> 2. If $\alpha \in \Phi$, the only multiples of $\alpha$ in $\Phi$ are $\pm\alpha$.
> 3. If $\alpha,\beta\in \Phi$ then $\beta(h_{\alpha})\in\mathbb{Z}$ and $\beta-\beta(h_{\alpha})\in \Phi$. The numbers $\beta(h_{\alpha})$ are called **Cartan Integers**.
> 4. If $\alpha, \beta \in \Phi$ and $\alpha+\beta\in \Phi$ then $[\mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}]=\mathfrak{g}_{\alpha+\beta}$.
> 5. Let $\alpha,\beta \in \Phi$ where $\beta\neq \pm \alpha$. Let $r,q$ be (resp) the largest integers such that $\beta-r\alpha$, $\beta+q\alpha$ are roots. Then, all $\beta+i\alpha\in \Phi$ for $-r\le i \le q$ and $\beta(h_{\alpha})=r-q$.

$\mathfrak{g}$ semisimple finite-dimensional Lie algebra over $\mathbb{F}=\overline{\mathbb{F}}$ and $char(\mathbb{F})=0$. Let $H \subset \mathfrak{g}$ be a maximal toral subalgebra. Then $\mathfrak{g}=H\oplus \bigsqcup_{\alpha \in \Phi}\mathfrak{g}_{\alpha}$ where $\Phi \subset H^{*}$. $\kappa\lvert_{H\times H}$ is non-degenerate we use it to define $(~,~):H^{*}\times H^{*}\to \mathbb{F}$ where $(\gamma,\delta):=\kappa(t_{\delta},t_{\delta})$ for all $\delta,\gamma\in H^{*}$. $\Phi$ spans $H^{*}$. Let $\alpha_{1},\ldots,\alpha_{l}\in \Phi$ be a basis. if $\beta\in \Phi$ then $\beta=\sum_{i=1}^{l} c_{i}\alpha_{i}$ (uniquely) $c_{i}\in \mathbb{F}$ (actually $c_{i}\in \mathbb{Q}$).

$
\begin{align}
(\beta,\alpha_{j})&=\sum_{i=1}^{l}c_{i}(\alpha_{i},\alpha_{j}) \\
\implies 2 \frac{\beta,\alpha_{j}}{(\alpha_{j},\alpha_{j})} &= \sum_{i=1}^{l} c_{i} 2 \frac{\alpha_{i},\alpha_{j}}{(\alpha_{j},\alpha_{j})} \\
\beta(h_{\alpha_{j}})=\sum_{i=1}^{l} c_{i} \alpha_{i}(h_{\alpha_{j}})
\end{align}
$
where $\beta(h_{\alpha_{j}})\in \mathbb{Z}$ and $\alpha_{i}(h_{\alpha_{j}})\in \mathbb{Z}$. System of $l$ equations with $l$ unknowns $c_{i}$ with coefficients in $\mathbb{Q}$. $A=(\alpha_{i},\alpha_{j})_{1\le i,j\le l}$ is non-singular therefore there exists a unique solution of $c_{i}\in \mathbb{Q}$.


> [!NOTE] $\mathbb{Q}$-subspace $\mathrm{span}_{\mathbb{Q}}\{\alpha_{1},\ldots,\alpha_{l}\}=E_{\mathbb{Q}}\subset H^{*}$ satisfies $\dim_{\mathbb{Q}}(E_{\mathbb{Q}})=l=\dim H^{*}$.

> [!NOTE] More is true:
> 1. All inner products $(\alpha,\beta)$ of vectors in $E_{\mathbb{Q}}$ are rational.
> $\lambda,\mu \in H^{*}$ then $(\lambda,\mu)=\kappa(t_{\lambda},t_{\mu})=\sum_{\alpha\in \Phi}^{}\alpha(t_{\lambda})\alpha(t_{\mu})=\sum_{\alpha \in \Phi}^{}\kappa(t_{\alpha},t_{\lambda})\kappa(t_{\alpha},t_{\mu})=\sum_{\alpha \in \Phi}^{}(\alpha,\lambda)(\alpha,\mu)$.
> In part, $(\beta,\beta)=\sum_{\alpha\in \Phi}^{}(\alpha,\beta)^{2}$ multiplying by $1 / (\beta,\beta)^{2}$ we can conclude that $(\beta,\beta)\in \mathbb{Q}$.
> 2. The bilinear form on $E_{\mathbb{Q}}$ is positive define.

Extending $\mathbb{Q}$ to $\mathbb{R}$: $\mathbb{R}\otimes_{\mathbb{Q}}E_{\mathbb{Q}}=E$ and extending $(~,~)$ to $E$ which is Euclidean space with $(~,~)$ positive definite form.

> [!thm] w
> $\mathfrak{g},H,\Phi,E$ as above
> 1. $\Phi$ spans $E$ and $0\in \Phi$
> 2. If $\alpha\in \Phi$ then $-\alpha \in \Phi$ but no other multiples of $\alpha$ are in $\Phi$.
> 3. If $\alpha,\beta\in \Phi$ then $\beta - \frac{2(\beta,\alpha)}{(\alpha,\alpha)}\alpha\in \Phi$.
> 4. If $\alpha,\beta \in \Phi$ then $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\in\mathbb{Z}$.

Consider $\mathfrak{sl}(3,\mathbb{C})$ with roots $\Phi=\{\epsilon_{1}-\epsilon_{2},\epsilon_{1}-\epsilon_{3},\epsilon_{2}-\epsilon_{3},\epsilon_{2}-\epsilon_{1},\epsilon_{3}-\epsilon_{1},\epsilon_{3}-\epsilon_{2}\}$. $\dim(\mathrm{span} \Phi)=2=\dim \mathfrak{sl}(3,\mathbb{C})$. Basis, $\alpha_{1}=\epsilon_{1}-\epsilon_{2}$ and $\alpha_{2}=\epsilon_{2}-\epsilon_{3}$.

## Root Systems
Fix Euclidean finite-dimensional $\mathbb{R}$-vector space with positive definite bilinear form $(\alpha,\beta)$ for $\alpha,\beta \in E$.

> [!def] Reflection
> A **reflection** $r$ is an invertible linear transformation leaving pointwise fixed some hyperplane and sending any vector $\perp$ to the hyperplane to its negative.
> ^def-reflection

If $\alpha \in E$ determines a reflection $\sigma_{\alpha}(\beta)=\beta-\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\alpha$.


> [!lemma] Invariant transformations are reflections
> Let $\Phi$ be a finite set that spans $E$. Suppose that all reflections $\sigma_{\alpha}$ ($\alpha \in \Phi$) leave $\Phi$ invariant. If $\sigma\in GL(E)$ leaves $\Phi$ invariant and it fixes pointwise a hyperplane $P$ of $E$ while sending some nonzero $\alpha\in \Phi$ to $-\alpha$ then $\sigma=\sigma_{\alpha}$ (and $P=P_{\alpha}$).


> [!def] Root System
> A subset $\Phi$ of Euclidean space $E$ is a **root system in $E$** if the following axioms are satisfied:
> 1. $\Phi$ is finite, spans $E$, and $0\neq \Phi$.
> 2. If $\alpha\in \Phi$, the only multiples of $\alpha$ in $\Phi$ are $\pm \alpha$.
> 3. If $\alpha \in \Phi$, then $\sigma_{\alpha}(\Phi)=\Phi$.
> 4. If $\alpha,\beta \in \Phi$ then $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\in \mathbb{Z}$.
> ^def-root-system


> [!def] Weyl Group
> The **Weyl** group is $\mathcal{W}=\langle \sigma_{\alpha}:\alpha \in \Phi \rangle \le GL(E)$ or $\mathcal{W}\le S_{|\Phi|}$ (symmetric group on $|\Phi|$ letters).
> ^def-Weyl-group


> [!lemma] $\Phi$ preserving maps
> Let $\Phi$ be a [[Independent Learning/Math/Algebra/Lie Algebras#^def-root-system\|root system]] in $E$ with [[Independent Learning/Math/Algebra/Lie Algebras#^def-Weyl-group\|Weyl group]] $\mathcal{W}$. If $\sigma \in GL(E)$ leaves $\Phi$ invariant then $\sigma \sigma_{\alpha}\sigma^{-1}=\sigma_{\sigma(\alpha)}$ for all $\alpha \in \Phi$ and $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}=\frac{2(\sigma(\beta),\sigma(\alpha))}{(\sigma(\alpha),\sigma(\alpha))}$ for all $\alpha,\beta \in \Phi$.
> ^lem-root-preserving-maps

> [!proof] 
> Notice that
> $
> \sigma \sigma_{\alpha}\sigma^{-1}(\sigma(\beta))=\sigma \sigma_{\alpha}(\beta) = \sigma(\beta)- \frac{2(\beta,\alpha)}{(\alpha,\alpha)}\sigma(\alpha).
> $
> $\sigma(\beta)$ covers $\Phi$ as $\beta$ covers $\Phi$. Further, $\sigma \sigma_{\alpha}\sigma^{-1}(\Phi)=\Phi$ and $\sigma \sigma_{\alpha} \sigma^{-1}(\sigma(P_{\alpha}))=\sigma(P_{\alpha})$, then $\sigma \sigma_{\alpha}\sigma^{-1}(\sigma(\alpha))=\sigma\left( \alpha-\frac{2(\alpha,\alpha)}{(\alpha,\alpha)} \alpha\right)=\sigma(-\alpha)=-\sigma(\alpha)$ therefore $\sigma \sigma_{\alpha}^{}\sigma^{-1}=\sigma_{\sigma(\alpha)}$. The rest can be proven using similar ideas

> [!def] Isomorphic Root Systems
> Two [[Independent Learning/Math/Algebra/Lie Algebras#^def-root-system\|root systems]] $\Phi$ and $\Phi'$ in Euclidean spaces $E$ and $E'$ respectively are isomorphic if there exists $T:E\to E'$ bijective linear map such that $T(\Phi)=\Phi'$ and $\frac{T\beta,T\alpha}{(T\alpha,T\alpha)}=\frac{\beta,\alpha}{(\alpha,\alpha)}$ for all $\alpha,\beta \in \Phi$.
> ^def-isomorphic-root-systems


> [!NOTE]
> 1. An [[Independent Learning/Math/Algebra/Lie Algebras#^def-isomorphic-root-systems\|isomorphism of root systems]] induces an isomorphism of [[Independent Learning/Math/Algebra/Lie Algebras#^def-Weyl-group\|Weyl groups]]. I.e. $\sigma_{T(\alpha)}(T(\beta))=T(\sigma_{\alpha}(\beta))$ by $\mathcal{W}\to \mathcal{W'}$ where $\sigma \mapsto T\sigma T^{-1}$.
> 2. By[[Independent Learning/Math/Algebra/Lie Algebras#^lem-root-preserving-maps\| the previous Lemma]]  we have that $T\in Aut(\Phi) \Leftrightarrow T\in GL(E)$ such that $T\Phi=\Phi$. In part, $\mathcal{W}\subset Aut_{\Phi}$
> 3. Further let $\alpha^{\vee}=\frac{2\alpha}{(\alpha,\alpha)}$ be called the "dual check of $\alpha$". Then $\Phi^{\vee}=\{\alpha^{\vee}:\alpha \in \Phi\}$ is isomorphic to $\Phi$ and $\mathcal{W}^{\vee}\cong \mathcal{W}$. $\mathfrak{g}$ semi-simple then $\alpha \leftrightarrow t_{\alpha}$ and $\alpha^{\vee}\leftrightarrow h_{\alpha}$.


> [!example] Examples of Root Systems 
> $l=\mathrm{Rank}~\Phi:=\dim E$
> 1. $l=1$ $A_{1}$ which is $|\mathcal{W}|=2$ so $\mathfrak{sl}(2,\mathbb{F})$. **DRAW SOME PICTURES**
> 2. $l=2$, $A_{1}\times A_{1}$ which corresponds to $\mathfrak{sl}(2,\mathbb{F})\oplus \mathfrak{sl}(2,\mathbb{F})$. There's also $A_{2}$, $B_{2}$, and $G_{2}$.

### Angles Between Roots
Based on the fourth axiom of a root system we have that for $\alpha,\beta \in \Phi$  it follows that $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}\in \mathbb{Z}$. Restricts possible angles a lot $(\alpha,\beta)=\left \lVert \alpha \right \rVert_{}\left \lVert \beta \right \rVert_{} \cos\theta$  where $\left \lVert \alpha \right \rVert_{}^{2}=(\alpha,\alpha)$which implies that $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}=\frac{2\left \lVert \beta \right \rVert_{}}{\left \lVert \alpha \right \rVert_{}}\cos \theta$. Fix $\beta \neq \pm \alpha$ and $\left \lVert \beta \right \rVert_{} \ge \left \lVert \alpha \right \rVert_{}$ then
$
\frac{2(\beta,\alpha)}{(\alpha,\alpha)} \frac{2(\alpha,\beta)}{(\beta,\beta)}=4\cos^{2}\theta.
$
where the left hand side must be an integer. Further the signs of the integers must coincide. Possible choices for $\cos^{2}\theta=0, \frac{1}{2}, \frac{1}{4}, \frac{3}{4}$ where we ignore $\cos^{2}\theta=1$ since it violates $\beta \neq \pm \alpha$. So the possible angles are $\frac{\pi}{2}, \frac{\pi}{3}, \frac{2\pi}{3}, \frac{\pi}{4}, \frac{3\pi}{4}, \frac{\pi}{6}, \frac{5\pi}{6}$.


> [!lemma] Lemma
> Let $\alpha,\beta$ be non proportional roots. If $(\alpha,\beta)>0$ (the angle between them is acute) then $\alpha-\beta$ is a root. If $(\alpha,\beta)<0$ then $\beta-\alpha$ is a root.

> [!proof]
> The last part follows from the first by replacing $\beta \to -\beta$. Now, $(\alpha,\beta)>0 \Leftrightarrow \frac{2(\alpha,\beta)}{(\beta,\beta)}>0$. By the table either $\frac{2(\alpha,\beta)}{(\beta,\beta)}$ or $\frac{2(\beta,\alpha)}{(\alpha,\alpha)}$ equals $1$. If $\frac{2(\alpha,\beta)}{(\beta,\beta)}=1$ then $\sigma_{\beta}(\alpha)=\alpha-\beta \in \Phi$.

An application of this lemma is to show that strings cannot be interrupted.

</div></div>



> [!example] Examples of Lie algebras
> 1. The space $\mathfrak{X}(M)$ of all smooth [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-vector-field\|vector fields]] on a smooth manifold $M$, under Lie bracket.
> 2. If $G$ is a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]], the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-left-invariant-vector-field\|left-invariant]] vector fields on $G$ form a subalgebra of all smooth vector fields on $G$, call the **Lie algebra** of $G$ denote $\mathrm{Lie}(G)$.
> 3. $\mathbb{R}^{3}$ under cross product.
> 4. The vector space $M_{n\times n}(\mathbb{R})$ under the **commutator bracket** $[A,B]=AB-BA$.


> [!thm] The Lie Algebra and its Lie group have the same dimension
> Let $G$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-Lie-group\|Lie group]]. The evaluation map $\epsilon:\mathrm{Lie}(G)\to T_{e}(G)$ given by $\epsilon(X)=X_{e}$ is a vector space isomorphism. In particular, $\dim(\mathrm{Lie}(G))=\dim G$.


> [!proof]
> First, $\epsilon$ is clearly a linear map. To show its injective, suppose that $\epsilon(X)=X_{e}=0$. Then, for any $g\in G$, $0=(dL_{g})_{e}(X_{e})=X_{g}=0$ so $X=0$. To show its surjective, let $v\in T_{e}G$ and define a (rough) vector field $v^{L}$ on $G$ by $v^{L}_{g}=(dL_{g})_{e}(v)\in T_{g}G$. If there is a left-invariant vector field on $G$ whose value at $e$ is $v$, this must be it. Check: $v^{L}$ is smooth and that $v^{L}$ is left-invariant.

**Fill in all of the missing lectures (pain)**



# Chapter 11: Covector Fields

## Restricting Covector Fields to Submanifolds
Nicer than restricting vector fields! If $S\subseteq M$ is an immersed submanifold, let $\iota:S\to M$ denote the inclusion map. If $\omega\in \mathfrak{X}^{*}(M)$, then $\iota^{*}\omega$ is a covector field on $S$. To see how it's related to $\omega$, let $p\in S$ and $\vec{v}\in T_{p}S$, then $(\iota^{*}\omega)_{p}(\vec{v})=\omega(d\iota_{p}(\vec{v}))=\omega(\vec{v})$. So $\iota^{*}\omega$ is defined as the pull back, but its really just the restriction of $\omega$ to $T_{p}S$ often called the **restriction** of $\omega$ to $S$.


> [!example]
> Let $M=\mathbb{R}^{2}$, $\omega=dy$, $S=\{x-\mathrm{axis}\}$. Let $p\in S$ and $v\in T_{p}S$, then $\vec{v}=a \frac{\partial }{\partial x}$ for some $a\in \mathbb{R}$. Then $\omega(\vec{v})=dy\left( a \frac{\partial }{\partial x} \right)=0$. So even though $\omega$ is a nonzero covector field on $M$ its restriction to $S$ is the zero covector field on $S$.

## Line Integrals
Simplest case: Let $[a,b]$ be an interval in $\mathbb{R}$, let $\omega$ be smooth covector field on $[a,b]$. If $t$ is the standard coordinate on the interval $[a,b]$ then $\omega$ has the form $w=f(t)dt$ for some smooth function $f(t)$. We define $\int_{[a,b]}\omega:=\int_{a}^{b} f(t)dt$ where the right hand side is Riemann integration.


> [!prop] Title
> If $\omega$ is a smooth covector field on $[a,b]$ and $\varphi:[c,d]\to [a,b]$ is an increasing diffeomorphism then
> $$
> \int_{[a,b]} \varphi^{*}\omega = \int_{[a,b]} \omega
> $$


> [!proof] 
> Standard Calc 1 change of variables for integrals. $t=\varphi(u)$ and $\varphi^{*}(dt)=\varphi'(u)du$.

Now let $M$ be a smooth manifold. A **smooth curve segment** in $M$ is a smooth map $\gamma:[a,b]\to M$.

> [!def] Line Integral of a Smooth Covector Field
> If $\gamma:[a,b]\to M$ is a smooth curve segment and $\omega\in \mathfrak{X}^{*}(M)$, then the **line integral** of $\omega$ over the curve $\gamma$ is
> $$
> \int_{\gamma} \omega = \int_{[a,b]} \gamma^{*}\omega.
> $$

Reoccurring example: Let $M=\mathbb{R}^{2}\setminus\{0\}$ and $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$. Let $\gamma:[0,2\pi]\to M$ be the unit circle $\gamma(t)=(\cos(t),\sin(t))$. Then
$$
\int_{\gamma} \omega = \int_{[0,2\pi]} \gamma^{*}\omega = \int_{0}^{2\pi} \frac{(\cos(t)\cos(t)-\sin(t)(-\sin(t)))dt}{\cdot} = \int_{0}^{2\pi} dt=2\pi
$$
Because of the proposition above, line integrals are *invariant* under *forward* reparameterizations of $\gamma$.
Special case: When $\omega$ is the differential of a function

> [!thm] Fundamental Thm of Line Integrals
> Let $M$ be a smooth manifold, $f\in C^{\infty}(M)$, $\gamma:[a,b]\to M$ a smooth curve segment. Then
> $$
> \int_{\gamma}df = f(\gamma(b))-f(\gamma(a)).
> $$
> In this case, the covector field $df$ is called **exact** or an **exact differential** and $f$ is its **potential function**. This property also implies that $\int_{\gamma}\omega=0$ for any **closed** curve $\gamma$. A covector field with this property is called **conservative**.



> [!thm] Conservative is equivalent to exact
> A smooth covector field on $M$ is **conservative** if and only if it is exact

> [!proof]
> We already know that exact implies conservative. For the conserve, if $\omega$ is conservative, then path independence of line integrals of $\omega$ implies that we can define a potential function as follows: choose $p_{0}\in M$ and for any other $p\in M$, define
> $$
> f(p)=\int_{\gamma}\omega
> $$
> where $\gamma:[a,b]\to M$ is any smooth curve segment with $\gamma(a)=p_{0}$ and $\gamma(b)=p$. Then $\omega=df$.

> [!note] $\omega$ in the reoccurring example is **not** exact, since its integral around the unit circle is $2\pi \neq 0$.

There's a clear necessary condition for exactness. Write $\omega=a_{i}(x)dx^{i}$. It $\omega=df$ then $a_{i}(x)=\frac{\partial f}{\partial x^{i}}$. So we would have $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial^{2} f }{\partial x^{i} \partial x^{j}}= \frac{\partial^{2} f }{\partial x^{j} \partial x^{i}}=\frac{\partial a_{j}}{\partial x^{i}}$.


> [!def] Closed Covector Fields
> A covector field $\omega=a_{i}(x)dx^{i}$ is called **closed** if $\frac{\partial a_{i}}{\partial x^{j}}=\frac{\partial a_{j}}{\partial x^{i}}$ for all $1\le i,j\le n$ and all coordinate expressions for $\omega$.

> [!note] Therefore, exact implies closed. However, the conserve is only *almost* true.

> [!lemma] Poincare Lemma for Covector Fields
> If $M$ is simply connected, then every closed covector field on $M$ is exact.

Even if $M$ is not simply connected, we have:

> [!prop] 
> Let $\omega$ be a closed covector field on a manifold $M$. Then every point $p\in M$ has a neighborhood $U$ such that $\omega\lvert_{U}$ is exact.


> [!prop]
> 1. Closedness of covector fields is independent of choice of local coordinates. In fact, a covector field $\omega$ is closed if and only if for all $X,Y \in \mathfrak{X}(M)$ $X(\omega(Y))-Y(\omega(X))=\omega([X,Y])$.
> 2. Pullbacks preserve closedness and exactness: If $F:M\to N$ is smooth then for $f\in C^{\infty}(N)$ and $\omega=df$ then $F^{*}\omega=F^{*}(df)=d(f\circ F)$. If $\omega \in \mathfrak{X}^{*}(M)$ is closed then $F^{*}\omega \in \mathfrak{X}^{*}(M)$ is also closed.

> [!example]
> $\omega = \frac{xdy-ydx}{x^{2}+y^{2}}$ on $\mathbb{R}\setminus\{0\}$ is closed. $\omega=d\left( \tan^{-1}\left( \frac{y}{x} \right) \right)=d\theta$ for any polar angle $\theta$. However, this function cannot be defined continuously on $\mathbb{R}^{2}\setminus \{0\}$. 

Next chapter is 14

# Chapter 14: Differential Forms

## Prologue: Tensor Products

> [!def] Tensor Product
> Let $V,W$ be finite dimensional [[Independent Learning/Math/Algebra/Linear Algebra#^def-vector-space\|vector spaces]] and $\alpha \in V^{*}, \beta \in W^{*}$. The **tensor product** $\alpha \otimes \beta$ is the linear function $\alpha \otimes \beta:V\times W \to \mathbb{R}$ defined by $\alpha \otimes \beta (\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})$. The tensor product space is $V^{*}\otimes W^{*}=\mathrm{span}_{\mathbb{R}}\{\alpha \otimes \beta: \alpha \in V^{*}, \beta\in W^{*}\}$. If $\dim V=m$ and $\dim W=n$ then $\dim(V^{*}\otimes W^{*})=mn$ with basis given by the tensor product of basis elements from $V^{*}$ and $W^{*}$ $\{\epsilon^{i}\otimes \varphi^{j}\}_{1\le i \le m, 1 \le j \le n}$.
>
{ #def-tensor-products}



> [!note]
> We won't need this, but [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-tensor-products\|tensor products]] $\vec{v}\otimes \vec{w}$ with $\vec{v}\in V$ and $\vec{w}\in W$ are defined via the canonical identifications $V \cong V^{**}$ and $W\cong W^{**}$. Tensor products are associative, so $\alpha \otimes \beta \otimes \gamma$ makes sense unambiguously, but its not commutative.

Now restrict to the case $V=W$.


> [!def] Wedge Product and Alternating $k$-tensors
> Let $\alpha,\beta \in V^{*}$. The **wedge product** of $\alpha$ and $\beta$ is $\alpha \wedge \beta = \alpha \otimes \beta - \beta \otimes \alpha$. This means that for $\vec{v},\vec{w}\in V$ we have $\alpha \wedge \beta(\vec{v},\vec{w})=\alpha(\vec{v})\beta(\vec{w})-\beta(\vec{v})\alpha(\vec{w})$. More generally, if $\alpha^{1},\ldots, \alpha^{k}\in V^{*}$ then $\alpha^{1}\wedge \cdots \wedge \alpha^{k}=\sum_{_{\sigma \in S^{k}}}^{} (\mathrm{sgn} ~\sigma) \alpha^{\sigma(1)}\otimes \cdots \otimes \alpha^{\sigma(k)}$. The space of **alternating $k$-tensors** or (**$k$-covectors**)  $\bigwedge$ of $V^{*}$ is $\bigwedge^{k}V^{*}=\mathrm{span}\{\alpha^{1}\wedge \cdots \wedge \alpha^{k}\}$.
>
{ #def-wedge-product-and-alternating-k-tensors}



> [!prop] Properties of the Wedge Product
> Let $\omega, \omega'\in \bigwedge^{k} V^{*}$ and $\eta,\eta' \in \bigwedge^{l}V^{*}$ and $\varphi\in \bigwedge^{m}V^{*}$
> 1. The wedge is bilinear
> 2. The wedge is associative
> 3. The wedge is antisymmetric
> 4. For any $\omega^{1},\ldots, \omega^{k}\in V^{*}$ we have that for $\vec{v}_{1},\ldots,\vec{v}_{k}\in V$ that $\omega^{1}\wedge \cdots \wedge \omega^{k}(\vec{v}_{1},\ldots,\vec{v}_{k})=\det([\omega^{i}(\vec{v}_{j})])$


> [!example]
> For $0\le k \le n$ we have that $\dim(\bigwedge^{k} V^{*})=\binom{n}{k}$, and a basis for $\bigwedge^{k}V^{*}$ is given by the wedge of the basis for $V^{*}$ up to $k$.


> [!def] Decomposable $k$-covectors
> A $k$-covector $\eta$ is called **decomposable** if it can be expressed as $\eta=\omega^{1}\wedge \cdots\wedge \omega^{k}$ for some covectors $\omega^{1},\ldots, \omega^{k}$. 
>
{ #def-decomposable-k-covectors}



> [!example]
> The $2$-covector in $(\mathbb{R}^{4})^{*}$ $dx^{1}\wedge dx^{2}+dx^{3}\wedge dx^{4}$ is not decomposable.


> [!def] Exterior Algebra
> For any $n$-dimensional vector space $V$, the **exterior algebra** of $V$ is the vector space $\bigwedge(V^{*})=\bigoplus_{k=0}^{n} \bigwedge^{k}(V^{*})$ equipped with the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-wedge-product-and-alternating-k-tensors\|wedge product]].
>
{ #def-exterior-algebra}


## Interior Multiplication
Let $V$ be a finite-dimensional vector space. Each $\vec{v}\in V$ defines a map $i_{\vec{v}}:\bigwedge^{k}(V^{*})\to \bigwedge^{k-1}(V^{*})$ as follows: Given $\omega \in \bigwedge^{k}(V^{*})$ and $\vec{v}_{1},\ldots, \vec{v}_{k-1}\in V$ then $i_{\vec{v}}(\omega)(\vec{v}_{1},\ldots,\vec{k-1})=\omega(\vec{v},\vec{v}_{1},\ldots,\vec{v}_{k-1})$. So $i_{\vec{v}}\omega$ is like a "partial evaluation" of $\omega$ obtained by putting $\vec{v}$ in the 1st slot of the input vectors for $\omega$. Also called "left hook" of $\vec{v}$ with $\omega$ denoted (weird symbol). Straightforward to show:
1. $i_{\vec{v}}(i_{\vec{v}}(\omega))=0$
2. For $\omega \in \bigwedge^{k}V^{*}, \eta\in \bigwedge^{l}V^{*}$, we have that $i_{\vec{v}}(\omega \wedge \eta)=(i_{\vec{v}}\omega)\wedge \eta+(-1)^{k}\omega \wedge(i_{\vec{v}}\eta)$

## Differential Forms on Manifolds

> [!def] Bundle of Alternating $k$-covectors
> The **bundle of alternating $k$-covectors** on a smooth manifold $M$ is $\bigwedge^{k}(T^{*}M)= \coprod_{p\in M} \bigwedge^{k}(T_{p}^{*}M)$ .
>
{ #def-bundle-of-alternating-k-covectors}



> [!def] Differential $k$-forms
> A **differential $k$-forms** on $M$ is a smooth section of $\bigwedge^{k}(T^{*}M)$ where $k$ is called the **degree** of the form.
>
{ #def-differential-k-forms}



> [!def] Title
> The vector space of smooth $k$-forms on $M$ is denoted $\Omega^{k}(M)=\Gamma(\bigwedge^{k}(T^{*}M))$ which are sections of a bundle.

The wedge product is defined pointwise $(\omega \wedge \eta)_{p}=\omega_{p}\wedge \eta_{p}$. So the wedge product of a $k$-form and $l$-form is a $(k+l)$-form. If $k=0$, $\omega$ is just a function $f:M\to \mathbb{R}$ and we define $f \wedge \eta=f\eta$.

With this product the vector space $\Omega^{*}(M)=\bigoplus_{k=0}^{n} \Omega^{k}(M)$ becomes a graded algebra.

In any local coordinate chart $(x^{i})$ any [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|k-form]] can be written as
$$
\omega = \sum_{(i_{1},\dots,i_{k})}^{} f_{i_{1},\ldots,i_{k}}(x) dx^{i_{1}} \wedge \cdots \wedge dx^{i_{k}}
$$
where the sum is over all *multi-indices* of degree $k$ $I=(i_{1},\ldots,i_{k})$. $\omega$ is smooth if and only if all the coefficient functions $f_{I}(x)$ are smooth. And since
$$
dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}\left(  \frac{\partial }{\partial x^{j_{1}}}, \ldots, \frac{\partial  }{\partial x^{j_{k}}} \right)=\pm\delta_{J}^{I}
$$
we can recover the function $f_{I}(x)$ by $f_{I}(x)=\omega \left( \frac{\partial }{\partial x^{i_{1}}} ,\ldots \frac{\partial }{\partial x^{i_{k}}}\right)$.

## Pullbacks
Let $F:M\to N$ be a smooth map and $\omega$ a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $N$. Then $F^{*}\omega$ is the $k$-form on $M$ defined by: for $\vec{v}_{1},\ldots,\vec{v}_{k}\in T_{p}M$ we have that $(F^{*}\omega)_{p}(\vec{v}_{1},\ldots,\vec{v}_{k})=\omega_{F(p)}(dF_{p}(\vec{v}_{1}),\ldots,dF_{p}(\vec{v}_{k}))$.


> [!lemma] Lemma
> Suppose $F:M\to N$ smooth
> 1. $F^{*}:\Omega^{k}(N)\to \Omega^{k}(M)$ is linear over $\mathbb{R}$.
> 2. $F^{*}(\omega \wedge \eta)=(F^{*}\omega)\wedge(F^{*}(\eta))$
> 3. In any smooth chart $(y^{i})$ on $N$,
> $$
> F^{*}\left( \sum_{|I|=k}^{} f_{I}(y)dy^{i_{1}}\wedge \cdots\wedge dy^{i_{k}} \right)=\sum_{|I|=k}^{} f_{I}(y \circ F) d(y^{i_{1}}\circ F)\wedge\cdots\wedge d(y^{i_{k}}\circ F)
> $$


> [!example] Computing the Pull Back
> Let $\omega=dx \wedge dy$ on $\mathbb{R}^{2}$. Consider the coordinate transformation $F:\mathbb{R}^{2}\to \mathbb{R}^{2}$ by sending $(r,\theta)\mapsto(x,y)$ for $F(r,\theta)=(r\cos \theta,r\sin\theta)$. Compute $F^{*}\omega$:
> $$
> \begin{align}
> F^{*}\omega&=d(r\cos \theta)\wedge d(r\sin \theta) \\
  &= (-(r\sin \theta )d\theta+(\cos\theta)dr)\wedge ((r\cos \theta)d\theta+(\sin \theta)dr) \\
  &= -(r\sin^{2}\theta)d\theta \wedge dr+(r\cos^{2}\theta)dr\wedge d\theta \\
  &= r(\cos^{2}\theta+\sin^{2}\theta)dr\wedge d\theta \\
  &= r dr\wedge d\theta
  \end{align}
> $$

In general, for a change of coordinates
$$
\tilde{x}^{i}=\tilde{x}^{i}(x^{1},\ldots,x^{n})\quad i=1,\ldots,n
$$
we have that
$$
d\tilde{x}^{1}\wedge \cdots \wedge d\tilde{x}^{n}= \det\left( \frac{\partial \tilde{x}^{j}}{\partial x^{j}} \right) dx^{1}\wedge \cdots \wedge dx^{n}
$$
## Exterior Derivative
> [!def] Exterior Derivative
> Let $\omega=\sum_{|I|=k}^{}f_{I}(x)dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ be a [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on $\mathbb{R}^{n}$. Then the **exterior derivative** of $\omega$ is the $(k+1)$-form $d\omega=\sum_{|I|=k}^{}df_{I} \wedge dx^{i_{1}}\wedge \cdots \wedge dx^{i_{k}}$ where $df_{I}$ is the differential of the function $f_{I}$.
>
{ #def-exterior-derivative}



> [!example] Computing the [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivative]]
> If $\omega$ is a $1$-form on $\mathbb{R}^{n}$ $\omega=\sum_{i=1}^{n}f_{i}(x)dx^{i}$ then
> $$
> \begin{align}
> d\omega &= \sum_{i=1}^{n}df_{i}\wedge dx^{i}= \sum_{i=1}^{n} \left( \frac{\partial f_{i}}{\partial x^{j}} dx^{j} \right) \wedge dx^{i} \\
  &= \sum_{i<j}^{} \left(  \frac{\partial f_{j}}{\partial x^{i}}- \frac{\partial f_{i}}{\partial x^{j}} \right) dx^{i}\wedge dx^{j}
  \end{align}
> $$
> Note that our previous definition for $\omega$ to be *closed* is equivalent to $d\omega=0$.


> [!prop] Properties of [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-exterior-derivative\|Exterior Derivatives]] $d$ on $\mathbb{R}^{n}$
> 1. $d$ is linear over $\mathbb{R}$
> 2. If $\omega\in \Omega^{k}(\mathbb{R}^{n})$ and $\eta\in \Omega^{l}(\mathbb{R}^{n})$ then $d(\omega \wedge \eta)=(d\omega)\wedge \eta+(-1)^{k}\omega \wedge (d \eta)$
> 3. $d\circ d=0$
> 4. $d$ commutes with pullback, i.e., if $F:M\to N$ is smooth and $\omega \in \Omega^{*}(N)$ then $F^{*}(d\omega)=d(F^{*}\omega)$.
>
{ #prop-properties-of-exterior-derivatives-on-Rn}


> [!proof]
> We'll give a proof of 2. for the case when $k=l=1$. By linearity, it suffices to consider $\omega=fdx^{i}$ and $\eta=gdx^{j}$ (no sums!). So $\omega \wedge \eta=fg dx^{i}\wedge dx^{j}$
> $$
> \begin{align}
  d\omega &= \left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}} dx^{k}\right)\wedge dx^{i} \\
  d\eta &= \left( \sum_{k}^{} \frac{\partial g}{\partial x^{k}} dx^{k} \right) \wedge dx^{j} \\
  \implies d(\omega \wedge \eta) &= \left( \sum_{k}^{} \left( g \frac{\partial f}{\partial x^{k}}+f \frac{\partial g}{\partial x^{k}} \right) dx^{k} \right)\wedge dx^{i}\wedge dx^{j} \\
  &= \underbrace{\left( \sum_{k}^{} \frac{\partial f}{\partial x^{k}}dx^{k} \wedge dx^{i} \right)}_{d\omega} \wedge (g dx^{i}) - (fdx^{i}) \wedge d\eta \\
  &= d\omega \wedge \eta - \omega \wedge d\eta
  \end{align}
> $$
> For part 3. consider a $0$-form $f$ then
> $$
> \begin{align}
> d(df)&=d\left( \frac{\partial f}{\partial x^{i}} dx^{i}\right)=d\left( \frac{\partial f}{\partial x^{i}} \right)\wedge dx^{i}=\cdots
> \end{align}
> $$
> where we conclude the result by using the fact that mixed partial derivatives commute.


> [!def] Closed and Exact Differential Forms
> [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] $\omega$ on $U\subseteq \mathbb{R}^{n}$ is **closed** if $d\omega=0$ and **exact** if there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-closed-and-exact-differential-forms}


We have already shown that exact $\Rightarrow$ closed, while the converse is *almost* true.


> [!lemma] Poincare Lemma
> If $\omega$ is a closed [[Independent Learning/Math/Topology and Geometry/Differential Geometry/Differential Geometry#^def-differential-k-forms\|$k$-form]] on an open set $U \subseteq \mathbb{R}^{n}$ that's homeomoprhic to an open ball in $\mathbb{R}^{n}$ then there exists a $(k-1)$-form $\eta$ on $U$ such that $\omega=d\eta$.
>
{ #def-poincare-lemma}


